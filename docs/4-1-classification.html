<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>4.1 Classification | Lecture notes for Statistical Inference and Machine Learning</title>
  <meta name="description" content="These are the lecture notes for the course Statistical Inference and Machine Learning at the Department of statistics, Uppsala University." />
  <meta name="generator" content="bookdown 0.20 and GitBook 2.6.7" />

  <meta property="og:title" content="4.1 Classification | Lecture notes for Statistical Inference and Machine Learning" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="These are the lecture notes for the course Statistical Inference and Machine Learning at the Department of statistics, Uppsala University." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="4.1 Classification | Lecture notes for Statistical Inference and Machine Learning" />
  
  <meta name="twitter:description" content="These are the lecture notes for the course Statistical Inference and Machine Learning at the Department of statistics, Uppsala University." />
  

<meta name="author" content="Patrik Andersson" />


<meta name="date" content="2020-09-07" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="4-ch-statLearn.html"/>
<link rel="next" href="4-2-support-vector-machines-i.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />











<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Inference and Machine Learning</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="1-ch-likelihood.html"><a href="1-ch-likelihood.html"><i class="fa fa-check"></i><b>1</b> Likelihood-based methods</a><ul>
<li class="chapter" data-level="1.1" data-path="1-1-maximum-likelihood-estimation.html"><a href="1-1-maximum-likelihood-estimation.html"><i class="fa fa-check"></i><b>1.1</b> Maximum likelihood estimation</a></li>
<li class="chapter" data-level="1.2" data-path="1-2-hypothesis-testing.html"><a href="1-2-hypothesis-testing.html"><i class="fa fa-check"></i><b>1.2</b> Hypothesis testing</a></li>
<li class="chapter" data-level="1.3" data-path="1-3-likelihood-ratio-test.html"><a href="1-3-likelihood-ratio-test.html"><i class="fa fa-check"></i><b>1.3</b> Likelihood ratio test</a></li>
<li class="chapter" data-level="1.4" data-path="1-4-mathematical-aside-taylor-expansion.html"><a href="1-4-mathematical-aside-taylor-expansion.html"><i class="fa fa-check"></i><b>1.4</b> Mathematical aside: Taylor expansion</a></li>
<li class="chapter" data-level="1.5" data-path="1-5-asymptotic-distribution-of-the-mle.html"><a href="1-5-asymptotic-distribution-of-the-mle.html"><i class="fa fa-check"></i><b>1.5</b> Asymptotic distribution of the MLE</a></li>
<li class="chapter" data-level="1.6" data-path="1-6-the-delta-method.html"><a href="1-6-the-delta-method.html"><i class="fa fa-check"></i><b>1.6</b> The delta method</a></li>
<li class="chapter" data-level="1.7" data-path="1-7-wilks-test.html"><a href="1-7-wilks-test.html"><i class="fa fa-check"></i><b>1.7</b> Wilks’ test</a></li>
<li class="chapter" data-level="1.8" data-path="1-8-walds-test.html"><a href="1-8-walds-test.html"><i class="fa fa-check"></i><b>1.8</b> Wald’s test</a></li>
<li class="chapter" data-level="1.9" data-path="1-9-score-test.html"><a href="1-9-score-test.html"><i class="fa fa-check"></i><b>1.9</b> Score test</a></li>
<li class="chapter" data-level="1.10" data-path="1-10-confidence-intervals.html"><a href="1-10-confidence-intervals.html"><i class="fa fa-check"></i><b>1.10</b> Confidence intervals</a></li>
<li class="chapter" data-level="1.11" data-path="1-11-an-application-i.html"><a href="1-11-an-application-i.html"><i class="fa fa-check"></i><b>1.11</b> An application I</a></li>
<li class="chapter" data-level="1.12" data-path="1-12-summary.html"><a href="1-12-summary.html"><i class="fa fa-check"></i><b>1.12</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="2-ch-bootstrap.html"><a href="2-ch-bootstrap.html"><i class="fa fa-check"></i><b>2</b> Bootstrap</a><ul>
<li class="chapter" data-level="2.1" data-path="2-1-parametric-vs-non-parametric.html"><a href="2-1-parametric-vs-non-parametric.html"><i class="fa fa-check"></i><b>2.1</b> Parametric vs non-parametric</a></li>
<li class="chapter" data-level="2.2" data-path="2-2-non-parametric-estimation.html"><a href="2-2-non-parametric-estimation.html"><i class="fa fa-check"></i><b>2.2</b> Non-parametric estimation</a></li>
<li class="chapter" data-level="2.3" data-path="2-3-bootstrap.html"><a href="2-3-bootstrap.html"><i class="fa fa-check"></i><b>2.3</b> Bootstrap</a></li>
<li class="chapter" data-level="2.4" data-path="2-4-parametric-bootstrap.html"><a href="2-4-parametric-bootstrap.html"><i class="fa fa-check"></i><b>2.4</b> Parametric bootstrap</a></li>
<li class="chapter" data-level="2.5" data-path="2-5-an-application-ii.html"><a href="2-5-an-application-ii.html"><i class="fa fa-check"></i><b>2.5</b> An application II</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="3-bayesian-statistics.html"><a href="3-bayesian-statistics.html"><i class="fa fa-check"></i><b>3</b> Bayesian statistics</a><ul>
<li class="chapter" data-level="3.1" data-path="3-1-some-basic-decision-theory.html"><a href="3-1-some-basic-decision-theory.html"><i class="fa fa-check"></i><b>3.1</b> Some basic decision theory</a></li>
<li class="chapter" data-level="3.2" data-path="3-2-bayesian-statistics-1.html"><a href="3-2-bayesian-statistics-1.html"><i class="fa fa-check"></i><b>3.2</b> Bayesian statistics</a></li>
<li class="chapter" data-level="3.3" data-path="3-3-choosing-prior.html"><a href="3-3-choosing-prior.html"><i class="fa fa-check"></i><b>3.3</b> Choosing prior</a></li>
<li class="chapter" data-level="3.4" data-path="3-4-multiparameter-problems.html"><a href="3-4-multiparameter-problems.html"><i class="fa fa-check"></i><b>3.4</b> Multiparameter problems</a></li>
<li class="chapter" data-level="3.5" data-path="3-5-markov-chain-monte-carlo.html"><a href="3-5-markov-chain-monte-carlo.html"><i class="fa fa-check"></i><b>3.5</b> Markov chain Monte Carlo</a></li>
<li class="chapter" data-level="3.6" data-path="3-6-an-application-iii.html"><a href="3-6-an-application-iii.html"><i class="fa fa-check"></i><b>3.6</b> An application III</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="4-ch-statLearn.html"><a href="4-ch-statLearn.html"><i class="fa fa-check"></i><b>4</b> Statistical learning (draft)</a><ul>
<li class="chapter" data-level="4.1" data-path="4-1-classification.html"><a href="4-1-classification.html"><i class="fa fa-check"></i><b>4.1</b> Classification</a></li>
<li class="chapter" data-level="4.2" data-path="4-2-support-vector-machines-i.html"><a href="4-2-support-vector-machines-i.html"><i class="fa fa-check"></i><b>4.2</b> Support vector machines I</a></li>
<li class="chapter" data-level="4.3" data-path="4-3-hoeffdings-inequality.html"><a href="4-3-hoeffdings-inequality.html"><i class="fa fa-check"></i><b>4.3</b> Hoeffding’s inequality</a></li>
<li class="chapter" data-level="4.4" data-path="4-4-generalization-error.html"><a href="4-4-generalization-error.html"><i class="fa fa-check"></i><b>4.4</b> Generalization error</a></li>
<li class="chapter" data-level="4.5" data-path="4-5-vc-dimension.html"><a href="4-5-vc-dimension.html"><i class="fa fa-check"></i><b>4.5</b> VC-dimension</a></li>
<li class="chapter" data-level="4.6" data-path="4-6-support-vector-machines-ii.html"><a href="4-6-support-vector-machines-ii.html"><i class="fa fa-check"></i><b>4.6</b> Support vector machines II</a></li>
<li class="chapter" data-level="4.7" data-path="4-7-bias-variance-decomposition.html"><a href="4-7-bias-variance-decomposition.html"><i class="fa fa-check"></i><b>4.7</b> Bias-Variance decomposition</a></li>
<li class="chapter" data-level="4.8" data-path="4-8-regression-regularization.html"><a href="4-8-regression-regularization.html"><i class="fa fa-check"></i><b>4.8</b> Regression regularization</a></li>
<li class="chapter" data-level="4.9" data-path="4-9-model-selection.html"><a href="4-9-model-selection.html"><i class="fa fa-check"></i><b>4.9</b> Model selection</a></li>
<li class="chapter" data-level="4.10" data-path="4-10-an-application-iv.html"><a href="4-10-an-application-iv.html"><i class="fa fa-check"></i><b>4.10</b> An application IV</a></li>
<li class="chapter" data-level="4.11" data-path="4-11-an-application-v.html"><a href="4-11-an-application-v.html"><i class="fa fa-check"></i><b>4.11</b> An application V</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="5-beyond-linearity-draft.html"><a href="5-beyond-linearity-draft.html"><i class="fa fa-check"></i><b>5</b> Beyond linearity (draft)</a><ul>
<li class="chapter" data-level="5.1" data-path="5-1-smoothing-splines.html"><a href="5-1-smoothing-splines.html"><i class="fa fa-check"></i><b>5.1</b> Smoothing splines</a></li>
<li class="chapter" data-level="5.2" data-path="5-2-generalized-additive-models.html"><a href="5-2-generalized-additive-models.html"><i class="fa fa-check"></i><b>5.2</b> Generalized additive models</a></li>
<li class="chapter" data-level="5.3" data-path="5-3-neural-networks.html"><a href="5-3-neural-networks.html"><i class="fa fa-check"></i><b>5.3</b> Neural networks</a></li>
<li class="chapter" data-level="5.4" data-path="5-4-stochasitc-gradient-descent.html"><a href="5-4-stochasitc-gradient-descent.html"><i class="fa fa-check"></i><b>5.4</b> Stochasitc gradient descent</a></li>
<li class="chapter" data-level="5.5" data-path="5-5-an-application-vi.html"><a href="5-5-an-application-vi.html"><i class="fa fa-check"></i><b>5.5</b> An application VI</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Lecture notes for Statistical Inference and Machine Learning</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="classification" class="section level2">
<h2><span class="header-section-number">4.1</span> Classification</h2>
<p>In this section we discuss some theory of classification, without going in to detail about any particlar classification method.</p>
<p>We are given training data <span class="math inline">\((x_i,y_i)\quad i=1,\ldots, n\)</span> what we assume are a random sample from an unknown distribution <span class="math inline">\(P_{X,Y}\)</span>. The goal is to find a classifier, that is a function <span class="math inline">\(h:\mathcal X \mapsto \mathcal Y= \left\{-1,1\right\}\)</span>, based on the training data. There is a loss function <span class="math inline">\(l:\mathcal Y\times \mathcal Y \mapsto \mathbb R\)</span>. Most commonly this is the 0-1 loss,
<span class="math display">\[
l(h(x),y) :=1(h(x)\neq y).
\]</span>
Then the out-of-sample error is
<span class="math display">\[
E_{out}(h) = E_{X,Y}\left[ l(h(X),Y) \right] = P(h(X)\neq Y).
\]</span></p>
To simplify notation we also define the conditional class probability,
<span class="math display">\[
\eta(x):=P\left( Y=1\mid X=x \right).
\]</span>

<div class="note">
In the imaginary case where we know the distribution of <span class="math inline">\(X,Y\)</span>, the <span class="math inline">\(h\)</span> that minimizes this out-of-sample error is called the Bayes classifier. We claim that it is
<span class="math display">\[\begin{align*}
h^\star(x) &amp;= \begin{cases}
1 &amp; \text{if } \eta(x)\geq 1/2\\
-1 &amp; \text{if } \eta(x)&lt; 1/2
\end{cases}\\
&amp;= \text{sign} (\eta(x) - 1/2).
\end{align*}\]</span>
</div>

<p>That is, we should classify to the class that has the highest probabability, conditioned on <span class="math inline">\(X\)</span>. The minimal out-of-sample error is called the Bayes risk.</p>
<p>Let us prove this. The claim is that for any other classifier <span class="math inline">\(h(x)\)</span>, the out-of-sample error is at least as large, i.e.<br />
<span class="math display">\[
E_{out}(h) = P\left( h(X)\neq Y \right) \geq P\left( h^\star(X)\neq Y \right) = E_{out}(h^\star).
\]</span>
Or, equivalently, <span class="math inline">\(P\left( h^\star(X)= Y \right)\geq P\left( h(X) = Y \right)\)</span>. First note that if we condition on <span class="math inline">\(X=x\)</span>, then either <span class="math inline">\(h(x)=-1\)</span> or <span class="math inline">\(h(x)=1\)</span>. Therefore,
<span class="math display">\[\begin{align*}
P\left( h(X) = Y  \mid X=x\right) &amp;= 1_{h(x) = 1}P\left(Y=1 \mid X=x\right) + 1_{h(x) = -1}P\left(Y=-1 \mid X=x\right)\\
&amp;= 1_{ h(x) = 1}\eta(x) + (1-1_{ h(x) = 1})(1-\eta(x))\\
&amp;= 1_{ h(x) = 1}\left(  2\eta(x)-1 \right) +1-\eta(x).
\end{align*}\]</span>
and the same is true if we replace <span class="math inline">\(h\)</span> by <span class="math inline">\(h^\star\)</span>. Then,
<span class="math display">\[
P\left( h^\star(X) = Y  \mid X=x\right) -P\left( h = Y  \mid X=x\right)  =  \left(1_{ h^\star(x) = 1} - 1_{ h(x) = 1}\right)\left(  2\eta(x)-1 \right).
\]</span>
Now if <span class="math inline">\(x\)</span> is such that <span class="math inline">\(\eta(x)\geq 1/2\)</span>, then
<span class="math display">\[
\underbrace{\left(\underbrace{1_{ h^\star(x) = 1}}_{=1} - \underbrace{1_{ h(x) = 1}}_{= 0 \text{ or } 1}\right)}_{\geq 0}\underbrace{\left(  2\eta(x)-1 \right)}_{\geq 0}\geq 0.
\]</span>
On the other hand, if <span class="math inline">\(x\)</span> is such that <span class="math inline">\(\eta(x)&lt; 1/2\)</span>, then
<span class="math display">\[
\underbrace{\left(\underbrace{1_{ h^\star(x) = 1}}_{=0} - \underbrace{1_{ h(x) = 1}}_{= 0 \text{ or } 1}\right)}_{\leq 0}\underbrace{\left(  2\eta(x)-1 \right)}_{\leq 0}\geq 0.
\]</span>
In any case, this implies that
<span class="math display">\[
P\left( h^\star(X) = Y  \mid X=x\right) -P\left( h = Y  \mid X=x\right) \geq 0.
\]</span>
Since this is true for any <span class="math inline">\(x\)</span>, it also holds that
<span class="math display">\[
P\left( h^\star(X) = Y  \right) -P\left( h(X) = Y  \right) \geq 0,
\]</span>
as we claimed.</p>
<p>Since the Bayes classifier is not available in practice, one needs to take another approach. One is to estimate the function <span class="math inline">\(\eta(x)\)</span> and then construct a classifier by plugging in to the Bayes classifier. That is,
<span class="math display">\[
h(x) = \text{sign} (\hat\eta(x) - 1/2).
\]</span>
Logistic regression can be thought of as an example of this.</p>
<p>Another option is to directly find a function <span class="math inline">\(h\)</span> that directly minimizes the in-sample error,
<span class="math display">\[
E_{in} = \frac{1}{n} \sum_{i=1}^n I(h(x_i)\neq y_i).
\]</span>
It would be done by specifying a class of candidate classifiers <span class="math inline">\(\mathcal H\)</span> from which we pick the best <span class="math inline">\(h\)</span>. For example we might consider all the linear classifiers,
<span class="math display">\[
\mathcal H = \left\{ h(x) = \text{sign}(x^T\beta + \beta_0) \mid \beta \in \mathbb R^d, \beta_0\in\mathbb R \right\}.
\]</span>
Note that if <span class="math inline">\(y_i(x_i^T\beta + \beta_0)&gt;0\)</span>, then <span class="math inline">\(y_i\)</span> is classified correctly and if <span class="math inline">\(y_i(x_i^T\beta + \beta_0)&lt;0\)</span>, then <span class="math inline">\(y_i\)</span> is missclassified. We can therefore think of <span class="math inline">\(y_i(x_i^T\beta + \beta_0)\)</span> as measuring how close <span class="math inline">\(y_i\)</span> is to being classified correctly or incorrectly. In fact, we can write the 0-1 loss as
<span class="math display">\[
l(h(x),y) = I(y(x^T\beta + \beta_0)\leq 0).
\]</span>
More generally we can consider a function <span class="math inline">\(f(x)\)</span>, a classifier <span class="math inline">\(h(x) = \text{sign}(f(x))\)</span> and a loss function that depends on the margin <span class="math inline">\(yf(x)\)</span>.</p>
<p>It turns out that <span class="math inline">\(E_{in}\)</span>, as defined above, is difficult to use for training. The reason for this can be understood in different ways. Mathematically, the 0-1 loss is non-convex, and non-convex functions are in general difficult to optimize. In terms of classification, consider the picture below. Two points are missclassified, but we can see that by moving the classification boundary, we can find a classifier that only missclassifies one point. However, the training algorithm will try to move the boundary a very small step, and see if that gives an improvement. If we use the 0-1 loss, the in-sample error will be the same as long as the boundary is not moved far enough. Therefore it is better to use a loss function that also measures how far away each point is from being classified correctly/incorrectly.</p>
<div class="figure" style="text-align: center"><span id="fig:classicificationExample"></span>
<img src="04-statisticalLearning_files/figure-html/classicificationExample-1.png" alt="Example of linear classification" width="80%" />
<p class="caption">
Figure 4.1: Example of linear classification
</p>
</div>
Below we discuss two alternative loss functions that produce two much used methods for classification, the hinge loss and the negative log-likelihood. They are plotted in the picture below.
<div class="figure" style="text-align: center"><span id="fig:lossFunctions"></span>
<img src="04-statisticalLearning_files/figure-html/lossFunctions-1.png" alt="Loss functions for classification" width="80%" />
<p class="caption">
Figure 4.2: Loss functions for classification
</p>
</div>
<p>The hinge loss is
<span class="math display">\[
l(f(x),y)=(1-yf(x))_+,
\]</span>
where <span class="math inline">\(f_+:=\max (0,f)\)</span>. This function takes care of our complaints about the 0-1 loss function. If a point <span class="math inline">\(x_i\)</span> is correctly classified, and it is far away from being missclassified, so that <span class="math inline">\(y_if(x_i)\)</span> is large and positive, the loss is 0. However if it is close to being missclassified it incurs a loss, even if it is correctly classified. It might however worry some that we are using a different function for training (e.g. the hinge loss) and evaluation (0-1). Let us therefore see what the population minimizer of the hinge loss is. The population minimizer for the 0-1 is the Bayes classifier, and for the hinge loss,
<span class="math display">\[
h^\star_{hinge} := \underset{h}{\text{argmin}}~ E\left[(1-Yh(X))_+ \right].
\]</span>
Let us fix an arbitrary <span class="math inline">\(x\)</span> and then we should find <span class="math inline">\(h(x)\)</span> that minimizes
<span class="math display">\[
E\left[(1-Yh(x))_+ \mid X=x\right] = (1-h(x))_+\eta(x) + (1+h(x))_+(1-\eta(x)).
\]</span>
We should always have <span class="math inline">\(-1\leq h(x)\leq 1\)</span>, because otherwise we could truncate <span class="math inline">\(h(x)\)</span> to get a smaller loss. So with that assumption,
<span class="math display">\[
E\left[(1-Yh(x))_+ \mid X=x\right] = (1-h(x))\eta(x) + (1+h(x))(1-\eta(x)) = 1+(1-2\eta(x))h(x).
\]</span>
Then we realize that depending on the sign of <span class="math inline">\(1-2\eta(x)\)</span> we should choose <span class="math inline">\(h(x)\)</span> to be as large positive or negative as possible, i.e.<br />
<span class="math display">\[
h^\star_{hinge} = \begin{cases}
1&amp;\text{if } \eta\geq 1/2\\
-1&amp;\text{if } \eta&lt; 1/2,
\end{cases}
\]</span>
which is exactly the Bayes classifier.</p>
<p>The other loss function we will discuss is the loss function of logistic regression. In logistic regression we model the conditional probability as
<span class="math display">\[
\eta(x) = \frac{1}{1+e^{-f(x)}}.
\]</span>
The likelihood of the observation <span class="math inline">\((x,y)\)</span> is therefore,
<span class="math display">\[
L(x,y) = \begin{cases}
\eta(x)&amp;\text{if } y=1\\
1-\eta(x)&amp;\text{if } y=-1.
\end{cases}
\]</span>
Then the negative log-likelihood (which should be minimized) is,
<span class="math display">\[
-\log_2(L(x,y)) = \begin{cases}
\log_2\left(1+e^{-f(x)}\right)&amp;\text{if } y=1\\
\log_2\left(1+e^{f(x)}\right)&amp;\text{if } y=-1
\end{cases}
= \log_2(1+e^{yf(x)})=:l(f(x),y)
\]</span>
Here we took base 2 logarithm since <span class="math inline">\(\log_2(1+e^0)=1\)</span> and then the loss function is on the same scale as the hinge and 0-1. Also for logistic regression you can show that if you classify according to the class with highest probability, the population minimizer is again the Bayes classifier.</p>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="4-ch-statLearn.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="4-2-support-vector-machines-i.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
