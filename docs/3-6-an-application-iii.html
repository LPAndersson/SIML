<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>3.6 An application III | Lecture notes for Statistical Inference and Machine Learning</title>
  <meta name="description" content="These are the lecture notes for the course Statistical Inference and Machine Learning at the Department of statistics, Uppsala University." />
  <meta name="generator" content="bookdown 0.20 and GitBook 2.6.7" />

  <meta property="og:title" content="3.6 An application III | Lecture notes for Statistical Inference and Machine Learning" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="These are the lecture notes for the course Statistical Inference and Machine Learning at the Department of statistics, Uppsala University." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="3.6 An application III | Lecture notes for Statistical Inference and Machine Learning" />
  
  <meta name="twitter:description" content="These are the lecture notes for the course Statistical Inference and Machine Learning at the Department of statistics, Uppsala University." />
  

<meta name="author" content="Patrik Andersson" />


<meta name="date" content="2020-09-02" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="3-5-markov-chain-monte-carlo.html"/>
<link rel="next" href="4-ch-statLearn.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />











<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Inference and Machine Learning</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="1-ch-likelihood.html"><a href="1-ch-likelihood.html"><i class="fa fa-check"></i><b>1</b> Likelihood-based methods</a><ul>
<li class="chapter" data-level="1.1" data-path="1-1-maximum-likelihood-estimation.html"><a href="1-1-maximum-likelihood-estimation.html"><i class="fa fa-check"></i><b>1.1</b> Maximum likelihood estimation</a></li>
<li class="chapter" data-level="1.2" data-path="1-2-hypothesis-testing.html"><a href="1-2-hypothesis-testing.html"><i class="fa fa-check"></i><b>1.2</b> Hypothesis testing</a></li>
<li class="chapter" data-level="1.3" data-path="1-3-likelihood-ratio-test.html"><a href="1-3-likelihood-ratio-test.html"><i class="fa fa-check"></i><b>1.3</b> Likelihood ratio test</a></li>
<li class="chapter" data-level="1.4" data-path="1-4-mathematical-aside-taylor-expansion.html"><a href="1-4-mathematical-aside-taylor-expansion.html"><i class="fa fa-check"></i><b>1.4</b> Mathematical aside: Taylor expansion</a></li>
<li class="chapter" data-level="1.5" data-path="1-5-asymptotic-distribution-of-the-mle.html"><a href="1-5-asymptotic-distribution-of-the-mle.html"><i class="fa fa-check"></i><b>1.5</b> Asymptotic distribution of the MLE</a></li>
<li class="chapter" data-level="1.6" data-path="1-6-the-delta-method.html"><a href="1-6-the-delta-method.html"><i class="fa fa-check"></i><b>1.6</b> The delta method</a></li>
<li class="chapter" data-level="1.7" data-path="1-7-wilks-test.html"><a href="1-7-wilks-test.html"><i class="fa fa-check"></i><b>1.7</b> Wilks’ test</a></li>
<li class="chapter" data-level="1.8" data-path="1-8-walds-test.html"><a href="1-8-walds-test.html"><i class="fa fa-check"></i><b>1.8</b> Wald’s test</a></li>
<li class="chapter" data-level="1.9" data-path="1-9-score-test.html"><a href="1-9-score-test.html"><i class="fa fa-check"></i><b>1.9</b> Score test</a></li>
<li class="chapter" data-level="1.10" data-path="1-10-confidence-intervals.html"><a href="1-10-confidence-intervals.html"><i class="fa fa-check"></i><b>1.10</b> Confidence intervals</a></li>
<li class="chapter" data-level="1.11" data-path="1-11-an-application-i.html"><a href="1-11-an-application-i.html"><i class="fa fa-check"></i><b>1.11</b> An application I</a></li>
<li class="chapter" data-level="1.12" data-path="1-12-summary.html"><a href="1-12-summary.html"><i class="fa fa-check"></i><b>1.12</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="2-ch-bootstrap.html"><a href="2-ch-bootstrap.html"><i class="fa fa-check"></i><b>2</b> Bootstrap</a><ul>
<li class="chapter" data-level="2.1" data-path="2-1-parametric-vs-non-parametric.html"><a href="2-1-parametric-vs-non-parametric.html"><i class="fa fa-check"></i><b>2.1</b> Parametric vs non-parametric</a></li>
<li class="chapter" data-level="2.2" data-path="2-2-non-parametric-estimation.html"><a href="2-2-non-parametric-estimation.html"><i class="fa fa-check"></i><b>2.2</b> Non-parametric estimation</a></li>
<li class="chapter" data-level="2.3" data-path="2-3-bootstrap.html"><a href="2-3-bootstrap.html"><i class="fa fa-check"></i><b>2.3</b> Bootstrap</a></li>
<li class="chapter" data-level="2.4" data-path="2-4-parametric-bootstrap.html"><a href="2-4-parametric-bootstrap.html"><i class="fa fa-check"></i><b>2.4</b> Parametric bootstrap</a></li>
<li class="chapter" data-level="2.5" data-path="2-5-an-application-ii.html"><a href="2-5-an-application-ii.html"><i class="fa fa-check"></i><b>2.5</b> An application II</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="3-bayesian-statistics-draft.html"><a href="3-bayesian-statistics-draft.html"><i class="fa fa-check"></i><b>3</b> Bayesian statistics (draft)</a><ul>
<li class="chapter" data-level="3.1" data-path="3-1-some-basic-decision-theory.html"><a href="3-1-some-basic-decision-theory.html"><i class="fa fa-check"></i><b>3.1</b> Some basic decision theory</a></li>
<li class="chapter" data-level="3.2" data-path="3-2-bayesian-statistics.html"><a href="3-2-bayesian-statistics.html"><i class="fa fa-check"></i><b>3.2</b> Bayesian statistics</a></li>
<li class="chapter" data-level="3.3" data-path="3-3-choosing-prior.html"><a href="3-3-choosing-prior.html"><i class="fa fa-check"></i><b>3.3</b> Choosing prior</a></li>
<li class="chapter" data-level="3.4" data-path="3-4-multiparameter-problems.html"><a href="3-4-multiparameter-problems.html"><i class="fa fa-check"></i><b>3.4</b> Multiparameter problems</a></li>
<li class="chapter" data-level="3.5" data-path="3-5-markov-chain-monte-carlo.html"><a href="3-5-markov-chain-monte-carlo.html"><i class="fa fa-check"></i><b>3.5</b> Markov chain monte carlo</a></li>
<li class="chapter" data-level="3.6" data-path="3-6-an-application-iii.html"><a href="3-6-an-application-iii.html"><i class="fa fa-check"></i><b>3.6</b> An application III</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="4-ch-statLearn.html"><a href="4-ch-statLearn.html"><i class="fa fa-check"></i><b>4</b> Statistical learning (draft)</a><ul>
<li class="chapter" data-level="4.1" data-path="4-1-classification.html"><a href="4-1-classification.html"><i class="fa fa-check"></i><b>4.1</b> Classification</a></li>
<li class="chapter" data-level="4.2" data-path="4-2-support-vector-machines-i.html"><a href="4-2-support-vector-machines-i.html"><i class="fa fa-check"></i><b>4.2</b> Support vector machines I</a></li>
<li class="chapter" data-level="4.3" data-path="4-3-hoeffdings-inequality.html"><a href="4-3-hoeffdings-inequality.html"><i class="fa fa-check"></i><b>4.3</b> Hoeffding’s inequality</a></li>
<li class="chapter" data-level="4.4" data-path="4-4-generalization-error.html"><a href="4-4-generalization-error.html"><i class="fa fa-check"></i><b>4.4</b> Generalization error</a></li>
<li class="chapter" data-level="4.5" data-path="4-5-vc-dimension.html"><a href="4-5-vc-dimension.html"><i class="fa fa-check"></i><b>4.5</b> VC-dimension</a></li>
<li class="chapter" data-level="4.6" data-path="4-6-support-vector-machines-ii.html"><a href="4-6-support-vector-machines-ii.html"><i class="fa fa-check"></i><b>4.6</b> Support vector machines II</a></li>
<li class="chapter" data-level="4.7" data-path="4-7-bias-variance-decomposition.html"><a href="4-7-bias-variance-decomposition.html"><i class="fa fa-check"></i><b>4.7</b> Bias-Variance decomposition</a></li>
<li class="chapter" data-level="4.8" data-path="4-8-regression-regularization.html"><a href="4-8-regression-regularization.html"><i class="fa fa-check"></i><b>4.8</b> Regression regularization</a></li>
<li class="chapter" data-level="4.9" data-path="4-9-model-selection.html"><a href="4-9-model-selection.html"><i class="fa fa-check"></i><b>4.9</b> Model selection</a></li>
<li class="chapter" data-level="4.10" data-path="4-10-an-application-iv.html"><a href="4-10-an-application-iv.html"><i class="fa fa-check"></i><b>4.10</b> An application IV</a></li>
<li class="chapter" data-level="4.11" data-path="4-11-an-application-v.html"><a href="4-11-an-application-v.html"><i class="fa fa-check"></i><b>4.11</b> An application V</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="5-beyond-linearity-draft.html"><a href="5-beyond-linearity-draft.html"><i class="fa fa-check"></i><b>5</b> Beyond linearity (draft)</a><ul>
<li class="chapter" data-level="5.1" data-path="5-1-smoothing-splines.html"><a href="5-1-smoothing-splines.html"><i class="fa fa-check"></i><b>5.1</b> Smoothing splines</a></li>
<li class="chapter" data-level="5.2" data-path="5-2-generalized-additive-models.html"><a href="5-2-generalized-additive-models.html"><i class="fa fa-check"></i><b>5.2</b> Generalized additive models</a></li>
<li class="chapter" data-level="5.3" data-path="5-3-neural-networks.html"><a href="5-3-neural-networks.html"><i class="fa fa-check"></i><b>5.3</b> Neural networks</a></li>
<li class="chapter" data-level="5.4" data-path="5-4-stochasitc-gradient-descent.html"><a href="5-4-stochasitc-gradient-descent.html"><i class="fa fa-check"></i><b>5.4</b> Stochasitc gradient descent</a></li>
<li class="chapter" data-level="5.5" data-path="5-5-an-application-vi.html"><a href="5-5-an-application-vi.html"><i class="fa fa-check"></i><b>5.5</b> An application VI</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Lecture notes for Statistical Inference and Machine Learning</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="an-application-iii" class="section level2">
<h2><span class="header-section-number">3.6</span> An application III</h2>
<!-- Taken from http://www2.geog.ucl.ac.uk/~mdisney/teaching/GEOGG121/sivia_skilling/mterop_hastings.pdf -->
<p>Here we consider the Bayesian probit model as an example. The data generating model is
<span class="math display">\[\begin{align}
Y_i&amp;\sim \mathsf{Bin}(n_i,\pi_i),\\
\pi_i &amp;= \Phi(z_i&#39;\beta),
\end{align}\]</span>
where <span class="math inline">\(z_i=\begin{bmatrix} 1&amp;z_{i1} &amp;z_{i2} &amp;z_{i3} \end{bmatrix}&#39;\)</span> are indicators and <span class="math inline">\(\Phi\)</span> is the <span class="math inline">\(\mathsf N(0,1)\)</span> distribution function. The data is shown in the table.</p>
<table>
<caption><span id="tab:bayesProbit">Table 3.1: </span></caption>
<thead>
<tr class="header">
<th align="right">y</th>
<th align="right">n</th>
<th align="right">z1</th>
<th align="right">z2</th>
<th align="right">z3</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">11</td>
<td align="right">98</td>
<td align="right">1</td>
<td align="right">1</td>
<td align="right">1</td>
</tr>
<tr class="even">
<td align="right">1</td>
<td align="right">18</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">1</td>
</tr>
<tr class="odd">
<td align="right">0</td>
<td align="right">2</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">1</td>
</tr>
<tr class="even">
<td align="right">23</td>
<td align="right">26</td>
<td align="right">1</td>
<td align="right">1</td>
<td align="right">0</td>
</tr>
<tr class="odd">
<td align="right">28</td>
<td align="right">58</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">0</td>
</tr>
<tr class="even">
<td align="right">0</td>
<td align="right">9</td>
<td align="right">1</td>
<td align="right">0</td>
<td align="right">0</td>
</tr>
<tr class="odd">
<td align="right">8</td>
<td align="right">40</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0</td>
</tr>
</tbody>
</table>
<p>The likelihood is
<span class="math display">\[
L(\beta)\propto \prod_{i=1}^n\Phi(z_i&#39;\beta)^{y_i}(1-\Phi(z_i&#39;\beta))^{n_i-y_i}.
\]</span>
We choose a normal distribution as prior on <span class="math inline">\(\beta\)</span>,
<span class="math display">\[
\beta \overset{iid}\sim \mathsf N(0,\lambda),
\]</span>
with <span class="math inline">\(\lambda=10\)</span>. We will use a random walk Metropolis Hastings algorithm with <span class="math inline">\(\varepsilon\overset{iid}\sim \mathsf N(0,\sigma^2)\)</span>. That is, the prior can be written as
<span class="math display">\[
p(\beta) \propto \exp\left( -\frac{1}{2\lambda}\sum_{j=1}^3 \beta_j^2 \right)
\]</span></p>
<p>Now, we implement this in R. First we load the data.</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2-1" data-line-number="1">data.df &lt;-<span class="st"> </span><span class="kw">read.csv</span>(<span class="st">&quot;data/bayesProbit.dat&quot;</span>, <span class="dt">header=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb2-2" data-line-number="2">data.df<span class="op">$</span>z0 &lt;-<span class="st"> </span><span class="kw">rep</span>(<span class="dv">1</span>,<span class="kw">nrow</span>(data.df))</a>
<a class="sourceLine" id="cb2-3" data-line-number="3"></a>
<a class="sourceLine" id="cb2-4" data-line-number="4">col_order &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&#39;y&#39;</span>, <span class="st">&#39;n&#39;</span>,<span class="st">&#39;z0&#39;</span>,<span class="st">&#39;z1&#39;</span>,<span class="st">&#39;z2&#39;</span>,<span class="st">&#39;z3&#39;</span>)</a>
<a class="sourceLine" id="cb2-5" data-line-number="5">data.df &lt;-<span class="st"> </span>data.df[, col_order]</a>
<a class="sourceLine" id="cb2-6" data-line-number="6"></a>
<a class="sourceLine" id="cb2-7" data-line-number="7">n &lt;-<span class="st"> </span><span class="kw">nrow</span>(data.df)</a></code></pre></div>
<p>Then implement the likelihood function and prior density. We do this on a log-scale.</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb3-1" data-line-number="1">logLFcn &lt;-<span class="st"> </span><span class="cf">function</span>(data){</a>
<a class="sourceLine" id="cb3-2" data-line-number="2">  <span class="cf">function</span>(beta){</a>
<a class="sourceLine" id="cb3-3" data-line-number="3">    p &lt;-<span class="st"> </span><span class="kw">pnorm</span>(<span class="kw">as.matrix</span>(data[<span class="dv">3</span><span class="op">:</span><span class="dv">6</span>])<span class="op">%*%</span>beta)</a>
<a class="sourceLine" id="cb3-4" data-line-number="4">    sum &lt;-<span class="st"> </span><span class="dv">0</span></a>
<a class="sourceLine" id="cb3-5" data-line-number="5">    <span class="cf">for</span> (i <span class="cf">in</span> <span class="kw">seq_len</span>(<span class="kw">nrow</span>(data))) {</a>
<a class="sourceLine" id="cb3-6" data-line-number="6">      sum &lt;-<span class="st"> </span>sum <span class="op">+</span><span class="st"> </span>data<span class="op">$</span>y[i]<span class="op">*</span><span class="kw">log</span>(p[i]) <span class="op">+</span><span class="st"> </span>(data<span class="op">$</span>n[i]<span class="op">-</span>data<span class="op">$</span>y[i])<span class="op">*</span><span class="kw">log</span>(<span class="dv">1</span><span class="op">-</span>p[i])</a>
<a class="sourceLine" id="cb3-7" data-line-number="7">    }</a>
<a class="sourceLine" id="cb3-8" data-line-number="8">    sum</a>
<a class="sourceLine" id="cb3-9" data-line-number="9">  }</a>
<a class="sourceLine" id="cb3-10" data-line-number="10">}</a>
<a class="sourceLine" id="cb3-11" data-line-number="11">logL &lt;-<span class="st"> </span><span class="kw">logLFcn</span>(data.df)</a>
<a class="sourceLine" id="cb3-12" data-line-number="12"></a>
<a class="sourceLine" id="cb3-13" data-line-number="13">lambda &lt;-<span class="st"> </span><span class="dv">10</span></a>
<a class="sourceLine" id="cb3-14" data-line-number="14">logPrior &lt;-<span class="st"> </span><span class="cf">function</span>(beta){</a>
<a class="sourceLine" id="cb3-15" data-line-number="15">    <span class="dv">-1</span><span class="op">/</span><span class="dv">2</span><span class="op">/</span>lambda<span class="op">*</span><span class="kw">sum</span>(beta<span class="op">^</span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb3-16" data-line-number="16">}</a>
<a class="sourceLine" id="cb3-17" data-line-number="17"></a>
<a class="sourceLine" id="cb3-18" data-line-number="18">logPosterior &lt;-<span class="st"> </span><span class="cf">function</span>(beta){ <span class="kw">logL</span>(beta) <span class="op">+</span><span class="st"> </span><span class="kw">logPrior</span>(beta)}</a></code></pre></div>
<p>Next we implement the main MCMC-loop. We make it a function so that we can easily reuse it later.</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb4-1" data-line-number="1">mcmc.iter &lt;-<span class="st"> </span><span class="cf">function</span>(x, logPosterior, sigma, n.iter){</a>
<a class="sourceLine" id="cb4-2" data-line-number="2"><span class="co">#Random walk Metropolis Hastings MCMC</span></a>
<a class="sourceLine" id="cb4-3" data-line-number="3"></a>
<a class="sourceLine" id="cb4-4" data-line-number="4">  res &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="ot">NA</span>, n.iter<span class="op">+</span><span class="dv">1</span>, <span class="kw">length</span>(x))</a>
<a class="sourceLine" id="cb4-5" data-line-number="5">  res[<span class="dv">1</span>,] &lt;-<span class="st"> </span>x</a>
<a class="sourceLine" id="cb4-6" data-line-number="6">  logPost &lt;-<span class="st"> </span><span class="kw">logPosterior</span>(x)</a>
<a class="sourceLine" id="cb4-7" data-line-number="7"></a>
<a class="sourceLine" id="cb4-8" data-line-number="8">  accProb &lt;-<span class="st"> </span><span class="dv">0</span></a>
<a class="sourceLine" id="cb4-9" data-line-number="9"></a>
<a class="sourceLine" id="cb4-10" data-line-number="10">  <span class="cf">for</span> (i <span class="cf">in</span> <span class="kw">seq_len</span>(n.iter)){</a>
<a class="sourceLine" id="cb4-11" data-line-number="11">    <span class="co">#New proposal</span></a>
<a class="sourceLine" id="cb4-12" data-line-number="12">    xProp &lt;-<span class="st"> </span>x <span class="op">+</span><span class="st"> </span><span class="kw">rnorm</span>(<span class="kw">length</span>(x), <span class="dv">0</span>, sigma)</a>
<a class="sourceLine" id="cb4-13" data-line-number="13">    <span class="co">#Log posterior of proposal</span></a>
<a class="sourceLine" id="cb4-14" data-line-number="14">    logPostProp &lt;-<span class="st"> </span><span class="kw">logPosterior</span>(xProp)</a>
<a class="sourceLine" id="cb4-15" data-line-number="15">    <span class="co">#Acceptance probability</span></a>
<a class="sourceLine" id="cb4-16" data-line-number="16">    r &lt;-<span class="st"> </span><span class="kw">min</span>( <span class="kw">c</span>(<span class="dv">1</span>, <span class="kw">exp</span>(logPostProp <span class="op">-</span><span class="st"> </span>logPost) ) )</a>
<a class="sourceLine" id="cb4-17" data-line-number="17"></a>
<a class="sourceLine" id="cb4-18" data-line-number="18">    <span class="cf">if</span>(r<span class="op">&gt;</span><span class="kw">runif</span>(<span class="dv">1</span>)){</a>
<a class="sourceLine" id="cb4-19" data-line-number="19">      <span class="co">#Accept</span></a>
<a class="sourceLine" id="cb4-20" data-line-number="20">      x &lt;-<span class="st"> </span>xProp</a>
<a class="sourceLine" id="cb4-21" data-line-number="21">      logPost &lt;-<span class="st"> </span>logPostProp</a>
<a class="sourceLine" id="cb4-22" data-line-number="22">      accProb &lt;-<span class="st"> </span>accProb <span class="op">+</span><span class="st"> </span><span class="dv">1</span></a>
<a class="sourceLine" id="cb4-23" data-line-number="23">    }</a>
<a class="sourceLine" id="cb4-24" data-line-number="24">    res[i<span class="op">+</span><span class="dv">1</span>,] &lt;-<span class="st"> </span>x</a>
<a class="sourceLine" id="cb4-25" data-line-number="25">  }</a>
<a class="sourceLine" id="cb4-26" data-line-number="26">  <span class="kw">list</span>(<span class="dt">sample =</span> res, <span class="dt">accProb =</span> accProb<span class="op">/</span>n.iter)</a>
<a class="sourceLine" id="cb4-27" data-line-number="27">}</a></code></pre></div>
<p>Now run the Markov chain. First a burn-in of 1000 steps, that we then discard. After that a longer run.</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb5-1" data-line-number="1">beta &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>) <span class="co">#Initial value</span></a>
<a class="sourceLine" id="cb5-2" data-line-number="2">nIter &lt;-<span class="st"> </span><span class="dv">100000</span> <span class="co">#Number of MC steps</span></a>
<a class="sourceLine" id="cb5-3" data-line-number="3"></a>
<a class="sourceLine" id="cb5-4" data-line-number="4"><span class="kw">set.seed</span>(<span class="dv">42</span>)</a>
<a class="sourceLine" id="cb5-5" data-line-number="5"></a>
<a class="sourceLine" id="cb5-6" data-line-number="6">beta.mcmc &lt;-<span class="st"> </span><span class="kw">mcmc.iter</span>(beta,</a>
<a class="sourceLine" id="cb5-7" data-line-number="7">                       logPosterior,</a>
<a class="sourceLine" id="cb5-8" data-line-number="8">                       <span class="fl">0.08</span>,</a>
<a class="sourceLine" id="cb5-9" data-line-number="9">                       <span class="dv">1000</span>)</a>
<a class="sourceLine" id="cb5-10" data-line-number="10"></a>
<a class="sourceLine" id="cb5-11" data-line-number="11">beta.mcmc &lt;-<span class="st"> </span><span class="kw">mcmc.iter</span>(beta.mcmc<span class="op">$</span>sample[<span class="kw">nrow</span>(beta.mcmc<span class="op">$</span>sample),],</a>
<a class="sourceLine" id="cb5-12" data-line-number="12">                       logPosterior,</a>
<a class="sourceLine" id="cb5-13" data-line-number="13">                       <span class="fl">0.08</span>,</a>
<a class="sourceLine" id="cb5-14" data-line-number="14">                       nIter)</a></code></pre></div>
<p>Now we do some diagnostics of the simulation. First check that the acceptance probability is reasonable.</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb6-1" data-line-number="1">beta.mcmc<span class="op">$</span>accProb</a></code></pre></div>
<pre><code>## [1] 0.57037</code></pre>
Then we plot the trajectories of the parameters. We want to see that the trajectories appear stationary and that they are not stuck in one state. We only plot the first 1000 steps.
<div class="figure" style="text-align: center"><span id="fig:logProbBayesTrajectory"></span>
<img src="03-bayesian_files/figure-html/logProbBayesTrajectory-1.png" alt="Trajectories of the Markov chain" width="80%" />
<p class="caption">
Figure 3.4: Trajectories of the Markov chain
</p>
</div>
<p>Then we calculate the cumulative mean. We want to see that the simulation is long enough so that the law of large numbers have come in to effect.</p>
<pre><code>## 
## Attaching package: &#39;dplyr&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:gridExtra&#39;:
## 
##     combine</code></pre>
<pre><code>## The following objects are masked from &#39;package:stats&#39;:
## 
##     filter, lag</code></pre>
<pre><code>## The following objects are masked from &#39;package:base&#39;:
## 
##     intersect, setdiff, setequal, union</code></pre>
<div class="figure" style="text-align: center"><span id="fig:logProbBayesCumMean"></span>
<img src="03-bayesian_files/figure-html/logProbBayesCumMean-1.png" alt="Cumulative mean of the Markov chain" width="80%" />
<p class="caption">
Figure 3.5: Cumulative mean of the Markov chain
</p>
</div>
Then we plot the posterior distribution of the parameters.
<div class="figure" style="text-align: center"><span id="fig:logProbBayesPostDist"></span>
<img src="03-bayesian_files/figure-html/logProbBayesPostDist-1.png" alt="Posterior distribution" width="80%" />
<p class="caption">
Figure 3.6: Posterior distribution
</p>
</div>
<p>From this we can get point estimates, the mean of the posterior distribution, and credible intervals.</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb12-1" data-line-number="1"><span class="co">#Point estimates</span></a>
<a class="sourceLine" id="cb12-2" data-line-number="2"><span class="kw">apply</span>(beta.mcmc<span class="op">$</span>sample, <span class="dv">2</span>, mean)</a></code></pre></div>
<pre><code>## [1] -1.0903722  0.6038363  1.1894802 -1.9005936</code></pre>
<div class="sourceCode" id="cb14"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb14-1" data-line-number="1"><span class="co">#95% CI</span></a>
<a class="sourceLine" id="cb14-2" data-line-number="2"><span class="kw">apply</span>(beta.mcmc<span class="op">$</span>sample, <span class="dv">2</span>, quantile, <span class="dt">probs =</span> <span class="kw">c</span>(<span class="fl">0.05</span>,<span class="fl">0.95</span>))</a></code></pre></div>
<pre><code>##           [,1]      [,2]      [,3]      [,4]
## 5%  -1.4605295 0.2098086 0.7740177 -2.353467
## 95% -0.7401921 1.0111992 1.6204207 -1.465202</code></pre>
<p>As a final check, let us verify that our estimates makes sense by comparing our data to our predictions.</p>
<div class="sourceCode" id="cb16"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb16-1" data-line-number="1">beta.fit &lt;-<span class="st"> </span><span class="kw">apply</span>(beta.mcmc<span class="op">$</span>sample, <span class="dv">2</span>, mean)</a>
<a class="sourceLine" id="cb16-2" data-line-number="2">p &lt;-<span class="st"> </span><span class="kw">pnorm</span>(<span class="kw">as.matrix</span>(data.df[<span class="dv">3</span><span class="op">:</span><span class="dv">6</span>])<span class="op">%*%</span>beta.fit)</a>
<a class="sourceLine" id="cb16-3" data-line-number="3">y.pred &lt;-<span class="st"> </span>data.df<span class="op">$</span>n<span class="op">*</span>p</a>
<a class="sourceLine" id="cb16-4" data-line-number="4">y.pred</a></code></pre></div>
<pre><code>##              [,1]
## [1,] 11.321624659
## [2,]  0.644637408
## [3,]  0.002780966
## [4,] 19.732823764
## [5,] 31.289477413
## [6,]  2.819642491
## [7,]  5.510984419</code></pre>
<p>This is similar to the data.</p>

</div>
<!-- </div> -->
            </section>

          </div>
        </div>
      </div>
<a href="3-5-markov-chain-monte-carlo.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="4-ch-statLearn.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
