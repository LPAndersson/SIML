<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>2.3 Bootstrap | Lecture notes for Statistical Inference and Machine Learning</title>
  <meta name="description" content="These are the lecture notes for the course Statistical Inference and Machine Learning at the Department of statistics, Uppsala University." />
  <meta name="generator" content="bookdown 0.20 and GitBook 2.6.7" />

  <meta property="og:title" content="2.3 Bootstrap | Lecture notes for Statistical Inference and Machine Learning" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="These are the lecture notes for the course Statistical Inference and Machine Learning at the Department of statistics, Uppsala University." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="2.3 Bootstrap | Lecture notes for Statistical Inference and Machine Learning" />
  
  <meta name="twitter:description" content="These are the lecture notes for the course Statistical Inference and Machine Learning at the Department of statistics, Uppsala University." />
  

<meta name="author" content="Patrik Andersson" />


<meta name="date" content="2020-09-02" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="2-2-non-parametric-estimation.html"/>
<link rel="next" href="2-4-parametric-bootstrap.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />











<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Inference and Machine Learning</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="1-ch-likelihood.html"><a href="1-ch-likelihood.html"><i class="fa fa-check"></i><b>1</b> Likelihood-based methods</a><ul>
<li class="chapter" data-level="1.1" data-path="1-1-maximum-likelihood-estimation.html"><a href="1-1-maximum-likelihood-estimation.html"><i class="fa fa-check"></i><b>1.1</b> Maximum likelihood estimation</a></li>
<li class="chapter" data-level="1.2" data-path="1-2-hypothesis-testing.html"><a href="1-2-hypothesis-testing.html"><i class="fa fa-check"></i><b>1.2</b> Hypothesis testing</a></li>
<li class="chapter" data-level="1.3" data-path="1-3-likelihood-ratio-test.html"><a href="1-3-likelihood-ratio-test.html"><i class="fa fa-check"></i><b>1.3</b> Likelihood ratio test</a></li>
<li class="chapter" data-level="1.4" data-path="1-4-mathematical-aside-taylor-expansion.html"><a href="1-4-mathematical-aside-taylor-expansion.html"><i class="fa fa-check"></i><b>1.4</b> Mathematical aside: Taylor expansion</a></li>
<li class="chapter" data-level="1.5" data-path="1-5-asymptotic-distribution-of-the-mle.html"><a href="1-5-asymptotic-distribution-of-the-mle.html"><i class="fa fa-check"></i><b>1.5</b> Asymptotic distribution of the MLE</a></li>
<li class="chapter" data-level="1.6" data-path="1-6-the-delta-method.html"><a href="1-6-the-delta-method.html"><i class="fa fa-check"></i><b>1.6</b> The delta method</a></li>
<li class="chapter" data-level="1.7" data-path="1-7-wilks-test.html"><a href="1-7-wilks-test.html"><i class="fa fa-check"></i><b>1.7</b> Wilks’ test</a></li>
<li class="chapter" data-level="1.8" data-path="1-8-walds-test.html"><a href="1-8-walds-test.html"><i class="fa fa-check"></i><b>1.8</b> Wald’s test</a></li>
<li class="chapter" data-level="1.9" data-path="1-9-score-test.html"><a href="1-9-score-test.html"><i class="fa fa-check"></i><b>1.9</b> Score test</a></li>
<li class="chapter" data-level="1.10" data-path="1-10-confidence-intervals.html"><a href="1-10-confidence-intervals.html"><i class="fa fa-check"></i><b>1.10</b> Confidence intervals</a></li>
<li class="chapter" data-level="1.11" data-path="1-11-an-application-i.html"><a href="1-11-an-application-i.html"><i class="fa fa-check"></i><b>1.11</b> An application I</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="2-ch-bootstrap.html"><a href="2-ch-bootstrap.html"><i class="fa fa-check"></i><b>2</b> Bootstrap</a><ul>
<li class="chapter" data-level="2.1" data-path="2-1-parametric-vs-non-parametric.html"><a href="2-1-parametric-vs-non-parametric.html"><i class="fa fa-check"></i><b>2.1</b> Parametric vs non-parametric</a></li>
<li class="chapter" data-level="2.2" data-path="2-2-non-parametric-estimation.html"><a href="2-2-non-parametric-estimation.html"><i class="fa fa-check"></i><b>2.2</b> Non-parametric estimation</a></li>
<li class="chapter" data-level="2.3" data-path="2-3-bootstrap.html"><a href="2-3-bootstrap.html"><i class="fa fa-check"></i><b>2.3</b> Bootstrap</a></li>
<li class="chapter" data-level="2.4" data-path="2-4-parametric-bootstrap.html"><a href="2-4-parametric-bootstrap.html"><i class="fa fa-check"></i><b>2.4</b> Parametric bootstrap</a></li>
<li class="chapter" data-level="2.5" data-path="2-5-an-application-ii.html"><a href="2-5-an-application-ii.html"><i class="fa fa-check"></i><b>2.5</b> An application II</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="3-bayesian-statistics-draft.html"><a href="3-bayesian-statistics-draft.html"><i class="fa fa-check"></i><b>3</b> Bayesian statistics (draft)</a><ul>
<li class="chapter" data-level="3.1" data-path="3-1-some-basic-decision-theory.html"><a href="3-1-some-basic-decision-theory.html"><i class="fa fa-check"></i><b>3.1</b> Some basic decision theory</a></li>
<li class="chapter" data-level="3.2" data-path="3-2-bayesian-statistics.html"><a href="3-2-bayesian-statistics.html"><i class="fa fa-check"></i><b>3.2</b> Bayesian statistics</a></li>
<li class="chapter" data-level="3.3" data-path="3-3-choosing-prior.html"><a href="3-3-choosing-prior.html"><i class="fa fa-check"></i><b>3.3</b> Choosing prior</a></li>
<li class="chapter" data-level="3.4" data-path="3-4-multiparameter-problems.html"><a href="3-4-multiparameter-problems.html"><i class="fa fa-check"></i><b>3.4</b> Multiparameter problems</a></li>
<li class="chapter" data-level="3.5" data-path="3-5-markov-chain-monte-carlo.html"><a href="3-5-markov-chain-monte-carlo.html"><i class="fa fa-check"></i><b>3.5</b> Markov chain monte carlo</a></li>
<li class="chapter" data-level="3.6" data-path="3-6-an-application-iii.html"><a href="3-6-an-application-iii.html"><i class="fa fa-check"></i><b>3.6</b> An application III</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="4-ch-statLearn.html"><a href="4-ch-statLearn.html"><i class="fa fa-check"></i><b>4</b> Statistical learning (draft)</a><ul>
<li class="chapter" data-level="4.1" data-path="4-1-classification.html"><a href="4-1-classification.html"><i class="fa fa-check"></i><b>4.1</b> Classification</a></li>
<li class="chapter" data-level="4.2" data-path="4-2-support-vector-machines-i.html"><a href="4-2-support-vector-machines-i.html"><i class="fa fa-check"></i><b>4.2</b> Support vector machines I</a></li>
<li class="chapter" data-level="4.3" data-path="4-3-hoeffdings-inequality.html"><a href="4-3-hoeffdings-inequality.html"><i class="fa fa-check"></i><b>4.3</b> Hoeffding’s inequality</a></li>
<li class="chapter" data-level="4.4" data-path="4-4-generalization-error.html"><a href="4-4-generalization-error.html"><i class="fa fa-check"></i><b>4.4</b> Generalization error</a></li>
<li class="chapter" data-level="4.5" data-path="4-5-vc-dimension.html"><a href="4-5-vc-dimension.html"><i class="fa fa-check"></i><b>4.5</b> VC-dimension</a></li>
<li class="chapter" data-level="4.6" data-path="4-6-support-vector-machines-ii.html"><a href="4-6-support-vector-machines-ii.html"><i class="fa fa-check"></i><b>4.6</b> Support vector machines II</a></li>
<li class="chapter" data-level="4.7" data-path="4-7-bias-variance-decomposition.html"><a href="4-7-bias-variance-decomposition.html"><i class="fa fa-check"></i><b>4.7</b> Bias-Variance decomposition</a></li>
<li class="chapter" data-level="4.8" data-path="4-8-regression-regularization.html"><a href="4-8-regression-regularization.html"><i class="fa fa-check"></i><b>4.8</b> Regression regularization</a></li>
<li class="chapter" data-level="4.9" data-path="4-9-model-selection.html"><a href="4-9-model-selection.html"><i class="fa fa-check"></i><b>4.9</b> Model selection</a></li>
<li class="chapter" data-level="4.10" data-path="4-10-an-application-iv.html"><a href="4-10-an-application-iv.html"><i class="fa fa-check"></i><b>4.10</b> An application IV</a></li>
<li class="chapter" data-level="4.11" data-path="4-11-an-application-v.html"><a href="4-11-an-application-v.html"><i class="fa fa-check"></i><b>4.11</b> An application V</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="5-beyond-linearity-draft.html"><a href="5-beyond-linearity-draft.html"><i class="fa fa-check"></i><b>5</b> Beyond linearity (draft)</a><ul>
<li class="chapter" data-level="5.1" data-path="5-1-smoothing-splines.html"><a href="5-1-smoothing-splines.html"><i class="fa fa-check"></i><b>5.1</b> Smoothing splines</a></li>
<li class="chapter" data-level="5.2" data-path="5-2-generalized-additive-models.html"><a href="5-2-generalized-additive-models.html"><i class="fa fa-check"></i><b>5.2</b> Generalized additive models</a></li>
<li class="chapter" data-level="5.3" data-path="5-3-neural-networks.html"><a href="5-3-neural-networks.html"><i class="fa fa-check"></i><b>5.3</b> Neural networks</a></li>
<li class="chapter" data-level="5.4" data-path="5-4-stochasitc-gradient-descent.html"><a href="5-4-stochasitc-gradient-descent.html"><i class="fa fa-check"></i><b>5.4</b> Stochasitc gradient descent</a></li>
<li class="chapter" data-level="5.5" data-path="5-5-an-application-vi.html"><a href="5-5-an-application-vi.html"><i class="fa fa-check"></i><b>5.5</b> An application VI</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Lecture notes for Statistical Inference and Machine Learning</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="bootstrap" class="section level2">
<h2><span class="header-section-number">2.3</span> Bootstrap</h2>
<p>In this section we discuss how to calculate standard errors and confidence intervals for a statistic <span class="math inline">\(T(X_1,X_2,\ldots, X_n)\)</span>. But first let us take a step back and consider the basics of what we are doing.</p>
<p>We are given a sample <span class="math inline">\((x_1,\ldots, x_n)\)</span>. We have some function that we apply to the sample <span class="math inline">\(T\)</span> which we call a statistic. This may be a simple function like the sample mean, or some more complicated function like a parameter in a regression problem. We apply the function to the data and get a number <span class="math inline">\(t=T(x_1,\ldots,x_n)\)</span>. To do inference we also need to know the variability of that number if we were to repeat the same experiment many times. We therefore consider the random variable <span class="math inline">\(T_n := T(X_1,\ldots, X_n)\)</span>, where <span class="math inline">\(X_i\sim F\)</span> iid. The distribution of this random variable is called the sampling distribution. In some cases, for a particular <span class="math inline">\(T\)</span> and <span class="math inline">\(F\)</span> we may be able to find the sampling distribution either exactly or approximately. For example, we previously found the asymptotic sampling distribution of the log-likelihood. Once we have that, we can do hypothesis testing, construct confidence intervals or simply state the standard deviation. The procedure can be summarized as:
<span class="math display">\[
F\overset{sample}{\to} x \overset{T}{\to} t.
\]</span></p>
<p>However, in many cases the calculation of the sampling distribution is not possible and one way to instead approximate the distribution is through simulation. For now, let us assume that we know the distribution <span class="math inline">\(F\)</span>. Then it may be possible to draw <span class="math inline">\(n\)</span> iid copies of the random variable <span class="math inline">\(X_1^\star,\ldots X^\star_n \sim F\)</span> using a computer. Then calculate <span class="math inline">\(t^\star = T(X_1^\star,\ldots X^\star_n)\)</span>. Repeat this <span class="math inline">\(B\)</span> times to obtain <span class="math inline">\(t_1^\star,\ldots, t_B^\star\)</span>.</p>
<p>If we have <span class="math inline">\(B\)</span> such copies, by the large of large numbers, as <span class="math inline">\(B\to \infty\)</span>,
<span class="math display">\[
\frac{1}{B}\sum_{i=1}^B f(t_i^\star)\overset{p}\to  E\left[ f(T_n) \right].
\]</span>
So that in practice, we would approximate the right hand side with the left hand side. The most important example is the variance:
<span class="math display">\[
Var\left( T_n \right) = E\left[T_n^2\right] - E\left[T_n\right]^2 \approx \frac{1}{B}\sum_{i=1}^B (t_i^\star)^2 - \left( \frac{1}{B}\sum_{i=1}^B t_i^\star \right)^2 = \frac{1}{B}\sum_{i=1}^B \left( t_i^\star - \bar{t^\star} \right)^2
\]</span></p>
<p>As an example, let us consider <span class="math inline">\(X_1,\ldots, X_{n}\overset{iid}\sim \mathsf{Exp}(1)\)</span>. We would like to know the distribution of the sample median and in particular the expected value and variance.</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb4-1" data-line-number="1">n &lt;-<span class="st"> </span><span class="dv">100</span></a>
<a class="sourceLine" id="cb4-2" data-line-number="2">B &lt;-<span class="st"> </span><span class="dv">1000</span></a>
<a class="sourceLine" id="cb4-3" data-line-number="3"></a>
<a class="sourceLine" id="cb4-4" data-line-number="4">T &lt;-<span class="st"> </span>median</a>
<a class="sourceLine" id="cb4-5" data-line-number="5"></a>
<a class="sourceLine" id="cb4-6" data-line-number="6">tstar &lt;-<span class="st"> </span><span class="kw">array</span>(<span class="dt">dim =</span> B)</a>
<a class="sourceLine" id="cb4-7" data-line-number="7"></a>
<a class="sourceLine" id="cb4-8" data-line-number="8"><span class="cf">for</span> (i <span class="cf">in</span> <span class="kw">seq_len</span>(B)) {</a>
<a class="sourceLine" id="cb4-9" data-line-number="9">  x &lt;-<span class="st"> </span><span class="kw">rexp</span>(n, <span class="dt">rate =</span> <span class="dv">1</span>)</a>
<a class="sourceLine" id="cb4-10" data-line-number="10">  tstar[i] &lt;-<span class="st"> </span><span class="kw">T</span>(x)</a>
<a class="sourceLine" id="cb4-11" data-line-number="11">}</a>
<a class="sourceLine" id="cb4-12" data-line-number="12"></a>
<a class="sourceLine" id="cb4-13" data-line-number="13"><span class="kw">mean</span>(tstar)</a></code></pre></div>
<pre><code>## [1] 0.6964009</code></pre>
<div class="sourceCode" id="cb6"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb6-1" data-line-number="1"><span class="kw">var</span>(tstar)</a></code></pre></div>
<pre><code>## [1] 0.01063886</code></pre>
<p>In practice this is not helpful since we rarely know <span class="math inline">\(F\)</span>. What we have is <span class="math inline">\(\hat F_n\)</span>, so let us approximate <span class="math inline">\(F\approx \hat F_n\)</span> and therefore also <span class="math inline">\(Var_F(T_n) \approx Var_{\hat F_n}(T_n)\)</span>. The right hand side here is then the variance of <span class="math inline">\(T(X_1,\ldots , X_n)\)</span>, where <span class="math inline">\(X_i\)</span> is iid and distributed as <span class="math inline">\(\hat F_n\)</span>. Then we proceed as above: Draw <span class="math inline">\(n\)</span> iid copies of the random variable <span class="math inline">\(X^\star_1,\ldots X^\star_n \sim \hat F_n\)</span> using a computer. Then calculate <span class="math inline">\(t^\star = T(X_1^\star,\ldots X^\star_n)\)</span>. Repeat this <span class="math inline">\(B\)</span> times to obtain <span class="math inline">\(t_1^\star,\ldots, t_B^\star\)</span>. Here, since <span class="math inline">\(\hat F_n\)</span> is the empirical distribution, drawing from <span class="math inline">\(\hat F_n\)</span> means simply to draw an observation at random from the original data set. In contrast to the above, this method can be summarized as:
<span class="math display">\[
\hat F_n\overset{sample}{\to} x^\star \overset{T}{\to} t^\star.
\]</span></p>
<p>This method is called the bootstrap.</p>

<div class="note">
<p>Bootstrap variance estimation:</p>
<ol style="list-style-type: decimal">
<li>Draw <span class="math inline">\(X_1^\star,\ldots, X_n^\star \sim \hat F_n\)</span>.</li>
<li>Compute <span class="math inline">\(t^\star = T(X_1^\star,\ldots, X_n^\star)\)</span>.</li>
<li>Repeat steps 1 and 2, <span class="math inline">\(B\)</span> times, to get <span class="math inline">\(t_{1}^\star,\ldots,t_{B}^\star\)</span></li>
<li>Approximate <span class="math inline">\(Var_F(T_n)\)</span> by
<span class="math display">\[
  v_{boot} = \frac{1}{B}\sum_{b=1}^B\left( t^\star_{b} - \frac{1}{B}\sum_{r=1}^B t^\star_{r} \right)^2
\]</span>
</div></li>
</ol>
<p>Let us implement this method to calculate the standard error of the median:</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb8-1" data-line-number="1">T &lt;-<span class="st"> </span>median</a>
<a class="sourceLine" id="cb8-2" data-line-number="2">n &lt;-<span class="st"> </span><span class="kw">nrow</span>(data.df)</a>
<a class="sourceLine" id="cb8-3" data-line-number="3">B &lt;-<span class="st"> </span><span class="dv">1000</span></a>
<a class="sourceLine" id="cb8-4" data-line-number="4"></a>
<a class="sourceLine" id="cb8-5" data-line-number="5"><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>B) {</a>
<a class="sourceLine" id="cb8-6" data-line-number="6">  indices &lt;-<span class="st"> </span><span class="kw">sample</span>(<span class="kw">seq_len</span>(n), <span class="dt">size =</span> n, <span class="dt">replace =</span> <span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb8-7" data-line-number="7">  tstar[i] &lt;-<span class="st"> </span><span class="kw">T</span>(data.df<span class="op">$</span>x[indices])</a>
<a class="sourceLine" id="cb8-8" data-line-number="8">}</a>
<a class="sourceLine" id="cb8-9" data-line-number="9"></a>
<a class="sourceLine" id="cb8-10" data-line-number="10"><span class="kw">sd</span>(tstar)</a></code></pre></div>
<pre><code>## [1] 0.1564483</code></pre>
<p>In practice we would rather use the boot library.</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb10-1" data-line-number="1"><span class="kw">library</span>(boot)</a>
<a class="sourceLine" id="cb10-2" data-line-number="2"><span class="kw">boot</span>(<span class="dt">data =</span> data.df, </a>
<a class="sourceLine" id="cb10-3" data-line-number="3">     <span class="dt">statistic =</span> <span class="cf">function</span>(data, index){ <span class="kw">T</span>(data<span class="op">$</span>x[index]) }, </a>
<a class="sourceLine" id="cb10-4" data-line-number="4">     <span class="dt">R =</span> <span class="dv">1000</span>)</a></code></pre></div>
<pre><code>## 
## ORDINARY NONPARAMETRIC BOOTSTRAP
## 
## 
## Call:
## boot(data = data.df, statistic = function(data, index) {
##     T(data$x[index])
## }, R = 1000)
## 
## 
## Bootstrap Statistics :
##       original     bias    std. error
## t1* 0.08979677 0.03632407   0.1475442</code></pre>
<p>Since here we know <span class="math inline">\(F\)</span> we can simulate the true standard deviation, an alternative not available in practice.</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb12-1" data-line-number="1">n &lt;-<span class="st"> </span><span class="kw">nrow</span>(data.df)</a>
<a class="sourceLine" id="cb12-2" data-line-number="2">B &lt;-<span class="st"> </span><span class="dv">1000</span></a>
<a class="sourceLine" id="cb12-3" data-line-number="3">Tsim &lt;-<span class="st"> </span><span class="kw">array</span>(<span class="dt">dim =</span> B)</a>
<a class="sourceLine" id="cb12-4" data-line-number="4"></a>
<a class="sourceLine" id="cb12-5" data-line-number="5"><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>B) {</a>
<a class="sourceLine" id="cb12-6" data-line-number="6">  simData.df &lt;-<span class="st"> </span><span class="kw">data.frame</span>( <span class="dt">x =</span> <span class="kw">rnorm</span>(n) )</a>
<a class="sourceLine" id="cb12-7" data-line-number="7">  Tsim[i] &lt;-<span class="st"> </span><span class="kw">T</span>(simData.df<span class="op">$</span>x)</a>
<a class="sourceLine" id="cb12-8" data-line-number="8">}</a>
<a class="sourceLine" id="cb12-9" data-line-number="9"></a>
<a class="sourceLine" id="cb12-10" data-line-number="10"><span class="kw">sd</span>(Tsim) <span class="co">#Standard error of median</span></a></code></pre></div>
<pre><code>## [1] 0.1253581</code></pre>
<p>It is also possible to construct confidence intervals using bootstrap. Here we present bootstrap pivotal confidence intervals, sometimes known as basic bootstrap intervals.</p>
<p>Let us call <span class="math inline">\(\theta = T(F)\)</span> and <span class="math inline">\(\hat\theta_n = T(\hat F_n)\)</span> and define the pivot <span class="math inline">\(R_n = \hat\theta_n - \theta\)</span>. Write the distribution function of <span class="math inline">\(R_n\)</span> as:
<span class="math display">\[
H(r) := P(R_n\leq r).
\]</span>
Then,
<span class="math display">\[\begin{align}
&amp;P\left(\hat\theta_n - H^{-1}(1-\alpha/2)\leq \theta \leq \hat\theta_n - H^{-1}(\alpha/2)\right) \\
=&amp; P\left( H^{-1}(\alpha/2) \leq \hat\theta_n - \theta \leq  H^{-1}(1-\alpha/2) \right)\\
=&amp; P\left( H^{-1}(\alpha/2) \leq R_n \leq  H^{-1}(1-\alpha/2) \right)\\
=&amp; H\left( H^{-1}(1-\alpha/2) \right) - H\left( H^{-1}(\alpha/2) \right)\\
=&amp; 1-\frac{\alpha}{2}-\frac{\alpha}{2} = 1-\alpha,
\end{align}\]</span>
proving that
<span class="math display">\[
\left[\hat\theta_n - H^{-1}(1-\alpha/2) , \hat\theta_n - H^{-1}(\alpha/2) \right]
\]</span>
is a <span class="math inline">\(1-\alpha\)</span> CI for <span class="math inline">\(\theta\)</span>.</p>
<p>Now, since <span class="math inline">\(H\)</span> is unknown this is not practical. We can however construct a bootstrap estimate of <span class="math inline">\(H\)</span> by drawing <span class="math inline">\(R^\star_{n,b} = \hat\theta^\star_{n,b}- \hat\theta_n\)</span> and defining:
<span class="math display">\[
\hat H(r) = \frac{1}{B}\sum_{b=1}^B I(R^\star_{n,b}\leq r).
\]</span>
The quantity <span class="math inline">\(H^{-1}(\alpha)\)</span> is the <span class="math inline">\(\alpha\)</span> quantile of <span class="math inline">\(H\)</span>. In the CI we replace <span class="math inline">\(H^{-1}\)</span> by <span class="math inline">\(\hat H^{-1}\)</span>, i.e. the <span class="math inline">\(\alpha\)</span> quantile of <span class="math inline">\(R^\star_{n,b}\)</span>. Since <span class="math inline">\(\hat \theta_n\)</span> is fixed in the bootstrap sample, the <span class="math inline">\(\alpha\)</span> quantile of <span class="math inline">\(R^\star_{n,b}\)</span> is simply <span class="math inline">\(\theta_\alpha^\star - \hat\theta_n\)</span>, where <span class="math inline">\(\theta_\alpha^\star\)</span> denotes the <span class="math inline">\(\alpha\)</span> quantile of <span class="math inline">\(\hat\theta^\star_{n,b}\)</span>.</p>
<p>Finally we get that the <span class="math inline">\(1-\alpha\)</span> bootstrap pivotal CI is
<span class="math display">\[
C_n = \left[ \hat\theta_n - (\theta^\star_{1-\alpha/2} - \hat\theta_n) , \hat\theta_n - (\theta^\star_{\alpha/2} - \hat\theta_n) \right] = \left[ 2\hat\theta_n - \theta^\star_{1-\alpha/2} , 2\hat\theta_n -\theta^\star_{\alpha/2}  \right]
\]</span>
Let us implement this on the same data set as above.</p>
<div class="sourceCode" id="cb14"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb14-1" data-line-number="1">alpha &lt;-<span class="st"> </span><span class="fl">0.05</span></a>
<a class="sourceLine" id="cb14-2" data-line-number="2">T &lt;-<span class="st"> </span>median</a>
<a class="sourceLine" id="cb14-3" data-line-number="3">n &lt;-<span class="st"> </span><span class="kw">nrow</span>(data.df)</a>
<a class="sourceLine" id="cb14-4" data-line-number="4">B &lt;-<span class="st"> </span><span class="dv">1000</span></a>
<a class="sourceLine" id="cb14-5" data-line-number="5">tstar &lt;-<span class="st"> </span><span class="kw">array</span>(<span class="dt">dim =</span> B)</a>
<a class="sourceLine" id="cb14-6" data-line-number="6"></a>
<a class="sourceLine" id="cb14-7" data-line-number="7"><span class="cf">for</span> (i <span class="cf">in</span> <span class="kw">seq_len</span>(B)) {</a>
<a class="sourceLine" id="cb14-8" data-line-number="8">  indices &lt;-<span class="st"> </span><span class="kw">sample</span>(<span class="kw">seq</span>(<span class="dv">1</span><span class="op">:</span>n), <span class="dt">size =</span> n, <span class="dt">replace =</span> <span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb14-9" data-line-number="9">  tstar[i] &lt;-<span class="st"> </span><span class="kw">T</span>(data.df<span class="op">$</span>x[indices])</a>
<a class="sourceLine" id="cb14-10" data-line-number="10">}</a>
<a class="sourceLine" id="cb14-11" data-line-number="11"></a>
<a class="sourceLine" id="cb14-12" data-line-number="12">q &lt;-<span class="st"> </span><span class="kw">unname</span>( <span class="kw">quantile</span>(tstar, <span class="dt">probs =</span> <span class="kw">c</span>(<span class="dv">1</span><span class="op">-</span>alpha<span class="op">/</span><span class="dv">2</span>, alpha<span class="op">/</span><span class="dv">2</span>)) )</a>
<a class="sourceLine" id="cb14-13" data-line-number="13"></a>
<a class="sourceLine" id="cb14-14" data-line-number="14">lowerCI &lt;-<span class="st"> </span><span class="dv">2</span><span class="op">*</span><span class="kw">T</span>(data.df<span class="op">$</span>x) <span class="op">-</span><span class="st"> </span>q[<span class="dv">1</span>]</a>
<a class="sourceLine" id="cb14-15" data-line-number="15">lowerCI</a></code></pre></div>
<pre><code>## [1] -0.2250317</code></pre>
<div class="sourceCode" id="cb16"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb16-1" data-line-number="1">upperCI &lt;-<span class="st"> </span><span class="dv">2</span><span class="op">*</span><span class="kw">T</span>(data.df<span class="op">$</span>x) <span class="op">-</span><span class="st"> </span>q[<span class="dv">2</span>]</a>
<a class="sourceLine" id="cb16-2" data-line-number="2">upperCI</a></code></pre></div>
<pre><code>## [1] 0.3515109</code></pre>
<p>Even simpler is to use the boot library.</p>
<div class="sourceCode" id="cb18"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb18-1" data-line-number="1"><span class="kw">library</span>(boot)</a>
<a class="sourceLine" id="cb18-2" data-line-number="2">boot.result&lt;-<span class="st"> </span><span class="kw">boot</span>(<span class="dt">data =</span> data.df, </a>
<a class="sourceLine" id="cb18-3" data-line-number="3">                   <span class="dt">statistic =</span> <span class="cf">function</span>(data, index) <span class="kw">T</span>(data<span class="op">$</span>x[index]), </a>
<a class="sourceLine" id="cb18-4" data-line-number="4">                   <span class="dt">R =</span> <span class="dv">1000</span>)</a>
<a class="sourceLine" id="cb18-5" data-line-number="5"></a>
<a class="sourceLine" id="cb18-6" data-line-number="6"><span class="kw">boot.ci</span>(boot.result, <span class="dt">type =</span> <span class="st">&quot;basic&quot;</span>)</a></code></pre></div>
<pre><code>## BOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS
## Based on 1000 bootstrap replicates
## 
## CALL : 
## boot.ci(boot.out = boot.result, type = &quot;basic&quot;)
## 
## Intervals : 
## Level      Basic         
## 95%   (-0.2389,  0.3129 )  
## Calculations and Intervals on Original Scale</code></pre>
<p>A better alternative which we do not cover in this course is the bias-corrected CI. It is however just as easy to use.</p>
<div class="sourceCode" id="cb20"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb20-1" data-line-number="1"><span class="kw">library</span>(boot)</a>
<a class="sourceLine" id="cb20-2" data-line-number="2">boot.result&lt;-<span class="st"> </span><span class="kw">boot</span>(<span class="dt">data =</span> data.df, </a>
<a class="sourceLine" id="cb20-3" data-line-number="3">                   <span class="dt">statistic =</span> <span class="cf">function</span>(data, index) <span class="kw">T</span>(data<span class="op">$</span>x[index]), </a>
<a class="sourceLine" id="cb20-4" data-line-number="4">                   <span class="dt">R =</span> <span class="dv">1000</span>)</a>
<a class="sourceLine" id="cb20-5" data-line-number="5"></a>
<a class="sourceLine" id="cb20-6" data-line-number="6"><span class="kw">boot.ci</span>(boot.result, <span class="dt">type =</span> <span class="st">&quot;bca&quot;</span>)</a></code></pre></div>
<pre><code>## BOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS
## Based on 1000 bootstrap replicates
## 
## CALL : 
## boot.ci(boot.out = boot.result, type = &quot;bca&quot;)
## 
## Intervals : 
## Level       BCa          
## 95%   (-0.1333,  0.3701 )  
## Calculations and Intervals on Original Scale</code></pre>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="2-2-non-parametric-estimation.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="2-4-parametric-bootstrap.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
