<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>4.8 Regression regularization | Lecture notes for Statistical Inference and Machine Learning</title>
  <meta name="description" content="These are the lecture notes for the course Statistical Inference and Machine Learning at the Department of statistics, Uppsala University." />
  <meta name="generator" content="bookdown 0.21 and GitBook 2.6.7" />

  <meta property="og:title" content="4.8 Regression regularization | Lecture notes for Statistical Inference and Machine Learning" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="These are the lecture notes for the course Statistical Inference and Machine Learning at the Department of statistics, Uppsala University." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="4.8 Regression regularization | Lecture notes for Statistical Inference and Machine Learning" />
  
  <meta name="twitter:description" content="These are the lecture notes for the course Statistical Inference and Machine Learning at the Department of statistics, Uppsala University." />
  

<meta name="author" content="Patrik Andersson" />


<meta name="date" content="2021-01-16" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="4-7-bias-variance-decomposition.html"/>
<link rel="next" href="4-9-model-selection.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>
<link href="libs/anchor-sections-1.0/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0/anchor-sections.js"></script>


<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Inference and Machine Learning</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="1-ch-likelihood.html"><a href="1-ch-likelihood.html"><i class="fa fa-check"></i><b>1</b> Likelihood-based methods</a><ul>
<li class="chapter" data-level="1.1" data-path="1-1-maximum-likelihood-estimation.html"><a href="1-1-maximum-likelihood-estimation.html"><i class="fa fa-check"></i><b>1.1</b> Maximum likelihood estimation</a></li>
<li class="chapter" data-level="1.2" data-path="1-2-hypothesis-testing.html"><a href="1-2-hypothesis-testing.html"><i class="fa fa-check"></i><b>1.2</b> Hypothesis testing</a></li>
<li class="chapter" data-level="1.3" data-path="1-3-likelihood-ratio-test.html"><a href="1-3-likelihood-ratio-test.html"><i class="fa fa-check"></i><b>1.3</b> Likelihood ratio test</a></li>
<li class="chapter" data-level="1.4" data-path="1-4-mathematical-aside-taylor-expansion.html"><a href="1-4-mathematical-aside-taylor-expansion.html"><i class="fa fa-check"></i><b>1.4</b> Mathematical aside: Taylor expansion</a></li>
<li class="chapter" data-level="1.5" data-path="1-5-asymptotic-distribution-of-the-mle.html"><a href="1-5-asymptotic-distribution-of-the-mle.html"><i class="fa fa-check"></i><b>1.5</b> Asymptotic distribution of the MLE</a></li>
<li class="chapter" data-level="1.6" data-path="1-6-the-delta-method.html"><a href="1-6-the-delta-method.html"><i class="fa fa-check"></i><b>1.6</b> The delta method</a></li>
<li class="chapter" data-level="1.7" data-path="1-7-wilks-test.html"><a href="1-7-wilks-test.html"><i class="fa fa-check"></i><b>1.7</b> Wilks’ test</a></li>
<li class="chapter" data-level="1.8" data-path="1-8-walds-test.html"><a href="1-8-walds-test.html"><i class="fa fa-check"></i><b>1.8</b> Wald’s test</a></li>
<li class="chapter" data-level="1.9" data-path="1-9-score-test.html"><a href="1-9-score-test.html"><i class="fa fa-check"></i><b>1.9</b> Score test</a></li>
<li class="chapter" data-level="1.10" data-path="1-10-confidence-intervals.html"><a href="1-10-confidence-intervals.html"><i class="fa fa-check"></i><b>1.10</b> Confidence intervals</a></li>
<li class="chapter" data-level="1.11" data-path="1-11-an-application.html"><a href="1-11-an-application.html"><i class="fa fa-check"></i><b>1.11</b> An application</a></li>
<li class="chapter" data-level="1.12" data-path="1-12-summary.html"><a href="1-12-summary.html"><i class="fa fa-check"></i><b>1.12</b> Summary</a></li>
<li class="chapter" data-level="1.13" data-path="1-13-review-questions.html"><a href="1-13-review-questions.html"><i class="fa fa-check"></i><b>1.13</b> Review questions</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="2-bayesian-statistics-draft.html"><a href="2-bayesian-statistics-draft.html"><i class="fa fa-check"></i><b>2</b> Bayesian statistics (draft)</a><ul>
<li class="chapter" data-level="2.1" data-path="2-1-some-basic-decision-theory.html"><a href="2-1-some-basic-decision-theory.html"><i class="fa fa-check"></i><b>2.1</b> Some basic decision theory</a></li>
<li class="chapter" data-level="2.2" data-path="2-2-bayesian-statistics.html"><a href="2-2-bayesian-statistics.html"><i class="fa fa-check"></i><b>2.2</b> Bayesian statistics</a></li>
<li class="chapter" data-level="2.3" data-path="2-3-choosing-prior.html"><a href="2-3-choosing-prior.html"><i class="fa fa-check"></i><b>2.3</b> Choosing prior</a></li>
<li class="chapter" data-level="2.4" data-path="2-4-multiparameter-problems.html"><a href="2-4-multiparameter-problems.html"><i class="fa fa-check"></i><b>2.4</b> Multiparameter problems</a></li>
<li class="chapter" data-level="2.5" data-path="2-5-markov-chain-monte-carlo.html"><a href="2-5-markov-chain-monte-carlo.html"><i class="fa fa-check"></i><b>2.5</b> Markov chain Monte Carlo</a></li>
<li class="chapter" data-level="2.6" data-path="2-6-an-application-1.html"><a href="2-6-an-application-1.html"><i class="fa fa-check"></i><b>2.6</b> An application</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="3-ch-bootstrap.html"><a href="3-ch-bootstrap.html"><i class="fa fa-check"></i><b>3</b> Bootstrap (draft)</a><ul>
<li class="chapter" data-level="3.1" data-path="3-1-parametric-vs-non-parametric.html"><a href="3-1-parametric-vs-non-parametric.html"><i class="fa fa-check"></i><b>3.1</b> Parametric vs non-parametric</a></li>
<li class="chapter" data-level="3.2" data-path="3-2-non-parametric-estimation.html"><a href="3-2-non-parametric-estimation.html"><i class="fa fa-check"></i><b>3.2</b> Non-parametric estimation</a></li>
<li class="chapter" data-level="3.3" data-path="3-3-bootstrap.html"><a href="3-3-bootstrap.html"><i class="fa fa-check"></i><b>3.3</b> Bootstrap</a></li>
<li class="chapter" data-level="3.4" data-path="3-4-parametric-bootstrap.html"><a href="3-4-parametric-bootstrap.html"><i class="fa fa-check"></i><b>3.4</b> Parametric bootstrap</a></li>
<li class="chapter" data-level="3.5" data-path="3-5-an-application-2.html"><a href="3-5-an-application-2.html"><i class="fa fa-check"></i><b>3.5</b> An application</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="4-ch-statLearn.html"><a href="4-ch-statLearn.html"><i class="fa fa-check"></i><b>4</b> Statistical learning (draft)</a><ul>
<li class="chapter" data-level="4.1" data-path="4-1-classification.html"><a href="4-1-classification.html"><i class="fa fa-check"></i><b>4.1</b> Classification</a></li>
<li class="chapter" data-level="4.2" data-path="4-2-support-vector-machines-i.html"><a href="4-2-support-vector-machines-i.html"><i class="fa fa-check"></i><b>4.2</b> Support vector machines I</a></li>
<li class="chapter" data-level="4.3" data-path="4-3-hoeffdings-inequality.html"><a href="4-3-hoeffdings-inequality.html"><i class="fa fa-check"></i><b>4.3</b> Hoeffding’s inequality</a></li>
<li class="chapter" data-level="4.4" data-path="4-4-generalization-error.html"><a href="4-4-generalization-error.html"><i class="fa fa-check"></i><b>4.4</b> Generalization error</a></li>
<li class="chapter" data-level="4.5" data-path="4-5-vc-dimension.html"><a href="4-5-vc-dimension.html"><i class="fa fa-check"></i><b>4.5</b> VC-dimension</a></li>
<li class="chapter" data-level="4.6" data-path="4-6-support-vector-machines-ii.html"><a href="4-6-support-vector-machines-ii.html"><i class="fa fa-check"></i><b>4.6</b> Support vector machines II</a></li>
<li class="chapter" data-level="4.7" data-path="4-7-bias-variance-decomposition.html"><a href="4-7-bias-variance-decomposition.html"><i class="fa fa-check"></i><b>4.7</b> Bias-Variance decomposition</a></li>
<li class="chapter" data-level="4.8" data-path="4-8-regression-regularization.html"><a href="4-8-regression-regularization.html"><i class="fa fa-check"></i><b>4.8</b> Regression regularization</a></li>
<li class="chapter" data-level="4.9" data-path="4-9-model-selection.html"><a href="4-9-model-selection.html"><i class="fa fa-check"></i><b>4.9</b> Model selection</a></li>
<li class="chapter" data-level="4.10" data-path="4-10-an-application-i.html"><a href="4-10-an-application-i.html"><i class="fa fa-check"></i><b>4.10</b> An application I</a></li>
<li class="chapter" data-level="4.11" data-path="4-11-an-application-ii.html"><a href="4-11-an-application-ii.html"><i class="fa fa-check"></i><b>4.11</b> An application II</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="5-beyond-linearity-draft.html"><a href="5-beyond-linearity-draft.html"><i class="fa fa-check"></i><b>5</b> Beyond linearity (draft)</a><ul>
<li class="chapter" data-level="5.1" data-path="5-1-smoothing-splines.html"><a href="5-1-smoothing-splines.html"><i class="fa fa-check"></i><b>5.1</b> Smoothing splines</a></li>
<li class="chapter" data-level="5.2" data-path="5-2-generalized-additive-models.html"><a href="5-2-generalized-additive-models.html"><i class="fa fa-check"></i><b>5.2</b> Generalized additive models</a></li>
<li class="chapter" data-level="5.3" data-path="5-3-neural-networks.html"><a href="5-3-neural-networks.html"><i class="fa fa-check"></i><b>5.3</b> Neural networks</a></li>
<li class="chapter" data-level="5.4" data-path="5-4-stochastic-gradient-descent.html"><a href="5-4-stochastic-gradient-descent.html"><i class="fa fa-check"></i><b>5.4</b> Stochastic gradient descent</a></li>
<li class="chapter" data-level="5.5" data-path="5-5-an-application-3.html"><a href="5-5-an-application-3.html"><i class="fa fa-check"></i><b>5.5</b> An application</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Lecture notes for Statistical Inference and Machine Learning</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="regression-regularization" class="section level2">
<h2><span class="header-section-number">4.8</span> Regression regularization</h2>
<p>In this section we discuss regularization in the context of regression as a way of controlling overfit.</p>
<p>Recall that in regression we have observations from <span class="math inline">\(\mathbb R^p\)</span> and our task is to predict a value in <span class="math inline">\(\mathbb R\)</span>. We will do this by finding a function <span class="math inline">\(h\in \mathcal H\)</span> that gives a small out-of-sample error
<span class="math display">\[
E_{out} = E\left[ (Y-h(X))^2 \right].
\]</span>
In linear regression, the collection of functions <span class="math inline">\(\mathcal H\)</span> are the functions of the form
<span class="math display">\[
h(x) = \beta_1x_1 + \cdots + \beta_p x_p,
\]</span>
and we are thus to choose the parameters <span class="math inline">\(\beta_1,\ldots,\beta_p\)</span> in a good way. We know from before that the choice that minimizes the in-sample error is
<span class="math display">\[
\beta = (x^Tx)^{-1}x^Ty.
\]</span>
As we have seen, we may specify some function <span class="math inline">\(\varphi_i\)</span> and use <span class="math inline">\(\varphi_1(x),\ldots,\varphi_M(x)\)</span> to predict <span class="math inline">\(y\)</span> and we are still in the case of linear regression. However we need to be careful not to overfit. Let us see a simple example were <span class="math inline">\(\varphi\)</span> are polynomials. First we construct the functions that will generate our data:</p>
<div class="sourceCode" id="cb131"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb131-1"><a href="4-8-regression-regularization.html#cb131-1"></a><span class="kw">library</span>(ggplot2)</span>
<span id="cb131-2"><a href="4-8-regression-regularization.html#cb131-2"></a><span class="kw">library</span>(gridExtra)</span>
<span id="cb131-3"><a href="4-8-regression-regularization.html#cb131-3"></a><span class="kw">library</span>(resample)</span>
<span id="cb131-4"><a href="4-8-regression-regularization.html#cb131-4"></a></span>
<span id="cb131-5"><a href="4-8-regression-regularization.html#cb131-5"></a>gen_data &lt;-<span class="st"> </span><span class="cf">function</span>(truth, <span class="dt">n.obs =</span> <span class="dv">100</span>) {</span>
<span id="cb131-6"><a href="4-8-regression-regularization.html#cb131-6"></a>  x &lt;-<span class="st"> </span><span class="kw">runif</span>(n.obs, <span class="dt">min =</span> <span class="dv">-2</span>, <span class="dt">max =</span> <span class="dv">2</span>)</span>
<span id="cb131-7"><a href="4-8-regression-regularization.html#cb131-7"></a>  y &lt;-<span class="st"> </span><span class="kw">truth</span>(x) <span class="op">+</span><span class="st"> </span><span class="kw">rnorm</span>(n.obs) <span class="op">*</span><span class="st"> </span><span class="fl">0.4</span></span>
<span id="cb131-8"><a href="4-8-regression-regularization.html#cb131-8"></a>  data.df &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">y =</span> y, <span class="dt">x =</span> x)</span>
<span id="cb131-9"><a href="4-8-regression-regularization.html#cb131-9"></a>}</span>
<span id="cb131-10"><a href="4-8-regression-regularization.html#cb131-10"></a></span>
<span id="cb131-11"><a href="4-8-regression-regularization.html#cb131-11"></a>truth &lt;-<span class="st"> </span><span class="cf">function</span>(x) {</span>
<span id="cb131-12"><a href="4-8-regression-regularization.html#cb131-12"></a>  <span class="kw">sin</span>(x <span class="op">*</span><span class="st"> </span><span class="dv">2</span> <span class="op">+</span><span class="st"> </span><span class="dv">1</span>) <span class="op">+</span><span class="st"> </span>x</span>
<span id="cb131-13"><a href="4-8-regression-regularization.html#cb131-13"></a>}</span></code></pre></div>
<p>For convenience, we write a function that fits three different polynomial regressions and returns the plot.</p>
<div class="sourceCode" id="cb132"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb132-1"><a href="4-8-regression-regularization.html#cb132-1"></a>fit_plot &lt;-<span class="st"> </span><span class="cf">function</span>(){</span>
<span id="cb132-2"><a href="4-8-regression-regularization.html#cb132-2"></a>  data.train.df &lt;-<span class="st"> </span><span class="kw">gen_data</span>(truth)</span>
<span id="cb132-3"><a href="4-8-regression-regularization.html#cb132-3"></a></span>
<span id="cb132-4"><a href="4-8-regression-regularization.html#cb132-4"></a>  x.grid &lt;-<span class="st"> </span><span class="kw">seq</span>(<span class="op">-</span><span class="dv">2</span>, <span class="dv">2</span>, <span class="fl">0.01</span>)</span>
<span id="cb132-5"><a href="4-8-regression-regularization.html#cb132-5"></a>  x.grid.df &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">x =</span> x.grid)</span>
<span id="cb132-6"><a href="4-8-regression-regularization.html#cb132-6"></a></span>
<span id="cb132-7"><a href="4-8-regression-regularization.html#cb132-7"></a>  fit1 &lt;-<span class="st"> </span><span class="kw">predict</span>(<span class="kw">lm</span>(y <span class="op">~</span><span class="st"> </span><span class="kw">poly</span>(x, <span class="dv">2</span>), data.train.df),</span>
<span id="cb132-8"><a href="4-8-regression-regularization.html#cb132-8"></a>                  x.grid.df)</span>
<span id="cb132-9"><a href="4-8-regression-regularization.html#cb132-9"></a>  fit2 &lt;-<span class="st"> </span><span class="kw">predict</span>(<span class="kw">lm</span>(y <span class="op">~</span><span class="st"> </span><span class="kw">poly</span>(x, <span class="dv">6</span>), data.train.df),</span>
<span id="cb132-10"><a href="4-8-regression-regularization.html#cb132-10"></a>                  x.grid.df)</span>
<span id="cb132-11"><a href="4-8-regression-regularization.html#cb132-11"></a>  fit3 &lt;-<span class="st"> </span><span class="kw">predict</span>(<span class="kw">lm</span>(y <span class="op">~</span><span class="st"> </span><span class="kw">poly</span>(x, <span class="dv">20</span>), data.train.df),</span>
<span id="cb132-12"><a href="4-8-regression-regularization.html#cb132-12"></a>                  x.grid.df)</span>
<span id="cb132-13"><a href="4-8-regression-regularization.html#cb132-13"></a></span>
<span id="cb132-14"><a href="4-8-regression-regularization.html#cb132-14"></a>  cbp1 &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;#999999&quot;</span>, <span class="st">&quot;#E69F00&quot;</span>, <span class="st">&quot;#56B4E9&quot;</span>, <span class="st">&quot;#009E73&quot;</span>,</span>
<span id="cb132-15"><a href="4-8-regression-regularization.html#cb132-15"></a>            <span class="st">&quot;#F0E442&quot;</span>, <span class="st">&quot;#0072B2&quot;</span>, <span class="st">&quot;#D55E00&quot;</span>, <span class="st">&quot;#CC79A7&quot;</span>)</span>
<span id="cb132-16"><a href="4-8-regression-regularization.html#cb132-16"></a></span>
<span id="cb132-17"><a href="4-8-regression-regularization.html#cb132-17"></a>  p &lt;-<span class="st"> </span><span class="kw">ggplot</span>(<span class="kw">data.frame</span>(</span>
<span id="cb132-18"><a href="4-8-regression-regularization.html#cb132-18"></a>    <span class="dt">x =</span> x.grid,</span>
<span id="cb132-19"><a href="4-8-regression-regularization.html#cb132-19"></a>    <span class="dt">fit1 =</span> fit1,</span>
<span id="cb132-20"><a href="4-8-regression-regularization.html#cb132-20"></a>    <span class="dt">fit2 =</span> fit2,</span>
<span id="cb132-21"><a href="4-8-regression-regularization.html#cb132-21"></a>    <span class="dt">fit3 =</span> fit3</span>
<span id="cb132-22"><a href="4-8-regression-regularization.html#cb132-22"></a>  )) <span class="op">+</span></span>
<span id="cb132-23"><a href="4-8-regression-regularization.html#cb132-23"></a><span class="st">    </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(<span class="dt">x =</span> x, <span class="dt">y =</span> fit1, <span class="dt">color =</span> <span class="st">&quot;1&quot;</span>), <span class="dt">size =</span> <span class="dv">1</span>) <span class="op">+</span></span>
<span id="cb132-24"><a href="4-8-regression-regularization.html#cb132-24"></a><span class="st">    </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(<span class="dt">x =</span> x, <span class="dt">y =</span> fit2, <span class="dt">color =</span> <span class="st">&quot;2&quot;</span>), <span class="dt">size =</span> <span class="dv">1</span>) <span class="op">+</span></span>
<span id="cb132-25"><a href="4-8-regression-regularization.html#cb132-25"></a><span class="st">    </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(<span class="dt">x =</span> x, <span class="dt">y =</span> fit3, <span class="dt">color =</span> <span class="st">&quot;3&quot;</span>), <span class="dt">size =</span> <span class="dv">1</span>) <span class="op">+</span></span>
<span id="cb132-26"><a href="4-8-regression-regularization.html#cb132-26"></a><span class="st">    </span><span class="kw">scale_colour_manual</span>(</span>
<span id="cb132-27"><a href="4-8-regression-regularization.html#cb132-27"></a>      <span class="dt">name =</span> <span class="st">&quot;&quot;</span>,</span>
<span id="cb132-28"><a href="4-8-regression-regularization.html#cb132-28"></a>      <span class="dt">values =</span> <span class="kw">c</span>(</span>
<span id="cb132-29"><a href="4-8-regression-regularization.html#cb132-29"></a>        <span class="st">&quot;train&quot;</span> =<span class="st"> </span>cbp1[<span class="dv">1</span>],</span>
<span id="cb132-30"><a href="4-8-regression-regularization.html#cb132-30"></a>        <span class="st">&quot;1&quot;</span> =<span class="st"> </span>cbp1[<span class="dv">2</span>],</span>
<span id="cb132-31"><a href="4-8-regression-regularization.html#cb132-31"></a>        <span class="st">&quot;2&quot;</span> =<span class="st"> </span>cbp1[<span class="dv">3</span>],</span>
<span id="cb132-32"><a href="4-8-regression-regularization.html#cb132-32"></a>        <span class="st">&quot;3&quot;</span> =<span class="st"> </span>cbp1[<span class="dv">4</span>]</span>
<span id="cb132-33"><a href="4-8-regression-regularization.html#cb132-33"></a>      ),</span>
<span id="cb132-34"><a href="4-8-regression-regularization.html#cb132-34"></a>      <span class="dt">labels =</span> <span class="kw">c</span>(<span class="st">&quot;1&quot;</span> =<span class="st"> &quot;2nd&quot;</span>, <span class="st">&quot;2&quot;</span> =<span class="st"> &quot;6th&quot;</span>, <span class="st">&quot;3&quot;</span> =<span class="st"> &quot;20th&quot;</span>)</span>
<span id="cb132-35"><a href="4-8-regression-regularization.html#cb132-35"></a>    ) <span class="op">+</span></span>
<span id="cb132-36"><a href="4-8-regression-regularization.html#cb132-36"></a><span class="st">    </span><span class="kw">geom_point</span>(<span class="kw">aes</span>(<span class="dt">x =</span> x, <span class="dt">y =</span> y, <span class="dt">color =</span> <span class="st">&quot;train&quot;</span>),</span>
<span id="cb132-37"><a href="4-8-regression-regularization.html#cb132-37"></a>               <span class="dt">data =</span> data.train.df,</span>
<span id="cb132-38"><a href="4-8-regression-regularization.html#cb132-38"></a>               <span class="dt">show.legend =</span> <span class="ot">FALSE</span>) <span class="op">+</span></span>
<span id="cb132-39"><a href="4-8-regression-regularization.html#cb132-39"></a><span class="st">    </span><span class="kw">ylab</span>(<span class="st">&quot;y&quot;</span>) <span class="op">+</span></span>
<span id="cb132-40"><a href="4-8-regression-regularization.html#cb132-40"></a><span class="st">    </span><span class="kw">coord_cartesian</span>(<span class="dt">ylim =</span> <span class="kw">c</span>(<span class="op">-</span><span class="dv">3</span>, <span class="dv">2</span>)) <span class="op">+</span></span>
<span id="cb132-41"><a href="4-8-regression-regularization.html#cb132-41"></a><span class="st">    </span><span class="kw">theme_minimal</span>()</span>
<span id="cb132-42"><a href="4-8-regression-regularization.html#cb132-42"></a></span>
<span id="cb132-43"><a href="4-8-regression-regularization.html#cb132-43"></a>  p</span>
<span id="cb132-44"><a href="4-8-regression-regularization.html#cb132-44"></a>}</span></code></pre></div>
<p>Next we fit the polynomial models to four different training sets, generated from the same distribution.</p>
<div class="sourceCode" id="cb133"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb133-1"><a href="4-8-regression-regularization.html#cb133-1"></a><span class="kw">set.seed</span>(<span class="dv">42</span>)</span>
<span id="cb133-2"><a href="4-8-regression-regularization.html#cb133-2"></a><span class="kw">grid.arrange</span>(<span class="kw">fit_plot</span>(), <span class="kw">fit_plot</span>(), <span class="kw">fit_plot</span>(), <span class="kw">fit_plot</span>(),</span>
<span id="cb133-3"><a href="4-8-regression-regularization.html#cb133-3"></a>             <span class="dt">nrow =</span> <span class="dv">2</span>)</span></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:4fits"></span>
<img src="04-statisticalLearning_files/figure-html/4fits-1.png" alt="Polynomial fits to four different sets of training data, sampled from the same distribution" width="100%" />
<p class="caption">
Figure 4.17: Polynomial fits to four different sets of training data, sampled from the same distribution
</p>
</div>
<p>We see from the pictures that then 2nd degree polynomial fit does not change very much on the different training sets, while the 20th degree polynomial looks very different in the four pictures.</p>
<p>
Now let us repeat this experiment many times, and only plot the mean and variance of <span class="math inline">\(h\)</span>.
</p>
<div class="figure" style="text-align: center"><span id="fig:polynomialFits"></span>
<img src="04-statisticalLearning_files/figure-html/polynomialFits-1.png" alt="Mean fit and 95% CI for the different polynomial regressions" width="100%" />
<p class="caption">
Figure 4.18: Mean fit and 95% CI for the different polynomial regressions
</p>
</div>
<p>From the pictures we see that the 2nd degree polynomial has a bias, but for 6h degree, the bias is close to 0. The error in the 2nd degree polynomial is a sum of bias and variance, but for 6th and 20th the error is dominated by the variance term. The variance becomes larger as the degree of the polynomial becomes larger, while the bias is already 0 for 6th degree. Therefore, the total error, the sum of bias and variance, is smallest for a 6th degree polynomial. As before, we see that a very flexible model will tend to overfit the data and may perform worse than a less flexible model.</p>
<p>Another way to see the same phenomena is to choose one point, here <span class="math inline">\(x=1.5\)</span>, and plot the bias and variance as we vary the degree of the polynomial. We do this for sample size <span class="math inline">\(n=100\)</span> and <span class="math inline">\(n=10000\)</span>.</p>
<div class="sourceCode" id="cb134"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb134-1"><a href="4-8-regression-regularization.html#cb134-1"></a><span class="kw">library</span>(ggplot2)</span>
<span id="cb134-2"><a href="4-8-regression-regularization.html#cb134-2"></a><span class="kw">library</span>(gridExtra)</span>
<span id="cb134-3"><a href="4-8-regression-regularization.html#cb134-3"></a></span>
<span id="cb134-4"><a href="4-8-regression-regularization.html#cb134-4"></a>biasVariancePlot &lt;-<span class="st"> </span><span class="cf">function</span>(n.obs, <span class="dt">x0 =</span> <span class="fl">1.5</span>, <span class="dt">n.sim =</span> <span class="fl">1e3</span>, <span class="dt">seed =</span> <span class="dv">42</span>){</span>
<span id="cb134-5"><a href="4-8-regression-regularization.html#cb134-5"></a>  degrees &lt;-<span class="st"> </span><span class="kw">seq</span>(<span class="dv">2</span>,<span class="dv">20</span>,<span class="dv">2</span>)</span>
<span id="cb134-6"><a href="4-8-regression-regularization.html#cb134-6"></a></span>
<span id="cb134-7"><a href="4-8-regression-regularization.html#cb134-7"></a>  predictions &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="dt">nrow =</span> n.sim, <span class="dt">ncol =</span> <span class="kw">length</span>(degrees))</span>
<span id="cb134-8"><a href="4-8-regression-regularization.html#cb134-8"></a>  x.df =<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">x =</span> x0)</span>
<span id="cb134-9"><a href="4-8-regression-regularization.html#cb134-9"></a></span>
<span id="cb134-10"><a href="4-8-regression-regularization.html#cb134-10"></a>  <span class="cf">for</span> (sim <span class="cf">in</span> <span class="kw">seq_len</span>(n.sim)) {</span>
<span id="cb134-11"><a href="4-8-regression-regularization.html#cb134-11"></a>    data.df &lt;-<span class="st"> </span><span class="kw">gen_data</span>(truth, n.obs)</span>
<span id="cb134-12"><a href="4-8-regression-regularization.html#cb134-12"></a>    <span class="cf">for</span> (i <span class="cf">in</span> <span class="kw">seq</span>(<span class="kw">length</span>(degrees))) {</span>
<span id="cb134-13"><a href="4-8-regression-regularization.html#cb134-13"></a>      predictions[sim,i] &lt;-<span class="st"> </span><span class="kw">predict</span>(<span class="kw">lm</span>(y <span class="op">~</span><span class="st"> </span><span class="kw">poly</span>(x, degrees[i]), data.df),</span>
<span id="cb134-14"><a href="4-8-regression-regularization.html#cb134-14"></a>                                    x.df)</span>
<span id="cb134-15"><a href="4-8-regression-regularization.html#cb134-15"></a>    }</span>
<span id="cb134-16"><a href="4-8-regression-regularization.html#cb134-16"></a>  }</span>
<span id="cb134-17"><a href="4-8-regression-regularization.html#cb134-17"></a></span>
<span id="cb134-18"><a href="4-8-regression-regularization.html#cb134-18"></a>  plot.df &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">hbar =</span> <span class="kw">colMeans</span>(predictions),</span>
<span id="cb134-19"><a href="4-8-regression-regularization.html#cb134-19"></a>                         <span class="dt">variance =</span> <span class="kw">colVars</span>(predictions),</span>
<span id="cb134-20"><a href="4-8-regression-regularization.html#cb134-20"></a>                         <span class="dt">bias =</span> (<span class="kw">colMeans</span>(predictions)<span class="op">-</span><span class="kw">truth</span>(x0))<span class="op">^</span><span class="dv">2</span>,</span>
<span id="cb134-21"><a href="4-8-regression-regularization.html#cb134-21"></a>                         <span class="dt">degree =</span> degrees,</span>
<span id="cb134-22"><a href="4-8-regression-regularization.html#cb134-22"></a>                         <span class="dt">total =</span> <span class="kw">colVars</span>(predictions) <span class="op">+</span><span class="st"> </span>(<span class="kw">colMeans</span>(predictions)<span class="op">-</span><span class="kw">truth</span>(x0))<span class="op">^</span><span class="dv">2</span>)</span>
<span id="cb134-23"><a href="4-8-regression-regularization.html#cb134-23"></a></span>
<span id="cb134-24"><a href="4-8-regression-regularization.html#cb134-24"></a>  cbp1 &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;#999999&quot;</span>, <span class="st">&quot;#E69F00&quot;</span>, <span class="st">&quot;#56B4E9&quot;</span>, <span class="st">&quot;#009E73&quot;</span>,</span>
<span id="cb134-25"><a href="4-8-regression-regularization.html#cb134-25"></a>            <span class="st">&quot;#F0E442&quot;</span>, <span class="st">&quot;#0072B2&quot;</span>, <span class="st">&quot;#D55E00&quot;</span>, <span class="st">&quot;#CC79A7&quot;</span>)</span>
<span id="cb134-26"><a href="4-8-regression-regularization.html#cb134-26"></a></span>
<span id="cb134-27"><a href="4-8-regression-regularization.html#cb134-27"></a>  <span class="kw">ggplot</span>(plot.df, <span class="kw">aes</span>(<span class="dt">x =</span> degree)) <span class="op">+</span></span>
<span id="cb134-28"><a href="4-8-regression-regularization.html#cb134-28"></a><span class="st">    </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(<span class="dt">y =</span> bias, <span class="dt">color =</span> <span class="st">&quot;bias&quot;</span>), <span class="dt">size =</span> <span class="dv">1</span>) <span class="op">+</span></span>
<span id="cb134-29"><a href="4-8-regression-regularization.html#cb134-29"></a><span class="st">    </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(<span class="dt">y =</span> variance, <span class="dt">color =</span> <span class="st">&quot;variance&quot;</span>), <span class="dt">size =</span> <span class="dv">1</span>) <span class="op">+</span></span>
<span id="cb134-30"><a href="4-8-regression-regularization.html#cb134-30"></a><span class="st">    </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(<span class="dt">y =</span> total, <span class="dt">color =</span> <span class="st">&quot;total&quot;</span>), <span class="dt">size =</span> <span class="dv">1</span>) <span class="op">+</span></span>
<span id="cb134-31"><a href="4-8-regression-regularization.html#cb134-31"></a><span class="st">    </span><span class="kw">scale_colour_manual</span>(<span class="dt">name =</span> <span class="st">&quot;&quot;</span>,</span>
<span id="cb134-32"><a href="4-8-regression-regularization.html#cb134-32"></a>                        <span class="dt">values =</span> <span class="kw">c</span>(<span class="st">&quot;bias&quot;</span> =<span class="st"> </span>cbp1[<span class="dv">2</span>], <span class="st">&quot;variance&quot;</span> =<span class="st"> </span>cbp1[<span class="dv">3</span>], <span class="st">&quot;total&quot;</span> =<span class="st"> </span>cbp1[<span class="dv">4</span>]),</span>
<span id="cb134-33"><a href="4-8-regression-regularization.html#cb134-33"></a>                        <span class="dt">labels =</span> <span class="kw">c</span>(<span class="st">&quot;bias&quot;</span> =<span class="st"> &quot;bias&quot;</span>, <span class="st">&quot;variance&quot;</span> =<span class="st"> &quot;variance&quot;</span>, <span class="st">&quot;total&quot;</span> =<span class="st"> &quot;total&quot;</span>)) <span class="op">+</span></span>
<span id="cb134-34"><a href="4-8-regression-regularization.html#cb134-34"></a><span class="st">    </span><span class="kw">scale_y_continuous</span>(<span class="dt">trans=</span><span class="st">&#39;log10&#39;</span>) <span class="op">+</span></span>
<span id="cb134-35"><a href="4-8-regression-regularization.html#cb134-35"></a><span class="st">    </span><span class="kw">ylab</span>(<span class="st">&quot;Error&quot;</span>) <span class="op">+</span></span>
<span id="cb134-36"><a href="4-8-regression-regularization.html#cb134-36"></a><span class="st">    </span><span class="kw">xlab</span>(<span class="st">&quot;Degree&quot;</span>) <span class="op">+</span></span>
<span id="cb134-37"><a href="4-8-regression-regularization.html#cb134-37"></a><span class="st">    </span><span class="kw">theme_minimal</span>()</span>
<span id="cb134-38"><a href="4-8-regression-regularization.html#cb134-38"></a></span>
<span id="cb134-39"><a href="4-8-regression-regularization.html#cb134-39"></a>}</span>
<span id="cb134-40"><a href="4-8-regression-regularization.html#cb134-40"></a></span>
<span id="cb134-41"><a href="4-8-regression-regularization.html#cb134-41"></a><span class="kw">grid.arrange</span>(<span class="kw">biasVariancePlot</span>(<span class="dv">100</span>), <span class="kw">biasVariancePlot</span>(<span class="dv">10000</span>),</span>
<span id="cb134-42"><a href="4-8-regression-regularization.html#cb134-42"></a>             <span class="dt">nrow =</span> <span class="dv">1</span>)</span></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:errorVsDegree"></span>
<img src="04-statisticalLearning_files/figure-html/errorVsDegree-1.png" alt="Bias/Variance decomposition when varying the degree of the polynomial. Left picture with n = 100, right with n = 10000" width="80%" />
<p class="caption">
Figure 4.19: Bias/Variance decomposition when varying the degree of the polynomial. Left picture with n = 100, right with n = 10000
</p>
</div>
<p>From the pictures we see that the bias is decreasing and the variance is increasing, as the degree increases. In the left picture, <span class="math inline">\(n=100\)</span>, the minimum total error is at a degree around 3. When the sample size increases, in the right picture, we can fit a more complex model, and the minimum error is at a degree around 8.</p>
<p>We have seen that we also in the regression setting need to bound the model complexity so that we do not overfit the model. This can be done by choosing a small family of models <span class="math inline">\(\mathcal H\)</span>. In the above case this would mean limiting the degree of the polynomial. Another option, that we will explore here, is to include a regularization term in the loss function.</p>
<p>Recall that least squares regression minimizes the in-sample error:
<span class="math display">\[
E_{in} = \sum_{i=1}^n\left( y_i - \beta_0 - \sum_{i=1}^p \beta_j x_{ij} \right)^2.
\]</span>
Ridge regression instead minimizes
<span class="math display">\[
E_{in} + \lambda\sum_{i=1}^p \beta_j^2,
\]</span>
and the lasso minimizes
<span class="math display">\[
E_{in} + \lambda\sum_{i=1}^p | \beta_j |.
\]</span>
Here <span class="math inline">\(\lambda\)</span> is a tuning parameter that needs to be determined separately. Ridge regression and lasso use different penalty terms of model complexity, but the main idea is the same. Models with many and large parameters will be penalized in favor of less complex models. Compared to least squares regression, the estimates of <span class="math inline">\(\beta_j\)</span> will be smaller in ridge and lasso. I.e. the estimates will shrink towards zero. These methods are therefore sometimes called shrinkage methods. We know that the estimates in least squares regression are unbiased and shrinkage methods therefore introduce a bias. The hope is that the variance will decrease sufficiently, so that the total prediction error is smaller. Note that neither of these methods are scale invariant, however usually software will take care of this automatically.</p>
<p>Let us look at the same example above, this time with a degree 20 polynomial, but penalized with ridge and lasso.</p>
<div class="sourceCode" id="cb135"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb135-1"><a href="4-8-regression-regularization.html#cb135-1"></a><span class="kw">library</span>(ggplot2)</span>
<span id="cb135-2"><a href="4-8-regression-regularization.html#cb135-2"></a><span class="kw">library</span>(gridExtra)</span>
<span id="cb135-3"><a href="4-8-regression-regularization.html#cb135-3"></a><span class="kw">library</span>(glmnet)</span>
<span id="cb135-4"><a href="4-8-regression-regularization.html#cb135-4"></a></span>
<span id="cb135-5"><a href="4-8-regression-regularization.html#cb135-5"></a>data.df &lt;-<span class="st"> </span><span class="kw">gen_data</span>(truth)</span>
<span id="cb135-6"><a href="4-8-regression-regularization.html#cb135-6"></a></span>
<span id="cb135-7"><a href="4-8-regression-regularization.html#cb135-7"></a>model.ridge &lt;-<span class="st"> </span><span class="kw">glmnet</span>(<span class="kw">poly</span>(data.df<span class="op">$</span>x,<span class="dv">20</span>), data.df<span class="op">$</span>y, <span class="dt">alpha =</span> <span class="dv">0</span> )</span>
<span id="cb135-8"><a href="4-8-regression-regularization.html#cb135-8"></a>model.lasso &lt;-<span class="st"> </span><span class="kw">glmnet</span>(<span class="kw">poly</span>(data.df<span class="op">$</span>x,<span class="dv">20</span>), data.df<span class="op">$</span>y, <span class="dt">alpha =</span> <span class="dv">1</span> )</span></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:ridge"></span>
<img src="04-statisticalLearning_files/figure-html/ridge-1.png" alt="Coefficents as lambda is varied for ridge regression" width="80%" />
<p class="caption">
Figure 4.20: Coefficents as lambda is varied for ridge regression
</p>
</div>
<div class="figure" style="text-align: center"><span id="fig:lasso"></span>
<img src="04-statisticalLearning_files/figure-html/lasso-1.png" alt="Coefficients as lambda is varied for the lasso" width="80%" />
<p class="caption">
Figure 4.21: Coefficients as lambda is varied for the lasso
</p>
</div>
<p>In the pictures we see that the fitted coefficients decrease towards 0 as <span class="math inline">\(\lambda\)</span> increases. The main difference between the two methods is that in the ridge regression all coefficients are strictly positive for any finite <span class="math inline">\(\lambda\)</span>, while for the lasso coefficients become 0 when <span class="math inline">\(\lambda\)</span> is large. This difference relates to model interpretability. That is, the lasso will produce a simple model where many variables can be disregarded. We also mention that the lasso and ridge regression can, unlike unregularized least squares, handle the case <span class="math inline">\(p&gt;&gt;n\)</span>. There is a generalization of ridge and lasso called elastic net, where a parameter <span class="math inline">\(0\leq \alpha \leq 1\)</span> is introduced and the loss function is instead,
<span class="math display">\[
E_{in} + \lambda\left( \alpha \sum_{i=1}^p | \beta_j | + (1-\alpha) \sum_{i=1}^p \beta_j^2\right).
\]</span></p>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="4-7-bias-variance-decomposition.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="4-9-model-selection.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
