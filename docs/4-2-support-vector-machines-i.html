<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>4.2 Support vector machines I | Lecture notes for Statistical Inference and Machine Learning</title>
  <meta name="description" content="These are the lecture notes for the course Statistical Inference and Machine Learning at the Department of statistics, Uppsala University." />
  <meta name="generator" content="bookdown 0.20 and GitBook 2.6.7" />

  <meta property="og:title" content="4.2 Support vector machines I | Lecture notes for Statistical Inference and Machine Learning" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="These are the lecture notes for the course Statistical Inference and Machine Learning at the Department of statistics, Uppsala University." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="4.2 Support vector machines I | Lecture notes for Statistical Inference and Machine Learning" />
  
  <meta name="twitter:description" content="These are the lecture notes for the course Statistical Inference and Machine Learning at the Department of statistics, Uppsala University." />
  

<meta name="author" content="Patrik Andersson" />


<meta name="date" content="2020-09-02" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="4-1-classification.html"/>
<link rel="next" href="4-3-hoeffdings-inequality.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />











<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Inference and Machine Learning</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="1-ch-likelihood.html"><a href="1-ch-likelihood.html"><i class="fa fa-check"></i><b>1</b> Likelihood-based methods</a><ul>
<li class="chapter" data-level="1.1" data-path="1-1-maximum-likelihood-estimation.html"><a href="1-1-maximum-likelihood-estimation.html"><i class="fa fa-check"></i><b>1.1</b> Maximum likelihood estimation</a></li>
<li class="chapter" data-level="1.2" data-path="1-2-hypothesis-testing.html"><a href="1-2-hypothesis-testing.html"><i class="fa fa-check"></i><b>1.2</b> Hypothesis testing</a></li>
<li class="chapter" data-level="1.3" data-path="1-3-likelihood-ratio-test.html"><a href="1-3-likelihood-ratio-test.html"><i class="fa fa-check"></i><b>1.3</b> Likelihood ratio test</a></li>
<li class="chapter" data-level="1.4" data-path="1-4-mathematical-aside-taylor-expansion.html"><a href="1-4-mathematical-aside-taylor-expansion.html"><i class="fa fa-check"></i><b>1.4</b> Mathematical aside: Taylor expansion</a></li>
<li class="chapter" data-level="1.5" data-path="1-5-asymptotic-distribution-of-the-mle.html"><a href="1-5-asymptotic-distribution-of-the-mle.html"><i class="fa fa-check"></i><b>1.5</b> Asymptotic distribution of the MLE</a></li>
<li class="chapter" data-level="1.6" data-path="1-6-the-delta-method.html"><a href="1-6-the-delta-method.html"><i class="fa fa-check"></i><b>1.6</b> The delta method</a></li>
<li class="chapter" data-level="1.7" data-path="1-7-wilks-test.html"><a href="1-7-wilks-test.html"><i class="fa fa-check"></i><b>1.7</b> Wilks’ test</a></li>
<li class="chapter" data-level="1.8" data-path="1-8-walds-test.html"><a href="1-8-walds-test.html"><i class="fa fa-check"></i><b>1.8</b> Wald’s test</a></li>
<li class="chapter" data-level="1.9" data-path="1-9-score-test.html"><a href="1-9-score-test.html"><i class="fa fa-check"></i><b>1.9</b> Score test</a></li>
<li class="chapter" data-level="1.10" data-path="1-10-confidence-intervals.html"><a href="1-10-confidence-intervals.html"><i class="fa fa-check"></i><b>1.10</b> Confidence intervals</a></li>
<li class="chapter" data-level="1.11" data-path="1-11-an-application-i.html"><a href="1-11-an-application-i.html"><i class="fa fa-check"></i><b>1.11</b> An application I</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="2-ch-bootstrap.html"><a href="2-ch-bootstrap.html"><i class="fa fa-check"></i><b>2</b> Bootstrap</a><ul>
<li class="chapter" data-level="2.1" data-path="2-1-parametric-vs-non-parametric.html"><a href="2-1-parametric-vs-non-parametric.html"><i class="fa fa-check"></i><b>2.1</b> Parametric vs non-parametric</a></li>
<li class="chapter" data-level="2.2" data-path="2-2-non-parametric-estimation.html"><a href="2-2-non-parametric-estimation.html"><i class="fa fa-check"></i><b>2.2</b> Non-parametric estimation</a></li>
<li class="chapter" data-level="2.3" data-path="2-3-bootstrap.html"><a href="2-3-bootstrap.html"><i class="fa fa-check"></i><b>2.3</b> Bootstrap</a></li>
<li class="chapter" data-level="2.4" data-path="2-4-parametric-bootstrap.html"><a href="2-4-parametric-bootstrap.html"><i class="fa fa-check"></i><b>2.4</b> Parametric bootstrap</a></li>
<li class="chapter" data-level="2.5" data-path="2-5-an-application-ii.html"><a href="2-5-an-application-ii.html"><i class="fa fa-check"></i><b>2.5</b> An application II</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="3-bayesian-statistics-draft.html"><a href="3-bayesian-statistics-draft.html"><i class="fa fa-check"></i><b>3</b> Bayesian statistics (draft)</a><ul>
<li class="chapter" data-level="3.1" data-path="3-1-some-basic-decision-theory.html"><a href="3-1-some-basic-decision-theory.html"><i class="fa fa-check"></i><b>3.1</b> Some basic decision theory</a></li>
<li class="chapter" data-level="3.2" data-path="3-2-bayesian-statistics.html"><a href="3-2-bayesian-statistics.html"><i class="fa fa-check"></i><b>3.2</b> Bayesian statistics</a></li>
<li class="chapter" data-level="3.3" data-path="3-3-choosing-prior.html"><a href="3-3-choosing-prior.html"><i class="fa fa-check"></i><b>3.3</b> Choosing prior</a></li>
<li class="chapter" data-level="3.4" data-path="3-4-multiparameter-problems.html"><a href="3-4-multiparameter-problems.html"><i class="fa fa-check"></i><b>3.4</b> Multiparameter problems</a></li>
<li class="chapter" data-level="3.5" data-path="3-5-markov-chain-monte-carlo.html"><a href="3-5-markov-chain-monte-carlo.html"><i class="fa fa-check"></i><b>3.5</b> Markov chain monte carlo</a></li>
<li class="chapter" data-level="3.6" data-path="3-6-an-application-iii.html"><a href="3-6-an-application-iii.html"><i class="fa fa-check"></i><b>3.6</b> An application III</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="4-ch-statLearn.html"><a href="4-ch-statLearn.html"><i class="fa fa-check"></i><b>4</b> Statistical learning (draft)</a><ul>
<li class="chapter" data-level="4.1" data-path="4-1-classification.html"><a href="4-1-classification.html"><i class="fa fa-check"></i><b>4.1</b> Classification</a></li>
<li class="chapter" data-level="4.2" data-path="4-2-support-vector-machines-i.html"><a href="4-2-support-vector-machines-i.html"><i class="fa fa-check"></i><b>4.2</b> Support vector machines I</a></li>
<li class="chapter" data-level="4.3" data-path="4-3-hoeffdings-inequality.html"><a href="4-3-hoeffdings-inequality.html"><i class="fa fa-check"></i><b>4.3</b> Hoeffding’s inequality</a></li>
<li class="chapter" data-level="4.4" data-path="4-4-generalization-error.html"><a href="4-4-generalization-error.html"><i class="fa fa-check"></i><b>4.4</b> Generalization error</a></li>
<li class="chapter" data-level="4.5" data-path="4-5-vc-dimension.html"><a href="4-5-vc-dimension.html"><i class="fa fa-check"></i><b>4.5</b> VC-dimension</a></li>
<li class="chapter" data-level="4.6" data-path="4-6-support-vector-machines-ii.html"><a href="4-6-support-vector-machines-ii.html"><i class="fa fa-check"></i><b>4.6</b> Support vector machines II</a></li>
<li class="chapter" data-level="4.7" data-path="4-7-bias-variance-decomposition.html"><a href="4-7-bias-variance-decomposition.html"><i class="fa fa-check"></i><b>4.7</b> Bias-Variance decomposition</a></li>
<li class="chapter" data-level="4.8" data-path="4-8-regression-regularization.html"><a href="4-8-regression-regularization.html"><i class="fa fa-check"></i><b>4.8</b> Regression regularization</a></li>
<li class="chapter" data-level="4.9" data-path="4-9-model-selection.html"><a href="4-9-model-selection.html"><i class="fa fa-check"></i><b>4.9</b> Model selection</a></li>
<li class="chapter" data-level="4.10" data-path="4-10-an-application-iv.html"><a href="4-10-an-application-iv.html"><i class="fa fa-check"></i><b>4.10</b> An application IV</a></li>
<li class="chapter" data-level="4.11" data-path="4-11-an-application-v.html"><a href="4-11-an-application-v.html"><i class="fa fa-check"></i><b>4.11</b> An application V</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="5-beyond-linearity-draft.html"><a href="5-beyond-linearity-draft.html"><i class="fa fa-check"></i><b>5</b> Beyond linearity (draft)</a><ul>
<li class="chapter" data-level="5.1" data-path="5-1-smoothing-splines.html"><a href="5-1-smoothing-splines.html"><i class="fa fa-check"></i><b>5.1</b> Smoothing splines</a></li>
<li class="chapter" data-level="5.2" data-path="5-2-generalized-additive-models.html"><a href="5-2-generalized-additive-models.html"><i class="fa fa-check"></i><b>5.2</b> Generalized additive models</a></li>
<li class="chapter" data-level="5.3" data-path="5-3-neural-networks.html"><a href="5-3-neural-networks.html"><i class="fa fa-check"></i><b>5.3</b> Neural networks</a></li>
<li class="chapter" data-level="5.4" data-path="5-4-stochasitc-gradient-descent.html"><a href="5-4-stochasitc-gradient-descent.html"><i class="fa fa-check"></i><b>5.4</b> Stochasitc gradient descent</a></li>
<li class="chapter" data-level="5.5" data-path="5-5-an-application-vi.html"><a href="5-5-an-application-vi.html"><i class="fa fa-check"></i><b>5.5</b> An application VI</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Lecture notes for Statistical Inference and Machine Learning</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="support-vector-machines-i" class="section level2">
<h2><span class="header-section-number">4.2</span> Support vector machines I</h2>
<p>In this section will discuss binary classification and in particular support vector machines (SVM). The approach taken here is different from the one in ISL. Our approach is easier to explain, generalizes to other method and perhaps also more modern. The approach in ESL however provides a different intuition and is also relevant when implementing the algorithms.</p>
<p>
<p>We are given training examples <span class="math inline">\((x_i,y_i)\)</span>, where <span class="math inline">\(x_i\in \mathbb R^p\)</span> and <span class="math inline">\(y_i\in \left\{-1,1\right\}\)</span>. The equation <span class="math display">\[
f(x):=x^T\beta + \beta_0 = 0,
\]</span>
defines hyperplane (a line in <span class="math inline">\(\mathbb R^2\)</span>, a plane in <span class="math inline">\(\mathbb R^3\)</span>). We are going to classify as +1 if the point is on one side of the hyperplane, <span class="math inline">\(x^T\beta + \beta_0&gt;0\)</span>, and -1 if it is on the other side, <span class="math inline">\(x^T\beta + \beta_0&lt;0\)</span>. In other words, the classification rule is
<span class="math display">\[
h(x) = \text{sign}(x^T\beta + \beta_0).
\]</span>
The value of <span class="math inline">\(f(x_i)\)</span> tells us how far away from the hyperplane the point is and if <span class="math inline">\(y_if(x_i)&gt;0\)</span> the point is classified correctly. That is, if <span class="math inline">\(y_if(x_i)\)</span> is large and positive, the point <span class="math inline">\(x_i\)</span> is classified correctly and with a safe margin. If <span class="math inline">\(y_if(x_i)\)</span> is large and negative, the point is classified correctly and is far away from beeing correctly classified. We will consider the hinge loss
<span class="math display">\[
l(y,f) = (1-yf)_+ ,
\]</span>
here <span class="math inline">\(()_+\)</span> indicates the positive part. We will minimize the in-sample error
<span class="math display">\[
\underset{w,b}{\text{minimize}}\quad \frac{1}{n}\sum_{i=1}^n \max(0,1-y_i(x^T\beta + \beta_0)).
\]</span></p>
<p>As an example, we generate some training data from a mixture of normal distributions.</p>
<div class="sourceCode" id="cb107"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb107-1" data-line-number="1"><span class="kw">library</span>(mvtnorm)</a>
<a class="sourceLine" id="cb107-2" data-line-number="2"><span class="kw">set.seed</span>(<span class="dv">42</span>)</a>
<a class="sourceLine" id="cb107-3" data-line-number="3"></a>
<a class="sourceLine" id="cb107-4" data-line-number="4">mu.p &lt;-<span class="st"> </span><span class="kw">rmvnorm</span>(<span class="dv">10</span>,<span class="dt">mean =</span> <span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">0</span>), <span class="dt">sigma =</span> <span class="kw">diag</span>(<span class="dv">2</span>))</a>
<a class="sourceLine" id="cb107-5" data-line-number="5">mu.n &lt;-<span class="st"> </span><span class="kw">rmvnorm</span>(<span class="dv">10</span>,<span class="dt">mean =</span> <span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">1</span>), <span class="dt">sigma =</span> <span class="kw">diag</span>(<span class="dv">2</span>))</a>
<a class="sourceLine" id="cb107-6" data-line-number="6"></a>
<a class="sourceLine" id="cb107-7" data-line-number="7">n.samples &lt;-<span class="st"> </span><span class="dv">100</span></a>
<a class="sourceLine" id="cb107-8" data-line-number="8"></a>
<a class="sourceLine" id="cb107-9" data-line-number="9">data.matrix &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="dt">nrow =</span> <span class="dv">2</span><span class="op">*</span>n.samples, <span class="dt">ncol =</span> <span class="dv">3</span>)</a>
<a class="sourceLine" id="cb107-10" data-line-number="10"></a>
<a class="sourceLine" id="cb107-11" data-line-number="11"><span class="cf">for</span> (i <span class="cf">in</span> <span class="kw">seq_len</span>(n.samples)) {</a>
<a class="sourceLine" id="cb107-12" data-line-number="12">  mu =<span class="st"> </span>mu.p[<span class="kw">sample</span>(<span class="dt">x=</span> <span class="kw">nrow</span>(mu.p), <span class="dt">size =</span> <span class="dv">1</span>),]</a>
<a class="sourceLine" id="cb107-13" data-line-number="13">  sample &lt;-<span class="st"> </span><span class="kw">rmvnorm</span>(<span class="dv">1</span>, <span class="dt">mean =</span> mu, <span class="dt">sigma =</span> <span class="kw">diag</span>(<span class="dv">2</span>)<span class="op">/</span><span class="dv">5</span>)</a>
<a class="sourceLine" id="cb107-14" data-line-number="14"></a>
<a class="sourceLine" id="cb107-15" data-line-number="15">  data.matrix[<span class="dv">2</span><span class="op">*</span>i<span class="dv">-1</span>,] &lt;-<span class="st"> </span><span class="kw">c</span>(sample, <span class="dv">1</span>)</a>
<a class="sourceLine" id="cb107-16" data-line-number="16"></a>
<a class="sourceLine" id="cb107-17" data-line-number="17">  mu =<span class="st"> </span>mu.n[<span class="kw">sample</span>(<span class="dt">x=</span> <span class="kw">nrow</span>(mu.n), <span class="dt">size =</span> <span class="dv">1</span>),]</a>
<a class="sourceLine" id="cb107-18" data-line-number="18">  sample &lt;-<span class="st"> </span><span class="kw">rmvnorm</span>(<span class="dv">1</span>, <span class="dt">mean =</span> mu, <span class="dt">sigma =</span> <span class="kw">diag</span>(<span class="dv">2</span>)<span class="op">/</span><span class="dv">5</span>)</a>
<a class="sourceLine" id="cb107-19" data-line-number="19">  data.matrix[<span class="dv">2</span><span class="op">*</span>i,] &lt;-<span class="st"> </span><span class="kw">c</span>(sample, <span class="dv">-1</span>)</a>
<a class="sourceLine" id="cb107-20" data-line-number="20">}</a>
<a class="sourceLine" id="cb107-21" data-line-number="21"></a>
<a class="sourceLine" id="cb107-22" data-line-number="22">data.df &lt;-<span class="st"> </span><span class="kw">data.frame</span>(data.matrix)</a>
<a class="sourceLine" id="cb107-23" data-line-number="23"><span class="kw">colnames</span>(data.df) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;x1&quot;</span>,<span class="st">&quot;x2&quot;</span>,<span class="st">&quot;y&quot;</span>)</a>
<a class="sourceLine" id="cb107-24" data-line-number="24">data.df<span class="op">$</span>y &lt;-<span class="st"> </span><span class="kw">as.factor</span>(data.df<span class="op">$</span>y)</a></code></pre></div>
<p>Then use R to calculate the hyperplane that minimizes the in-sample error.</p>
<div class="sourceCode" id="cb108"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb108-1" data-line-number="1"><span class="kw">library</span>(kernlab)</a>
<a class="sourceLine" id="cb108-2" data-line-number="2"></a>
<a class="sourceLine" id="cb108-3" data-line-number="3">svm.model &lt;-<span class="st"> </span><span class="kw">ksvm</span>(y<span class="op">~</span>x1<span class="op">+</span>x2, <span class="dt">data =</span> data.df,</a>
<a class="sourceLine" id="cb108-4" data-line-number="4">                  <span class="dt">type =</span> <span class="st">&quot;C-svc&quot;</span>,</a>
<a class="sourceLine" id="cb108-5" data-line-number="5">                  <span class="dt">kernel =</span> <span class="st">&quot;vanilladot&quot;</span>,</a>
<a class="sourceLine" id="cb108-6" data-line-number="6">                  <span class="dt">C =</span> <span class="dv">1000</span>)</a>
<a class="sourceLine" id="cb108-7" data-line-number="7"></a>
<a class="sourceLine" id="cb108-8" data-line-number="8">grid &lt;-<span class="st"> </span><span class="kw">expand.grid</span>(<span class="dt">x1 =</span> <span class="kw">seq</span>(<span class="op">-</span><span class="dv">5</span>,<span class="dv">5</span>, <span class="dt">length =</span> <span class="dv">500</span>), <span class="dt">x2 =</span> <span class="kw">seq</span>(<span class="op">-</span><span class="dv">5</span>,<span class="dv">5</span>, <span class="dt">length =</span> <span class="dv">500</span>))</a>
<a class="sourceLine" id="cb108-9" data-line-number="9">grid<span class="op">$</span>predicted &lt;-<span class="st"> </span><span class="kw">as.factor</span>(<span class="kw">predict</span>(svm.model, grid))</a></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:SVMlinear"></span>
<img src="04-statisticalLearning_files/figure-html/SVMlinear-1.png" alt="Training data and linear classification" width="80%" />
<p class="caption">
Figure 4.3: Training data and linear classification
</p>
</div>
<p>We can also calculate the in-sample error</p>
<div class="sourceCode" id="cb109"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb109-1" data-line-number="1"><span class="kw">mean</span>(data.df<span class="op">$</span>y <span class="op">!=</span><span class="st"> </span><span class="kw">predict</span>(svm.model, data.df))</a></code></pre></div>
<pre><code>## [1] 0.285</code></pre>
<p>Not so bad, but let us try to improve it. We select some basis functions, <span class="math inline">\(\varphi_m(x)\)</span>, <span class="math inline">\(m=1,\ldots, M\)</span> and use the same classifier but with input features $(x_i) = (_1(x_i),, (x_m)). We can for example choose <span class="math inline">\(\varphi_m\)</span> to be polynomials of increasing order. For order 2, we get the classifier below, an elipsoid.</p>
<div class="sourceCode" id="cb111"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb111-1" data-line-number="1"><span class="kw">library</span>(kernlab)</a>
<a class="sourceLine" id="cb111-2" data-line-number="2"></a>
<a class="sourceLine" id="cb111-3" data-line-number="3">svm.model &lt;-<span class="st"> </span><span class="kw">ksvm</span>(y<span class="op">~</span><span class="kw">poly</span>(x1, x2, <span class="dt">degree =</span> <span class="dv">2</span>), <span class="dt">data =</span> data.df,</a>
<a class="sourceLine" id="cb111-4" data-line-number="4">                  <span class="dt">type =</span> <span class="st">&quot;C-svc&quot;</span>,</a>
<a class="sourceLine" id="cb111-5" data-line-number="5">                  <span class="dt">kernel =</span> <span class="st">&quot;vanilladot&quot;</span>,</a>
<a class="sourceLine" id="cb111-6" data-line-number="6">                  <span class="dt">C =</span> <span class="dv">1000</span>)</a>
<a class="sourceLine" id="cb111-7" data-line-number="7">grid &lt;-<span class="st"> </span><span class="kw">expand.grid</span>(<span class="dt">x1 =</span> <span class="kw">seq</span>(<span class="op">-</span><span class="dv">5</span>,<span class="dv">5</span>, <span class="dt">length =</span> <span class="dv">500</span>), <span class="dt">x2 =</span> <span class="kw">seq</span>(<span class="op">-</span><span class="dv">5</span>,<span class="dv">5</span>, <span class="dt">length =</span> <span class="dv">500</span>))</a>
<a class="sourceLine" id="cb111-8" data-line-number="8">grid<span class="op">$</span>predicted &lt;-<span class="st"> </span><span class="kw">as.factor</span>(<span class="kw">predict</span>(svm.model, grid))</a></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:SVMpolynomial"></span>
<img src="04-statisticalLearning_files/figure-html/SVMpolynomial-1.png" alt="Training data and quadratic classification" width="80%" />
<p class="caption">
Figure 4.4: Training data and quadratic classification
</p>
</div>
<p>This time the in-sample error is</p>
<div class="sourceCode" id="cb112"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb112-1" data-line-number="1"><span class="kw">mean</span>(data.df<span class="op">$</span>y <span class="op">!=</span><span class="st"> </span><span class="kw">fitted</span>(svm.model))</a></code></pre></div>
<pre><code>## [1] 0.275</code></pre>
<p>Better. Let us continue with increasing order polynomials, and calculate the error.</p>
<div class="sourceCode" id="cb114"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb114-1" data-line-number="1"><span class="kw">library</span>(kernlab)</a>
<a class="sourceLine" id="cb114-2" data-line-number="2"></a>
<a class="sourceLine" id="cb114-3" data-line-number="3">maxDegree &lt;-<span class="st"> </span><span class="dv">20</span></a>
<a class="sourceLine" id="cb114-4" data-line-number="4">error.df &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="st">&quot;degree&quot;</span> =<span class="st"> </span><span class="dv">1</span><span class="op">:</span>maxDegree, <span class="dt">inError =</span> <span class="ot">NA</span>, <span class="dt">outError =</span> <span class="ot">NA</span>)</a>
<a class="sourceLine" id="cb114-5" data-line-number="5"></a>
<a class="sourceLine" id="cb114-6" data-line-number="6">svm.model.list &lt;-<span class="st"> </span><span class="kw">vector</span>(<span class="dt">mode =</span> <span class="st">&quot;list&quot;</span>, <span class="dt">length =</span> maxDegree)</a>
<a class="sourceLine" id="cb114-7" data-line-number="7"></a>
<a class="sourceLine" id="cb114-8" data-line-number="8"><span class="cf">for</span> (degree <span class="cf">in</span> <span class="kw">seq</span>(<span class="dv">1</span>,maxDegree)) {</a>
<a class="sourceLine" id="cb114-9" data-line-number="9">  svm.model.list[degree] &lt;-<span class="st"> </span><span class="kw">ksvm</span>(y <span class="op">~</span><span class="st"> </span><span class="kw">poly</span>(x1, x2, <span class="dt">degree =</span> degree), <span class="dt">data =</span> data.df,</a>
<a class="sourceLine" id="cb114-10" data-line-number="10">                                 <span class="dt">type =</span> <span class="st">&quot;C-svc&quot;</span>,</a>
<a class="sourceLine" id="cb114-11" data-line-number="11">                                 <span class="dt">kernel =</span> <span class="st">&quot;vanilladot&quot;</span>,</a>
<a class="sourceLine" id="cb114-12" data-line-number="12">                                 <span class="dt">C =</span> <span class="dv">1000</span>)</a>
<a class="sourceLine" id="cb114-13" data-line-number="13">  error.df<span class="op">$</span>inError[degree] &lt;-<span class="st"> </span><span class="kw">mean</span>(data.df<span class="op">$</span>y <span class="op">!=</span><span class="st"> </span><span class="kw">predict</span>(svm.model.list[[degree]], data.df))</a>
<a class="sourceLine" id="cb114-14" data-line-number="14">}</a></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:errorPlot"></span>
<img src="04-statisticalLearning_files/figure-html/errorPlot-1.png" alt="Training data and quadratic classification" width="80%" />
<p class="caption">
Figure 4.5: Training data and quadratic classification
</p>
</div>
<p>In-sample error gets smaller as we increase the order of the polynomial. For degree 20, the in-sample error is</p>
<div class="sourceCode" id="cb115"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb115-1" data-line-number="1">error.df<span class="op">$</span>inError[<span class="dv">20</span>]</a></code></pre></div>
<pre><code>## [1] 0.03</code></pre>
The classifier looks complex.
<div class="figure" style="text-align: center"><span id="fig:SVMpolynomial20"></span>
<img src="04-statisticalLearning_files/figure-html/SVMpolynomial20-1.png" alt="Training data and degree 20 polynomial classification" width="80%" />
<p class="caption">
Figure 4.6: Training data and degree 20 polynomial classification
</p>
</div>
<p>Since we know the data generating distribution, we can approximate the out-of-sample error for each classifier, by simulation.</p>
<div class="sourceCode" id="cb117"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb117-1" data-line-number="1"><span class="kw">library</span>(mvtnorm)</a>
<a class="sourceLine" id="cb117-2" data-line-number="2"><span class="kw">set.seed</span>(<span class="dv">42</span>)</a>
<a class="sourceLine" id="cb117-3" data-line-number="3"></a>
<a class="sourceLine" id="cb117-4" data-line-number="4">n.samples &lt;-<span class="st"> </span><span class="fl">1e4</span></a>
<a class="sourceLine" id="cb117-5" data-line-number="5"></a>
<a class="sourceLine" id="cb117-6" data-line-number="6">data.matrix &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="dt">nrow =</span> <span class="dv">2</span><span class="op">*</span>n.samples, <span class="dt">ncol =</span> <span class="dv">3</span>)</a>
<a class="sourceLine" id="cb117-7" data-line-number="7"></a>
<a class="sourceLine" id="cb117-8" data-line-number="8"><span class="cf">for</span> (i <span class="cf">in</span> <span class="kw">seq_len</span>(n.samples)) {</a>
<a class="sourceLine" id="cb117-9" data-line-number="9">  mu =<span class="st"> </span>mu.p[<span class="kw">sample</span>(<span class="dt">x=</span> <span class="kw">nrow</span>(mu.p), <span class="dt">size =</span> <span class="dv">1</span>),]</a>
<a class="sourceLine" id="cb117-10" data-line-number="10">  sample &lt;-<span class="st"> </span><span class="kw">rmvnorm</span>(<span class="dv">1</span>, <span class="dt">mean =</span> mu, <span class="dt">sigma =</span> <span class="kw">diag</span>(<span class="dv">2</span>)<span class="op">/</span><span class="dv">5</span>)</a>
<a class="sourceLine" id="cb117-11" data-line-number="11"></a>
<a class="sourceLine" id="cb117-12" data-line-number="12">  data.matrix[<span class="dv">2</span><span class="op">*</span>i<span class="dv">-1</span>,] &lt;-<span class="st"> </span><span class="kw">c</span>(sample, <span class="dv">1</span>)</a>
<a class="sourceLine" id="cb117-13" data-line-number="13"></a>
<a class="sourceLine" id="cb117-14" data-line-number="14">  mu =<span class="st"> </span>mu.n[<span class="kw">sample</span>(<span class="dt">x=</span> <span class="kw">nrow</span>(mu.n), <span class="dt">size =</span> <span class="dv">1</span>),]</a>
<a class="sourceLine" id="cb117-15" data-line-number="15">  sample &lt;-<span class="st"> </span><span class="kw">rmvnorm</span>(<span class="dv">1</span>, <span class="dt">mean =</span> mu, <span class="dt">sigma =</span> <span class="kw">diag</span>(<span class="dv">2</span>)<span class="op">/</span><span class="dv">5</span>)</a>
<a class="sourceLine" id="cb117-16" data-line-number="16">  data.matrix[<span class="dv">2</span><span class="op">*</span>i,] &lt;-<span class="st"> </span><span class="kw">c</span>(sample, <span class="dv">-1</span>)</a>
<a class="sourceLine" id="cb117-17" data-line-number="17">}</a>
<a class="sourceLine" id="cb117-18" data-line-number="18"></a>
<a class="sourceLine" id="cb117-19" data-line-number="19">data.test.df &lt;-<span class="st"> </span><span class="kw">data.frame</span>(data.matrix)</a>
<a class="sourceLine" id="cb117-20" data-line-number="20"><span class="kw">colnames</span>(data.test.df) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;x1&quot;</span>,<span class="st">&quot;x2&quot;</span>,<span class="st">&quot;y&quot;</span>)</a>
<a class="sourceLine" id="cb117-21" data-line-number="21">data.test.df<span class="op">$</span>y &lt;-<span class="st"> </span><span class="kw">as.factor</span>(data.test.df<span class="op">$</span>y)</a>
<a class="sourceLine" id="cb117-22" data-line-number="22"></a>
<a class="sourceLine" id="cb117-23" data-line-number="23"><span class="cf">for</span> (degree <span class="cf">in</span> <span class="kw">seq</span>(<span class="dv">1</span>,maxDegree)) {</a>
<a class="sourceLine" id="cb117-24" data-line-number="24">  error.df<span class="op">$</span>outError[degree] &lt;-<span class="st"> </span><span class="kw">mean</span>(data.test.df<span class="op">$</span>y <span class="op">!=</span><span class="st"> </span><span class="kw">predict</span>(svm.model.list[[degree]],</a>
<a class="sourceLine" id="cb117-25" data-line-number="25">                                                              data.test.df))</a>
<a class="sourceLine" id="cb117-26" data-line-number="26">}</a></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:inOutSampleError"></span>
<img src="04-statisticalLearning_files/figure-html/inOutSampleError-1.png" alt="Error vs. degree of polynomial." width="80%" />
<p class="caption">
Figure 4.7: Error vs. degree of polynomial.
</p>
</div>
<p>The in-sample error is decreasing as the degree of the polynomial increases, and so also the complexity of the model. When the degree is small, the in-sample error is a close approximation of the out-of-sample error, but as the degree increases the difference increases and for large degress the in-sample error provides little information about the out-of-sample error. The out-of-sample error is at first decreasing, but unlike the in-sample-error, starts increasing after degree.</p>
<p>In the following sections we will investigate the connection between the in-sample and out-of-sample error.</p>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="4-1-classification.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="4-3-hoeffdings-inequality.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
