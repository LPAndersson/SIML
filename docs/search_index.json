[
["index.html", "Lecture notes for Statistical Inference and Machine Learning Preface", " Lecture notes for Statistical Inference and Machine Learning Patrik Andersson 2020-08-29 Preface These lecture notes are for the course 2ST126 at Uppsala University. The course litterature is All of Statistics (AOS) by Wasserman and Introduction to Statistical Learning (ISLR) by James et al. These lecture notes complement those books. That is, there are many things that are similar in the books and these notes. Some things are only in the books and some things are only in these notes. Some things are in both of the books and in these lectures notes, but presented differently. The course is therefore defined as the union of all three. At the beginning of each chapter, the readings from the books that are relevant for the chapter is given. "],
["1-ch-likelihood.html", "Chapter 1 Likelihood-based methods", " Chapter 1 Likelihood-based methods This chapter is about methods for statistical inference based on the likelihood function. Readings for this chapter is: AOS 6 AOS 9.1, 9.3, 9.4, 9.6, 9.7, 9.9 AOS 10.0-3, 10.6 "],
["1-1-maximum-likelihood-estimation.html", "1.1 Maximum likelihood estimation", " 1.1 Maximum likelihood estimation We start by reminding ourselves of the basics of likelihood-based inference. Consider the following example: The time until something happens is assumed to be exponentially distributed with parameter \\(\\lambda\\). Therefore the probability density of observing \\(T=t\\) is \\[ f_T(t) = \\lambda e^{-\\lambda t}. \\] On the other hand, we can also think of this as the likelihood function of the parameter \\(\\lambda\\), \\[ L(\\lambda) = \\lambda e^{-\\lambda t}. \\] If we have \\(n\\) independent observations, the joint density of these observations is the product of the marginal densities. Therefore the likelihood is also the product of the marginal likelihoods. That is, if we write \\(L_n(\\lambda)\\) for the likelihood of \\(n\\) observations, \\[ L_n(\\lambda) = \\prod_{i=1}^n \\lambda e^{-\\lambda t_i}. \\] Many times it is advantageous to instead consider the logarithm of the likelihood, the log-likelihood, \\[ l_n(\\lambda):=\\ln L_n(\\lambda) = \\sum_{i=1}^n( \\ln \\lambda - \\lambda t_i ) = n\\ln\\lambda - \\lambda\\sum_{i=1}^n t_i= n(\\ln\\lambda - \\lambda \\bar t) , \\] where we as usual let \\(\\bar t = \\frac{1}{n}\\sum_{i=1}^n t_i\\). Let us say we observe a sample of size 100. Figure 1.1: Histogram of the sample We define a function in R that calculates the log-likelihood of a singe observation logL &lt;- function(t,lambda){ log(lambda) - lambda * t } Since the sample will not change, let us create the log-likelihood function of this particular sample: logLn &lt;- Vectorize(function(lambda){sum(logL(t, lambda))}, &quot;lambda&quot;) Then we may calculate the log-likelihood of, for example, \\(\\lambda = 0.1\\), logLn(0.1) ## [1] -316.1482 In fact, let us plot the log-likelihood for a range of \\(\\lambda\\)-values. Figure 1.2: Log likelihood of the sample We can find the maximum likelihood estimate as the \\(\\lambda\\) that maximizes the likelihood (or log likelihood). That is: \\[ \\hat\\lambda = \\operatorname*{argmax}_\\lambda L_n(\\lambda) = \\operatorname*{argmax}_\\lambda l_n(\\lambda). \\] Since the likelihood is simple, we can find \\(\\hat \\lambda\\) directly: \\[ \\partial_\\lambda l_n(\\lambda)\\Big|_{\\lambda = \\hat\\lambda} = n\\left(\\frac{1}{\\hat \\lambda} - \\bar t\\right) = 0. \\] With solution \\(\\hat\\lambda =1/ \\bar t\\). For this sample: lambdaHat &lt;- 1/mean(t) lambdaHat ## [1] 0.1164284 We may also find the estimate using numerical optimization. optimResult = optimize(logLn,c(0,10),maximum = TRUE) optimResult$maximum ## [1] 0.1164486 "],
["1-2-hypothesis-testing.html", "1.2 Hypothesis testing", " 1.2 Hypothesis testing In the previous section we saw how to estimate unknown parameters using maximum likelihood. While this is all well and good, we would like to also be able to test hypotheses regarding parameters. Consider Figure 1.3. Figure 1.3: Log likelihood of the sample There we have an estimate of \\(\\theta\\) which is \\(\\hat\\theta = 0.9\\). We would like to test \\(H_0: \\theta=\\theta_0 = 1\\) against \\(\\theta \\neq \\theta_0\\). This should be based on how far away, in some sense, the maximum likelihood estimate is from \\(\\theta_0\\). Looking at the figure, we can see three different ways of measuring how close \\(\\hat\\theta\\) and \\(\\theta_0\\) are from each other. One way would be to measure the vertical distance between the log-likelihood function in \\(\\hat\\theta\\) and \\(\\theta_0\\). I.e. we would calculate: \\[ l(\\hat\\theta) - l(\\theta_0). \\] This is know as the likelihood ratio test. Another option is to calculate the horizontal distance between \\(\\hat\\theta\\) and \\(\\theta_0\\). I.e. to calculate the distance \\[ |\\hat\\theta - \\theta_0|. \\] This is know has the Wald test. Lastly we know that \\(\\partial_\\theta l(\\theta)|_{\\theta=\\hat\\theta} = 0\\). So we could calculate \\[ |\\partial_\\theta l(\\theta)|_{\\theta=\\theta_0}|, \\] and see how close it is to 0. This is known as the Score test. In the following sections we examine each test in detail. "],
["1-3-likelihood-ratio-test.html", "1.3 Likelihood ratio test", " 1.3 Likelihood ratio test We are interested in testing the following hypotheses \\[ H_0:\\theta \\in \\Theta_0\\text{ vs. } H_1:\\theta \\in \\Theta_0^\\complement. \\] Here \\(\\Theta_0\\) is some set of parameter values. It could for example be that \\(\\Theta_0 = (-\\infty,\\theta_0)\\) or simply \\(\\Theta_0 = \\theta_0\\). We define the likelihood ratio as \\[ \\lambda_{\\text{LR}} := 2(l(\\hat\\theta) - l(\\hat\\theta_0)), \\] where \\[ l(\\hat\\theta_0) = \\sup_{\\theta\\in\\Theta_0}l(\\theta) \\] and \\[ l(\\hat\\theta) = \\sup_{\\theta\\in\\Theta}l(\\theta). \\] Since \\(l(\\hat\\theta)\\geq l(\\hat\\theta_0)\\), we have that \\(\\lambda_{\\text{LR}} \\geq 0\\) and data agrees well with \\(H_0\\) if \\(\\lambda_{\\text{LR}}\\) is small. Therefore the rejection region will be of the form \\(\\lambda_{\\text{LR}} &gt; k\\), where \\(k\\) is determined to get the correct size of the test. We can not say more in general, the continuation depends on the particular problem and tends to become complicated for anything but simple models. As before let us say that we have an iid sample from an exponential distribution and wish to test \\(H_0:~\\lambda = \\lambda_0\\) against \\(H_1:~\\lambda \\neq \\lambda_0\\), with \\(\\lambda_0=0.1\\). We have already seen that \\[ l(\\lambda) = n(\\ln \\lambda - \\lambda \\bar t) \\] and that \\(\\hat\\lambda = 1/\\bar t\\). Therefore \\[ l(\\lambda) = n \\left( \\ln\\lambda - \\frac{\\lambda}{\\hat\\lambda} \\right). \\] Now, we can write \\(l(\\hat\\lambda) = n\\left(\\ln\\hat\\lambda - 1\\right)\\). The likelihood ratio is then: \\[ \\lambda_{\\text{LR}} = 2(l(\\hat\\lambda) - l(\\lambda_0)) = 2n\\left( \\ln\\hat\\lambda - 1 - \\ln \\lambda_0 + \\frac{\\lambda_0}{\\hat\\lambda} \\right) = 2n\\left(\\ln \\frac{\\hat\\lambda}{\\lambda_0} + \\frac{\\lambda_0-\\hat\\lambda}{\\hat\\lambda}\\right). \\] Recall that we should reject \\(H_0\\) if \\(\\lambda_{\\text{LR}}&gt;k\\), and that \\(k\\) is set to get the correct size. But to do this we need to know the distribution of \\(\\lambda_{\\text{LR}}\\) and looking at the formula above, this seems difficult. Instead we search for something which is equivalent to \\(\\lambda_{\\text{LR}}&gt;k\\), but with a known distribution. Towards this we plot \\(\\lambda_{LR}(\\hat\\lambda)\\): Figure 1.4: Illustration of the likelihood ratio We see that \\(\\lambda_{LR}\\) is decreasing for \\(\\hat\\lambda &lt; \\lambda_0\\) and increasing for \\(\\hat\\lambda &gt; \\lambda_0\\) with minimum at \\(\\hat\\lambda = \\lambda_0\\). Therefore \\(\\lambda_{LR}&gt;k\\) is equivalent to \\(\\hat\\lambda &lt; k_L\\) or \\(\\hat\\lambda &gt; k_U\\), for some choices of \\(k_L\\) and \\(k_U\\). These should be determined so that the test gets the correct size. The size of the test is \\[\\begin{align} &amp;1- P_{\\lambda_0} \\left(k_L&lt; \\hat\\lambda &lt; k_U \\right) = 1- P_{\\lambda_0} \\left(k_L&lt; \\frac{1}{\\bar t} &lt; k_U \\right) = 1- P_{\\lambda_0} \\left(1/k_U&lt; \\bar t &lt; 1/k_L \\right)\\\\ =&amp; 1- P_{\\lambda_0} \\left(\\tilde k_L&lt; \\sum_{i=1}^n T_i&lt; \\tilde k_U \\right), \\end{align}\\] with \\(\\tilde k_L := n/k_U\\) and similarly for \\(\\tilde k_U\\). This probability can be calculated since we know that \\(\\sum_{i=1}^n T_i \\sim \\Gamma(n,\\lambda)\\). If we let \\(\\Gamma_{\\alpha}(n,\\lambda)\\) be the \\(\\alpha\\)-quantile of the gamma distribution, i.e. the number such that \\[ \\alpha = P\\left( \\Gamma(n,\\lambda )&gt; \\Gamma_\\alpha(n,\\lambda) \\right), \\] we see that the rejection region for a size \\(\\alpha\\) test is \\[ \\left\\{ T_i\\mid \\sum_{i=1}^n T_i &gt; \\Gamma_{\\alpha/2}(n,\\lambda_0) \\text{ or } \\sum_{i=1}^n T_i &lt; \\Gamma_{1-\\alpha/2}(n,\\lambda_0) \\right\\}. \\] or equivalently \\[ \\left\\{ \\hat \\lambda \\mid \\hat\\lambda &lt; \\frac{n}{\\Gamma_{\\alpha/2}(n,\\lambda_0)} \\text{ or } \\hat\\lambda &gt; \\frac{n}{\\Gamma_{1-\\alpha/2}(n,\\lambda_0)} \\right\\}. \\] Let us implement this: alpha &lt;- 0.05 lambda0 &lt;- 0.1 n &lt;- 100 upperCriticalValue &lt;- n / qgamma(alpha/2, shape = n, rate = lambda0) lowerCriticalValue &lt;- n / qgamma(1-alpha/2, shape = n, rate = lambda0) upperCriticalValue ## [1] 0.1229045 lowerCriticalValue ## [1] 0.08296762 In this case we had \\(\\hat\\lambda =\\) 0.1164 and so we would not reject \\(\\lambda \\neq 0.1\\). Another option is to calculate the p-value. Recall that the p-value is the smallest level for which \\(H_0\\) is rejected. That is, it is the \\(\\alpha\\) that solves \\(\\Gamma_{1-\\alpha/2}(n,\\lambda_0) = n/\\hat\\lambda\\). But, by definition \\[ \\alpha = 2 P \\left( \\Gamma(n,\\lambda_0) &lt; \\Gamma_{1-\\alpha/2}) \\right) \\] and therefore the p-value is \\[ 2P\\left( \\Gamma(n,\\lambda_0) &lt; \\frac{n}{\\hat\\lambda} \\right). \\] 2* pgamma(n/lambdaHat, shape = n, rate = lambda0) ## [1] 0.1471398 Again we see that we would not reject \\(H_0\\) on the 5%-level. "],
["1-4-mathematical-aside-taylor-expansion.html", "1.4 Mathematical aside: Taylor expansion", " 1.4 Mathematical aside: Taylor expansion We would like to approximate a function \\(f(x)\\) by a polynomial \\(p(x)\\) of degree \\(n\\), around a point \\(x_0\\). That is, if \\(x\\approx x_0\\) we would like \\(f(x)\\approx p(x)\\). How should we choose \\(p(x)\\)? Let us write \\[ p(x) = c_0 + c_1(x-x_0) + c_2(x-x_0)^2 + \\cdots + c_n(x-x_0)^n. \\] To ensure that \\(f(x)\\approx p(x)\\) close to \\(x_0\\), we first require that \\[ f(x_0)=p(x_0)=c_0, \\] so that we have found the first parameter. To make the approximation better, we further require that the first derivatives are the same at \\(x_0\\), \\[ f&#39;(x_0) = p&#39;(x_0) = c_1 + 2c_2(x-x_0) + 3c_3(x-x_0)^2+\\cdots + nc_n(x-x_0)^{n-1}|_{x=x_0} = c_1. \\] Continuing, we want \\[ f&#39;&#39;(x_0) = p&#39;&#39;(x_0) = 2c_2 + 2\\cdot 3c_3(x-x_0) + 3\\cdot 4c_4(x-x_0)^2 + \\cdot (n-1)nc_n(x-x_0)^{n-2}|_{x=x_0} = 2c_2, \\] so that \\(c_2 = f&#39;&#39;(x_0)/2\\). For the \\(k\\)th derivative, \\[ f^{(k)}(x_0) = p^{(k)}(x_0) = 2\\cdot 3\\cdots kc_k = k!c_k, \\] so that \\(c_k = f^{(k)}(x_0)/k!\\). To summarize, the order \\(n\\) polynomial approximation of \\(f(x)\\) close to \\(x_0\\) is \\[ f(x) \\approx f(x_0) + f&#39;(x_0)(x-x_0) + \\frac{f&#39;&#39;(x_0)}{2!}(x-x_0)^2 + \\cdots \\frac{f^{(n)}(x_0)}{n!}(x-x_0)^n. \\] This is known as a Taylor series. In fact, we can do even better by giving a formula for the error. Taylor’s theorem says that \\[ f(x) = f(x_0) + f&#39;(x_0)(x-x_0) + \\cdots \\frac{f^{(n)}(x_0)}{n!}(x-x_0)^n + \\frac{f^{(n+1)}(\\xi)}{(n+1)!}(x-x_0)^{n+1}, \\] where \\(\\xi\\) is some number between \\(x\\) and \\(x_0\\). As an example, let us consider the approximation of \\(\\ln x\\) around 1. We begin by calculating the derivatives, \\[\\begin{align} \\ln 1 &amp;= 0 ,\\\\ \\partial_x\\ln x|_{x=1} &amp;= \\frac{1}{x}|_{x=1} = 1 \\\\ \\partial_x^2 \\ln x|_{x=1} &amp;= -\\frac{1}{x^2}|_{x=1} = -1,\\\\ \\partial_x^3 \\ln x|_{x=1} &amp;= \\frac{2}{x^3}|_{x=1} = 2. \\end{align}\\] Therefore the 3rd order polynomial approximation of \\(\\ln x\\) is \\[ \\ln x \\approx (x-1) - \\frac{1}{2}(x-1)^2 + \\frac{1}{3}(x-1)^3. \\] Figure 1.5: Taylor series approximation of ln x "],
["1-5-asymptotic-distribution-of-the-mle.html", "1.5 Asymptotic distribution of the MLE", " 1.5 Asymptotic distribution of the MLE Here we examine the asymptotic properties of maximum likelihood estimators. To start, we imagine that we observe a random variable \\(X\\), from a parameterized distribution with density \\(p_\\theta\\). Our discussion will also be valid if we have a discrete random variable with a probability function \\(p_\\theta\\). Then we have a log-likelihood \\(l_x(\\theta):=\\ln p_\\theta(x)\\). Here we will calculate the expected value and variance of the random variable \\(l_X&#39;(\\theta):=\\partial_\\theta \\ln p_\\theta(X)\\). First note that since \\[ \\int p_\\theta(x)\\mathrm{d} x = 1, \\] therefore \\[ 0 = \\partial_\\theta \\int p_\\theta(x)\\mathrm{d} x = \\int \\partial_\\theta p_\\theta(x)\\mathrm{d} x = \\int \\partial_\\theta (\\ln p_\\theta(x))p_\\theta(x)\\mathrm{d} x = \\int l&#39;_x(\\theta) p_\\theta(x)\\mathrm{d} x = E[l_X&#39;(\\theta)]. \\] This means, \\[ E[l_X&#39;(\\theta)] =0. \\] To find the variance, we instead consider, \\[\\begin{align*} 0 &amp;= \\partial^2_\\theta \\int p_\\theta(x)\\mathrm{d} x = \\int \\partial_\\theta(l&#39;_x(\\theta) p_\\theta(x))\\mathrm{d} x =\\int (l&#39;&#39;_x(\\theta) p_\\theta(x) + (l&#39;_x(\\theta) )^2p_\\theta(x))\\mathrm{d} x \\\\ &amp;= E[l&#39;&#39;_X(\\theta)] + E[(l&#39;_X(\\theta))^2]. \\end{align*}\\] Therefore, \\[ \\operatorname{Var}(l&#39;_X(\\theta)) = E[(l&#39;_X(\\theta))^2] = -E[l&#39;&#39;_X(\\theta)] := I(\\theta). \\] Here, \\(I(\\theta)\\) is called the Fisher information. This calculation was for a sample \\(X\\) of size 1. If we have an independent sample of size \\(n\\), we define the log-likelihood as \\[ l_n(\\theta) = \\sum_{i=1}^n l_{X_i}(\\theta). \\] Then we can also calculate \\[\\begin{align*} E[l&#39;_n(\\theta)] &amp;= E[\\partial_\\theta \\sum_{i=1}^n l_{X_i}(\\theta)] =\\sum_{i=1}^n E[\\partial_\\theta l_{X_i}(\\theta)] = 0,\\\\ Var(l&#39;_n(\\theta)) &amp;= Var(\\partial_\\theta \\sum_{i=1}^n l_{X_i}(\\theta)) = \\sum_{i=1}^n Var(l_{X_i}&#39;(\\theta)) = n I(\\theta) =:I_n(\\theta). \\end{align*}\\] Since \\(l&#39;_n(\\theta)\\) is a sum of random variables \\(l_{X_i}&#39;(\\theta)\\), and we now know the expected value and variance, we can apply the law of large numbers and central limit theorem to get the following asymptotics: \\[\\begin{align*} \\frac{1}{n}l&#39;_n(\\theta) &amp;\\overset{p}{\\rightarrow} 0,\\quad \\text{as }n\\to\\infty, \\\\ \\frac{1}{\\sqrt{n}}l&#39;_n(\\theta) &amp;\\overset{d}{\\rightarrow} \\mathsf{N}(0,I(\\theta)),\\quad \\text{as }n\\to\\infty,\\\\ -\\frac{1}{n}l&#39;&#39;_n(\\theta) &amp;\\overset{p}{\\rightarrow} I(\\theta)\\text{ as } n\\to\\infty. \\end{align*}\\] Now we are ready to find the asymptotic distribution of \\(\\hat\\theta_n\\). We make a first-order Taylor expansion of \\(l_n&#39;(\\hat\\theta_n)\\) around \\(\\theta\\), \\[ l&#39;_n(\\hat\\theta_n) \\approx l&#39;_n(\\theta) + (\\hat\\theta_n-\\theta)l&#39;&#39;_n(\\theta). \\] Then use that \\(l&#39;_n(\\hat\\theta_n)=0\\) and rewrite as \\[ \\sqrt{n} (\\hat\\theta_n-\\theta) \\approx -\\frac{l&#39;_n(\\theta)/\\sqrt{n}}{l&#39;&#39;_n(\\theta)/n}. \\] Now, using the above asymptotics we arrive at: \\[ \\sqrt{n} (\\hat\\theta_n-\\theta) \\overset{d}{\\rightarrow} \\frac{\\mathsf N(0,I(\\theta))}{I(\\theta)} \\overset{d}{=} \\mathsf N(0,I^{-1}(\\theta)). \\] In particular, this implies that \\(\\hat\\theta_n - \\theta \\overset{p}{\\rightarrow} 0\\), or in other words that \\(\\hat\\theta_n\\) is a consistent estimator of \\(\\theta\\). One problem with the above is however that \\(I(\\theta)\\) is often difficult to calculate. However \\(-l&#39;&#39;_n(\\theta)/n=:\\hat I(\\theta)\\) is a consistent estimator of \\(I(\\theta)\\). Then we may also write: \\[ \\sqrt{n \\hat I(\\theta)}(\\hat\\theta_n - \\theta) \\overset{d}{\\rightarrow} \\mathsf N(0,1). \\] A further problem is that \\(\\theta\\) is in general unknown. But since \\(\\hat\\theta_n\\) is consistent, we may simply replace \\(\\theta\\) by \\(\\hat\\theta_n\\), \\[ \\sqrt{n \\hat I(\\hat\\theta_n)}(\\hat\\theta_n - \\theta) \\overset{d}{\\rightarrow} \\mathsf N(0,1). \\] This is an amazing result. Without knowing in detail how \\(\\hat\\theta\\) is determined from the sample; perhaps from some numerical optimization, we can say what the large-sample distribution is. We can rewrite this as an approximation: \\[ \\hat \\theta_n \\approx \\mathsf N\\left(\\theta, \\frac{1}{n \\hat I(\\hat\\theta_n)}\\right). \\] Now let us apply this to the example of the exponential distribution. We had that \\(\\hat\\lambda_n = 1/\\bar t\\). Further \\[ l&#39;&#39;(\\lambda) = -\\frac{1}{\\lambda^2}. \\] Thus, the Fisher information is simply \\[ I(\\lambda) = -E[l&#39;&#39;_T(\\lambda)] = \\frac{1}{\\lambda^2}. \\] The asymptotic distribution of \\(\\hat \\lambda_n\\) is therefore: \\[ \\hat\\lambda_n \\approx \\mathsf N\\left(\\lambda, \\frac{\\hat\\lambda_n^2}{n} \\right). \\] "],
["1-6-the-delta-method.html", "1.6 The delta method", " 1.6 The delta method In this section we discuss how to find the asymptotic distribution of a function of the estimate. Let us assume that we already know that \\[ \\sqrt{n} (\\hat\\theta_n - \\theta) \\overset{d}{\\rightarrow} N(0,\\sigma^2). \\] This might be because \\(\\hat\\theta_n\\) is the MLE and we have used the results from the previous section or we have applied some central limit theorem. We have a function \\(f\\) and we would like to know the asymptotic distribution of \\(f(\\hat\\theta_n)\\). Let us again write a Taylor expansion \\[ f(\\hat\\theta_n) \\approx f(\\theta) + f&#39;(\\theta)(\\hat\\theta_n - \\theta). \\] Rearranging and multiplying by \\(\\sqrt n\\) gives, \\[ \\sqrt{n}(f(\\hat\\theta_n) - f(\\theta)) \\approx f&#39;(\\theta)\\sqrt{n}(\\hat\\theta_n - \\theta) \\overset{d}{\\rightarrow} N(0,\\sigma^2f&#39;(\\theta)^2) . \\] This result is called the first order Delta method. For this to make sense we need that \\(f&#39;(\\theta)\\neq 0\\). If this is not the case we can instead do a second order Taylor expansion \\[ f(\\hat\\theta_n) \\approx f(\\theta) + f&#39;(\\theta)(\\hat\\theta_n - \\theta) + \\frac{f&#39;&#39;(\\theta)}{2}(\\hat\\theta_n - \\theta)^2 = f(\\theta) + \\frac{f&#39;&#39;(\\theta)}{2}(\\hat\\theta_n - \\theta)^2. \\] Rearranging gives, \\[ n(f(\\hat\\theta_n) - f(\\theta)) \\approx \\frac{f&#39;&#39;(\\theta)}{2}(\\sqrt{n}(\\hat\\theta_n-\\theta))^2. \\] We assumed that \\(\\sqrt{n} (\\hat\\theta_n - \\theta) \\overset{d}{\\rightarrow} N(0,\\sigma^2)\\). The continuous mapping theorem (not covered here) states that if \\(f\\) is a continuous function and if \\(X_n \\overset{d}{\\rightarrow} X\\), then \\(f(X_N) \\overset{d}{\\rightarrow} f(X)\\). Therefore, if we let \\(Z\\sim N(0,1),\\) we can write our assumption as \\(\\sqrt{n} (\\hat\\theta_n - \\theta) \\overset{d}{\\rightarrow} \\sigma Z\\) and thus, recalling that the square of a standard normal random variable has a \\(\\chi^2_1\\)-distribution, \\[ (\\sqrt{n}(\\hat\\theta_n-\\theta))^2 \\overset{d}{\\rightarrow} \\sigma^2Z^2 \\overset{d}{=} \\sigma^2 \\chi^2_1. \\] With that we get the second order Delta method: \\[ n(f(\\hat\\theta_n) - f(\\theta)) \\approx \\frac{f&#39;&#39;(\\theta)}{2}\\sigma^2\\chi_1^2. \\] Now let us apply the delta method to the exponential distribution. We would like to estimate the probability that the time until the next event is larger than 10. That is the probability \\[ p = P(T&gt;10) = e^{-10\\lambda}. \\] The MLE follows from the invariance principle of maximum likelihood, i.e. \\(\\hat p = e^{-10\\hat\\lambda}\\). The distribution of \\(\\hat p\\) can be found by the delta method if we let \\(p=f(\\lambda) = e^{-10\\lambda}\\). Then \\(f&#39;(\\lambda) = -10e^{-10\\lambda} = -10p\\). In this case \\(f&#39;(\\lambda)\\neq 0\\), so we may apply the first order delta method. Recall from the previous section that \\[ \\sqrt n\\left( \\hat\\lambda_n - \\lambda \\right) \\approx \\mathsf N(0, \\lambda^2), \\] that is, the \\(\\sigma^2\\) appearing in the delta method is \\(\\lambda^2\\). Now, applying the delta method gives \\[ \\sqrt n(\\hat p_n- p) \\approx \\mathsf N(0,100p^2\\lambda^2). \\] As usual, we may replace unknown parameters with a consistent estimate, i.e. \\(p\\) with \\(\\hat p\\) and \\(\\lambda\\) with \\(\\hat\\lambda\\). Therefore, \\[ \\sqrt n(\\hat p_n- p) \\approx \\mathsf N(0,100\\hat p^2\\hat\\lambda^2) = \\mathsf N(0,0.132), \\] or \\[ \\hat p_n \\approx \\mathsf N(p,0.132/n). \\] "],
["1-7-wilks-test.html", "1.7 Wilks’ test", " 1.7 Wilks’ test In the previous sections we found the asymptotic distribution of \\(\\hat\\theta_n\\). Here we seek the asymptotic distribution of the likelihood ratio \\(-2(l_n(\\theta_0) - l_n(\\hat\\theta_n)))\\). The reason is that, as we have seen, finding the exact distribution is difficult. If we have the approximate, asymptotic, distribution, we can use that to do for example hypothesis testing. For ease of notation we suppress the \\(n\\) and write \\(l\\) and \\(\\hat\\theta\\). We will use the following asymptotically valid approximations, that we have seen before: \\[\\begin{align*} l&#39;(\\hat\\theta) &amp;= 0,\\\\ -\\frac{1}{n}l&#39;&#39;(\\hat\\theta) &amp;\\approx -\\frac{1}{n}l&#39;&#39;(\\theta) \\approx I(\\theta),\\\\ \\sqrt{n}(\\hat\\theta - \\theta) &amp;\\approx I^{-1/2}(\\theta )Z, \\end{align*}\\] with \\(Z\\sim \\mathsf N(0,1)\\). Just as in the delta method, we now do a Taylor expansion of \\(l(\\theta)\\) around \\(\\hat\\theta\\): \\[\\begin{align} l(\\theta) \\approx&amp; l(\\hat \\theta) + l&#39;(\\hat\\theta)(\\theta-\\hat\\theta) + \\frac{l&#39;&#39;(\\hat\\theta)}{2}(\\theta-\\hat\\theta) ^2\\\\ =&amp; l(\\hat\\theta) + \\frac{l&#39;&#39;(\\hat\\theta)}{2}(\\theta-\\hat\\theta) ^2 = l(\\hat \\theta) +\\frac{1}{2} \\frac{1}{n}l&#39;&#39;(\\hat\\theta)n(\\theta-\\hat\\theta) ^2\\\\ \\approx &amp; l(\\hat\\theta) -\\frac{1}{2} I(\\theta)I^{-1}(\\theta)Z^2 = l(\\hat\\theta) - \\frac{1}{2}\\chi_1^2. \\end{align}\\] In other words, for a large sample, \\[ \\lambda_{LR} = 2( l(\\hat\\theta)-l(\\theta_0))\\approx \\chi_1^2. \\] Which is Wilks’ theorem. Let us again apply this to the exponential distribution. Of course, we have already found the exact likelihood ratio test, so we would in reality not use an asymptotic test in this case. Nonetheless, we can calculate it as: lrStatistic &lt;- 2*(logLn(optimResult$maximum) - logLn(0.1)) lrStatistic ## [1] 2.20065 Recall that we reject \\(H_0\\) if \\(\\lambda_{LR}\\) is large. Therefore the p-value is 1 - pchisq(lrStatistic, 1) ## [1] 0.1379525 "],
["1-8-walds-test.html", "1.8 Wald’s test", " 1.8 Wald’s test Another way to measure if \\(\\hat\\theta\\) agrees with the null hypothesis is to calculate \\(\\hat\\theta - \\theta_0\\). If this is large, in absolute value, the test should reject the null hypothesis. We have already seen that, under the assumption of \\(H_0\\) \\[ \\frac{\\hat\\theta- \\theta_0}{\\textrm{Sd}(\\hat\\theta)} \\overset{d}{\\approx} \\mathsf{N}(0,1), \\] where the standard deviation can be calculated as \\[ Sd(\\hat\\theta) = (n\\hat I(\\hat\\theta))^{-1/2}. \\] Therefore the test can be done by comparing the left hand-side with the appropriate quantile of the Normal distribution. Note that this implies also that \\[ \\frac{(\\hat \\theta - \\theta_0)^2}{Var(\\hat\\theta)} \\approx \\chi^2_1, \\] which is similar to Wilks’ test. Let us apply this again to the exponential distribution. We have already seen that \\(I(\\lambda) = 1/\\lambda^2\\) and so the standard deviation is: \\[ Sd(\\hat \\lambda) = \\frac{\\hat\\lambda}{\\sqrt n}. \\] Since we have a two-sided test, the test statistic is: \\[ \\frac{\\left| \\hat\\lambda - \\lambda_0 \\right|}{\\hat\\lambda / \\sqrt n }. \\] waldStatistics &lt;- abs(lambdaHat - lambda0)/(lambdaHat/sqrt(n)) waldStatistics ## [1] 1.41103 This is now compared to \\(z_{\\alpha/2}=\\) 1.96 if \\(\\alpha = 0.05\\) and so we do not reject \\(H_0\\). The p-value is: 2*(1-pnorm(waldStatistics)) ## [1] 0.1582359 Here we found the standard deviation of \\(\\hat \\lambda\\) by knowing the asymptotic distribution of the MLE. It is also possible to calculate this directly from the delta method. That is, we know that \\[\\begin{align} E\\left[ T\\right] &amp;= \\frac{1}{\\lambda},\\\\ Var\\left( T \\right) &amp;= \\frac{1}{\\lambda^2}. \\end{align}\\] So by the central limit theorem (\\(\\bar t\\) is a sum of random variables): \\[ \\sqrt n (\\bar t - 1/\\lambda) \\overset{d}{\\rightarrow} N(0,1/\\lambda^2). \\] If \\(f(x)=1/x\\), then \\(\\hat\\lambda = f(\\bar t) = 1/\\bar t\\) and applying the delta method gives, \\[ \\sqrt n (\\hat\\lambda - \\lambda) \\overset{d}{\\rightarrow} N(0,f&#39;(\\bar t)^2/\\lambda^2) = N(0,f&#39;(1/\\lambda)^2/\\lambda^2) = N(0,\\lambda^2). \\] This agrees with what we obtained previously. "],
["1-9-score-test.html", "1.9 Score test", " 1.9 Score test In this section we discuss the score test, sometimes called the Rao test or the Lagrange multiplier (LM) test. If \\(\\hat\\theta\\) is close to \\(\\theta_0\\) then we should have that \\(l&#39;(\\theta_0)\\approx 0\\). The score test is therefore that we reject \\(H_0\\) if \\(\\left| l&#39;(\\theta_0)\\right|&gt;k\\), for some \\(k\\) chosen depending on the size of the test. We can use the asymptotics we already calculated, that is \\[ \\frac{l_n&#39;(\\theta_0)}{\\sqrt{I_n(\\theta_0)}} \\approx \\mathsf N(0,1). \\] So the score test of size \\(\\alpha\\) is to reject \\(H_0\\) if \\[ \\frac{\\left|l_n&#39;(\\theta_0)\\right|}{\\sqrt{I_n(\\theta_0)}}&gt;z_{\\alpha/2}. \\] Note that this test statistic does not require us to calculate the MLE \\(\\hat\\theta\\). By squaring the test statistic we get, as in the previous section, a test statistic that is \\(\\chi^2_1\\)-distributed, similar to Wilks’ test. We apply this to the exponential distribution. We have already calculated everything we need so it is just a matter of putting it together: lp &lt;- n*( 1/lambda0 - mean(t) ) fisherInfo &lt;- n/lambda0^2 scoreStatistic &lt;- lp / sqrt(fisherInfo) scoreStatistic ## [1] 1.41103 With p-value: 2*(1-pnorm(scoreStatistic)) ## [1] 0.1582359 In this particular case the score test and the Wald test are exactly the same. This is not true in general. "],
["1-10-confidence-intervals.html", "1.10 Confidence intervals", " 1.10 Confidence intervals We have derived a number of different tests. In principle all of them can be turned into confidence intervals since there is a correspondence between hypothesis tests and confidence intervals. Let us first examine how we can use the Wald test to construct confidence intervals. The Wald test is based on that for large \\(n\\): \\[ \\frac{\\hat\\theta- \\theta}{\\textrm{Sd}(\\hat\\theta)} \\overset{d}{\\approx} \\mathsf{N}(0,1), \\] Therefore we can write \\[ 1-\\alpha = P\\left( -z_{\\alpha/2} &lt; \\frac{\\hat\\theta- \\theta}{\\textrm{Sd}(\\hat\\theta)} \\leq z_{\\alpha/2} \\right) = P\\left( \\hat\\theta -\\textrm{Sd}(\\hat\\theta) z_{\\alpha/2}\\leq \\theta \\leq \\hat\\theta + \\textrm{Sd}(\\hat\\theta) z_{\\alpha/2}\\right). \\] Which means that \\(\\left[\\hat\\theta -\\textrm{Sd}(\\hat\\theta) z_{\\alpha/2}, \\hat\\theta +\\textrm{Sd}(\\hat\\theta) z_{\\alpha/2}\\right]\\) is a \\(1-\\alpha\\) confidence interval for \\(\\theta\\). Let us derive the same CI in a slightly different way. In the Wald test we would accept \\(H_0: \\theta=\\theta_0\\) if \\(\\left| \\hat\\theta - \\theta_0 \\right|/Sd(\\hat\\theta)&lt;z_{\\alpha/2}\\). Solving this for \\(\\theta_0\\) gives \\[ \\hat\\theta -\\textrm{Sd}(\\hat\\theta) z_{\\alpha/2}\\leq \\theta_0 \\leq \\hat\\theta + \\textrm{Sd}(\\hat\\theta) z_{\\alpha/2}. \\] If we replace \\(\\theta_0\\) with \\(\\theta\\) we obtain the CI above. Therefore we may think of the CI as being the set of \\(\\theta_0\\) that we would accept in a hypothesis test. The same principle can be applied to convert any hypothesis test to a corresponding CI. For example the score test accepts \\(H_0\\) if \\[ \\frac{\\left| l&#39;_n(\\theta)\\right |}{\\sqrt{I_n(\\theta)}}&lt; z_{\\alpha/2}, \\] and so solving this for \\(\\theta\\) gives a CI. However, in most cases it is not possible to obtain a closed form solution and we have solve it numerically. Let us as an example do it for the exponential distribution: \\[\\begin{align} l&#39;_n(\\lambda) &amp;= n\\left( \\frac{1}{\\lambda} - \\frac{1}{\\hat\\lambda} \\right),\\\\ I_n(\\lambda) &amp;= - l&#39;&#39;_n(\\lambda) = \\frac{n}{\\lambda^2}. \\end{align}\\] We plot the score statistic as a function of \\(\\lambda\\): Figure 1.6: Score statistic and confidence intervall To find the CI we need to solve \\[ \\frac{\\left| l&#39;_n(\\lambda)\\right |}{\\sqrt{I_n(\\lambda)}} = z_{\\alpha/2}, \\] in terms of \\(\\lambda\\). Looking at the figure, this has two solutions, one for \\(\\lambda &gt; \\hat\\lambda\\) and one for \\(\\lambda &lt; \\hat\\lambda\\). These will be the left and right endpoints of the CI. alpha = 0.05 scoreStatistic &lt;- function(lambda){ abs( n*(1/lambda - 1/lambdaHat) ) / sqrt( n/lambda^2 ) } f &lt;- function(lambda){ scoreStatistic(lambda) - qnorm(1-alpha/2) } rootResults &lt;- uniroot(f, interval = c(lambdaHat-0.05,lambdaHat)) leftCILimit &lt;- rootResults$root rootResults &lt;- uniroot(f, interval = c(lambdaHat+0.05,lambdaHat)) rightCILimit &lt;- rootResults$root leftCILimit ## [1] 0.09360885 rightCILimit ## [1] 0.1392479 We can compare this to the Wald based CI: alpha = 0.05 z = qnorm(1-alpha/2) leftCILimit &lt;- lambdaHat - qnorm(1-alpha/2)*lambdaHat/sqrt(n) rightCILimit &lt;- lambdaHat + qnorm(1-alpha/2)*lambdaHat/sqrt(n) leftCILimit ## [1] 0.09360885 rightCILimit ## [1] 0.1392479 Again, in this particular example, the two intervals are the same. "],
["1-11-an-application-i.html", "1.11 An application I", " 1.11 An application I Here we present an application of what we have learned in this chapter. Consider the binary regression model, where we observe random variables \\(Y_i\\) that take on the values 0 or 1. The distribution of \\(Y_i\\) depends on the value of a covariate \\(X_i\\), \\[ P(Y_i=1\\mid X_i=x_i) = s(\\beta x_i), \\] where \\(\\beta\\) is a parameter and \\[ s(x) = \\frac{e^{x}}{1+e^{x}} \\] is the logistic function. We have a sample of size \\(n=\\) 1000 and we would like to do inference on \\(\\beta\\). First we plot our data. Figure 1.7: Observed sample We will estimate \\(\\beta\\) by maximum likelihood, so we begin by writing the likelihood of observation \\(i\\), \\[ L_i(\\beta) = s(\\beta x_i)^{y_i}\\left( 1-s(\\beta x_i)\\right)^{(1-y_i)} . \\] So that the log-likelihood of the \\(i\\)th observation is \\[ l_i(\\beta) = y_i\\ln s(\\beta x_i) + (1-y_i)\\ln ( 1-s(\\beta x_i)). \\] Since we assume that our observations are iid, the total log-likelihood is then \\[ l(\\beta) = \\sum_{i=1}^n l_i(\\beta). \\] Let us implement what we have so far. s &lt;- function(x) { exp(x) / (exp(x) + 1) } logL &lt;- function(beta, x, y) { s &lt;- s(x * beta) sum(y * log(s) + (1 - y) * log(1 - s)) } logLOurSample &lt;- Vectorize( function(beta) { logL(beta, data.df$x, data.df$y) }, &quot;beta&quot;) To maximize the likelihood there are now two options. Either we ask the computer to solve \\[ \\underset{\\beta}{\\text{argma}x}~ l(\\beta), \\] or we calculate and solve \\(l&#39;(\\beta)=0\\). For practice we do both ways here. optimResult = optimize(logLOurSample, c(0,3), maximum = TRUE) betahat &lt;- optimResult$maximum betahat ## [1] 1.791579 For the second way we need \\[\\begin{align} s&#39;(x) &amp;= \\frac{e^x}{\\left(1+e^x\\right)^2},\\\\ l&#39;(\\beta) &amp;= \\frac{x y s&#39;(x \\beta )}{s(x \\beta )}-\\frac{x (1-y) s&#39;(x \\beta )}{1-s(x \\beta )}. \\end{align}\\] In R: sp &lt;- function(x){ exp(x)/(1+exp(x))^2 } logLp &lt;- function(beta,x,y){ s &lt;- s(x*beta) sp &lt;- sp(x*beta) sum( -x*(1-y)*sp/(1-s)+x*y*sp/s ) } logLpOurSample &lt;- Vectorize(function(beta){ logLp(beta, data.df$x, data.df$y) }, &quot;beta&quot;) rootResults &lt;- uniroot(logLpOurSample, interval = c(0,3)) rootResults$root ## [1] 1.791589 Both methods giving the same result. To confirm that we indeed found the MLE we plot the log-likelihood. Figure 1.8: Log likelihood of the sample Figure 1.9: Observed sample and fitted model Now we turn to hypothesis testing. Let us say we want to test \\(H_0: \\beta = 2\\) against \\(H_1:\\beta \\neq 2\\). First we do the asymptotic likelihood ratio test. So we need to calculate \\(\\lambda_{\\text{LR}}\\): lr &lt;- function(beta0){ 2*(logLOurSample(betahat) - logLOurSample(beta0)) } lr(2.0) ## [1] 2.97212 If \\(H_0\\) is true, this is an observation of a \\(\\chi_1^2\\)-distributed random variable. Therefore the p-value is 1 - pchisq(lr(2.0), 1) ## [1] 0.0847108 Next we do a Wald’s test. For this we need an estimate of the standard deviation of the MLE. Perhaps the easiest way is to calculate the Fisher information, that is \\(-l&#39;&#39;(\\beta)\\). Here there are again two options, we can do it numerically or exactly. First we calculate it numerically: observedFisherInfo &lt;- function(beta){ drop(-pracma::hessian(logLOurSample,beta)) } observedFisherInfo(betahat) ## [1] 72.6459 Calculating the second derivative exactly involves more work but is preferable whenever possible. We get, \\[ l&#39;&#39;(\\beta)=(1-y) \\left(-\\frac{x^2 \\sigma &#39;(x \\beta )^2}{(1-\\sigma (x \\beta )^2}-\\frac{x^2 \\sigma &#39;&#39;(x \\beta )}{1-\\sigma (x \\beta )}\\right)+y \\left(-\\frac{x^2 \\sigma &#39;(x \\beta )^2}{\\sigma (x \\beta )^2}+\\frac{x^2 \\sigma &#39;&#39;(x \\beta )}{\\sigma (x \\beta )}\\right) \\] Implemented in R: spp &lt;- function(x){ -exp(x)*(exp(x)-1)/(exp(x)+1)^3 } logLpp &lt;- function(beta,x,y){ s &lt;- s(x*beta) sp &lt;- sp(x*beta) spp &lt;- spp(x*beta) sum( (1-y)*(-x^2*sp^2/(1-s)^2 - x^2*spp/(1-s))+y*(-x^2*sp^2/s^2 + x^2*spp/s) ) } logLppOurSample &lt;- Vectorize( function(beta){ logLpp(beta, data.df$x, data.df$y) }, &quot;beta&quot;) observedFisherInfo &lt;- function(beta){ -logLppOurSample(beta) } observedFisherInfo(betahat) ## [1] 72.64591 Recall that Wald’s test statistic is standard normal under \\(H_0\\). So we may calculate the p-value: zWald &lt;- function(beta0){ abs(betahat- beta0)*sqrt(observedFisherInfo(betahat)) } 2 * ( 1 - pnorm( zWald(2.0) ) ) ## [1] 0.07566311 We might also do a Score test. Here, all we need is \\(l&#39;\\) and \\(l&#39;&#39;\\), which we have already calculated. The score statistic is again standard normal under \\(H_0\\). zScore &lt;- function(beta0){ abs(pracma::grad(logLOurSample,beta0)/sqrt(observedFisherInfo(beta0))) } 2 * ( 1 - pnorm(zScore(2.0) ) ) ## [1] 0.0754018 Lastly, we might calculate a CI on \\(\\beta\\). Using the Wald’s statistic, this would be: alpha &lt;- 0.05 leftCILimit &lt;- betahat - qnorm(1-alpha/2) / sqrt(observedFisherInfo(betahat)) rightCILimit &lt;- betahat + qnorm(1-alpha/2) / sqrt(observedFisherInfo(betahat)) leftCILimit ## [1] 1.561624 rightCILimit ## [1] 2.021534 For a score based CI we first plot the score statistic. Figure 1.10: Observed sample and fitted model We need to find the points where the score statistic is \\(z_{\\alpha/2}\\), which are the limits of the CI. alpha = 0.05 f &lt;- function(beta){ zScore(beta) - qnorm(1-alpha/2) } rootResults &lt;- uniroot(f, interval = c(betahat-1,betahat)) leftCILimit &lt;- rootResults$root rootResults &lt;- uniroot(f, interval = c(betahat+1,betahat)) rightCILimit &lt;- rootResults$root leftCILimit ## [1] 1.561916 rightCILimit ## [1] 2.021279 "]
]
