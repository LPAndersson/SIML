[["5-beyond-linearity.html", "Chapter 5 Beyond linearity", " Chapter 5 Beyond linearity In this chapter we discuss some non-linear models. The purpose is only to give a short introduction to each model, enough to be able to use it in practice. Readings for this chapter is: ISL 7.5, 7.7 This video series on neural networks "],["5-1-smoothing-splines.html", "5.1 Smoothing splines", " 5.1 Smoothing splines For smoothing splines, the hypothesis set is the functions with two continuous derivatives, \\(C^2\\). The optimization problem that it solves is \\[ \\min_{h\\in C^2} \\sum_{i=1}^n\\left( y_i - h(x_i) \\right)^2 + \\lambda \\int h&#39;&#39;(t)^2dt. \\] The second term penalizes variability in \\(h\\). Note that there are no parameters in this model and so it is a little surprising that the solutions can be easily characterized. It turns out that the optimal \\(h\\) is a piecewise cubic polynomial with knots at \\(x_1,\\ldots x_n\\) and continuous first and second derivatives. The parameter \\(\\lambda\\) is usually determined by CV. Instead of \\(\\lambda\\), the penalty parameters is sometimes reparametrized as degrees of freedom, which has a similar interpretation as in linear regression. Let us apply this in an example. We wish to predict wage based on age. We first split the data into a training and test set and take a look at the data. library(caret) library(ISLR) data(&quot;Wage&quot;, package = &quot;ISLR&quot;) Wage &lt;- na.omit(Wage) set.seed(42) training.samples &lt;- createDataPartition(Wage$wage, p = 0.7, list = FALSE) train.data &lt;- Wage[training.samples, ] test.data &lt;- Wage[-training.samples, ] ggplot(train.data, aes(x = age, y = wage)) + geom_point() + theme_minimal() We see that the relationship appears non-linear. For lower ages, the wage increases with age and is then roughly constant. Let us fit a smoothing spline. To determine \\(\\lambda\\) by CV, the default in this package is leave-one-out CV. model.smooth &lt;- smooth.spline(train.data$age,train.data$wage, cv = TRUE) predictions.smooth &lt;- predict(model.smooth, test.data$age) mean((predictions.smooth$y - test.data$wage)^2) ## [1] 1553.058 Let us compare this to a polynomial regression. model.linear &lt;- lm(wage ~ poly(age,3), data = train.data) predictions.linear &lt;- predict(model.linear, test.data) mean((predictions.linear - test.data$wage)^2) ## [1] 1545.619 We plot both models. Figure 5.1: Smoothing spline fit to wage date "],["5-2-generalized-additive-models.html", "5.2 Generalized additive models", " 5.2 Generalized additive models If there is more than one predictor, a useful model is the Generalized additive models (GAM). They are defined by the hypothesis functions of the form \\[ h(x_i) = \\beta_0 + \\sum_{j=1}^p h_j(x_{ij}). \\] Here \\(h_j\\) can be in principle any function, for example smoothing splines. We now present a small example using the wage data from ISLR. First load the data, partition into training/test and plot. library(caret) ## Loading required package: lattice ## Loading required package: ggplot2 library(ISLR) library(gam) ## Loading required package: splines ## Loading required package: foreach ## Loaded gam 1.20 library(GGally) ## Registered S3 method overwritten by &#39;GGally&#39;: ## method from ## +.gg ggplot2 data(&quot;Wage&quot;, package = &quot;ISLR&quot;) Wage &lt;- na.omit(Wage) set.seed(42) training.samples &lt;- createDataPartition(Wage$wage, p = 0.7, list = FALSE) train.data &lt;- Wage[training.samples,] test.data &lt;- Wage[-training.samples,] cbp1 &lt;- c(&quot;#999999&quot;, &quot;#E69F00&quot;, &quot;#56B4E9&quot;, &quot;#009E73&quot;, &quot;#F0E442&quot;, &quot;#0072B2&quot;, &quot;#D55E00&quot;, &quot;#CC79A7&quot;) ggpairs(train.data, columns=c(&quot;wage&quot;,&quot;year&quot;,&quot;age&quot;,&quot;education&quot;), upper=list(continuous=wrap(&quot;smooth_loess&quot;,alpha=0.1,color=cbp1[2])), lower = list(combo = wrap(ggally_facethist, bins = 30))) + theme_minimal() Figure 5.2: Scatter plot matrix of wage data. Looking at first row, for the year variable it would probably be fine with a linear fit, while for the age variable, a linear fit seems doubtful. Let us fit two models, one which is linear in year and one with a smoothing spline in year. Both are smoothing splines in age. gam.m1 &lt;- gam(wage~ year + s(age , 4) + education , data = Wage) gam.m2 &lt;- gam(wage~ s(year , 4) + s(age , 4) + education , data = Wage) anova(gam.m1, gam.m2) ## Analysis of Deviance Table ## ## Model 1: wage ~ year + s(age, 4) + education ## Model 2: wage ~ s(year, 4) + s(age, 4) + education ## Resid. Df Resid. Dev Df Deviance Pr(&gt;Chi) ## 1 2990 3696846 ## 2 2987 3692824 3 4021.7 0.3542 The p-value indicates that the linear fit is satisfactory. Let us calculate the test error, also comparing to a linear model. ls &lt;- lm(wage~ year + age + education , data = Wage) predictions &lt;- predict(ls, test.data ) mean((predictions - test.data$wage)^2) ## [1] 1431.626 predictions &lt;- predict(gam.m1, test.data ) mean((predictions - test.data$wage)^2) ## [1] 1373.112 predictions &lt;- predict(gam.m2, test.data ) mean((predictions - test.data$wage)^2) ## [1] 1371.094 The gam models outperform the linear model. The test error is smaller for the gam model with year as smoothed spline, although the difference is small. "],["5-3-neural-networks.html", "5.3 Neural networks", " 5.3 Neural networks In this section we give a short introduction to (artificial) neural networks (NN). After reading this you should know enough to understand how to implement a simple NN and be able to learn about more advanced models on your own. Let us consider the classification setting, the regression setting is very similar. We have a set of predictors \\(x_1,\\ldots, x_p\\) and wish to classify into one of \\(K\\) classes. Consider the following simple model: \\[\\begin{align*} a &amp;= \\sigma(Wx + b)\\\\ h &amp;= \\text{softmax}(a). \\end{align*}\\] Here \\(W\\in \\mathbb R^{K\\times p}\\) is a weight matrix, \\(b\\in \\mathbb R^K\\) is a bias vector and \\(\\sigma\\) is called the activation function. Traditionally sigmoid functions (s-shaped) were used as activiation function but recently it is more popular to use the so called rectified linear unit (ReLU), which is \\[ \\sigma(x) = \\max(0,x). \\] Since we are doing classification, it is convenient if the output of the network can be interpreted as the probability of each class, this is the purporse of the softmax function: \\[ \\text{softmax}(x) := \\frac{e^{x_i}}{\\sum_{j=1}^K e^{x_j}},\\text{ for } i=1,\\ldots,K \\text{ and } x=(x_1,\\ldots , x_K). \\] We then classify each observation into the class with the highest probability. The function above, \\(\\sigma(Wx + b)\\) is called a layer, and now we can stack them on to each other, \\[\\begin{align*} a_1 &amp;= \\sigma(W^1 x + b^1)\\\\ a_2 &amp;= \\sigma(W^2 a_1 + b^2)\\\\ &amp;\\vdots \\\\ a_L&amp;= \\sigma(W^{L} a_{L-1} + b^{L}) \\\\ h &amp;= \\text{softmax}(a_L). \\end{align*}\\] That is essentially all there is, different networks can be obtained by choosing different types of weight matrices and different number of layers \\(L\\). There are results that show that NNs can approximate essentially any function. So looking back at our discussion about generalization error, it is not surprising that it is possible to achieve a low in-sample error using NN. What is however surprising, given the large number of parameters, is that they many times also achieve a low out-of-sample error. As an example, let us consider the same data set as in Section 4.2. We do classification with 1, 2 and 3 layer NNs and plot the result. Figure 5.3: Training data and classification with 1 layer neural network Figure 5.4: Training data and classification with 2 layer neural network Figure 5.5: Training data and classification with 3 layer neural network We see that NNs produce a flexible class of classifiers. What remains to discuss is how to train NNs. "],["5-4-stochastic-gradient-descent.html", "5.4 Stochastic gradient descent", " 5.4 Stochastic gradient descent In this section we discuss an optimization method called stochastic gradient descent, which is used in training, among other models, NNs. However, first let us consider (vanilla) gradient descent. We have a function \\(f\\), which in general would be a function from \\(\\mathbb R^d\\) to \\(\\mathbb R\\), but for simplicity let us say it is from \\(\\mathbb R\\) to \\(\\mathbb R\\). We would like to minimize this function, that is we want to find \\(\\theta^\\star\\) such that \\(f(\\theta^\\star)\\leq f(\\theta)\\), for any \\(\\theta\\in \\mathbb R\\). Gradient descent is the algorithm that iterates the update: \\[ \\theta_{new} = \\theta_{old} - \\eta f&#39;(\\theta_{old}). \\] The algorithm calculates \\(f&#39;\\) evaluated at the current point \\(\\theta_{old}\\). If this is positive, the function is sloping upwards at that point and so if we take a small step to the left, the function should decrease. Therefore we take a step in the direction opposite of the sign of \\(f&#39;\\). Of course, this is only true close to \\(\\theta_{old}\\) so if we take a too large step, we risk increasing the function. Therefore we multiply by a small number \\(\\eta&gt;0\\), usually called learning rate in machine learning. The choice of the learning rate is crucial, too small and the algorithm will be slow to find the minimum, too big and it might not find it at all. One can write done conditions when gradient descent is guaranteed to converge to the correct value, however in machine learning these conditions are rarely fullfilled. So instead one simply evaluate the model given by the algorithm, and if it works well, one is happy. As a toy example, let us implement gradient descent on the function \\(f(\\theta) = \\theta^2\\), were clearly \\(\\theta^\\star = 0\\). Figure 5.6: Gradient descent We see that the fastest convergence (of these choices) is \\(\\eta = 0.1\\). Making \\(\\eta\\) smaller gives slower convergence since the step size is smaller, making \\(\\eta\\) larger makes the step size to large so that the algorithm overshoots and \\(\\theta\\) becomes negative. In statistics we often want to minimize (or maximize) functions of the form \\[ f(\\theta) = \\frac{1}{n} \\sum_{i=1}^n f_i(\\theta), \\] and then of course \\[ f&#39;(\\theta) = \\frac{1}{n} \\sum_{i=1}^n f_i&#39;(\\theta). \\] Both the log-likelihood and the in-sample error are of this form. If \\(n\\) is large, to calculation of the sum will however be expensive. Stochastic gradient descent therefore samples a small number of terms from the sum and takes a gradient descent step based on the derivative of only those terms. These terms are called a mini-batch and the number of terms is the batch size. Then we choose a number of different terms from the sum, and take a step based on them. Once we have gone through all the \\(n\\) terms, we have completed one epoch. The only thing that remains is to discuss how the derivatives are calculated. The parameters that we need to differentiate with respect to are the weight matrices \\(W\\) and the biases \\(b\\). Doing the differentiation by hand is too complicated and not an option. The composition of two functions \\(f_1\\) and \\(f_2\\) is the function \\(f_1(f_2(x))\\). Note that the neural network is of this form, where a linear transformation is composed with the activation function, which forms a layer. That layer is then composed with the next layer, and so on. Calculating derivatives of composed functions can be done with the chain rule and in the context of neural networks this is known as backpropagation. For more on backpropogation and a visualization of neural network, I recommend this video series. "],["5-5-an-application-3.html", "5.5 An application", " 5.5 An application Let us see an example of how to implement a neural network classifier. We will use Keras, which is just a wrapper for the machine learning library Tensorflow. You may find the documentation useful Our goal is to classify hand-written digits from the MNIST database, which is conveniently included in Keras. The first time you install Keras, you do. install.packages(&quot;keras&quot;) keras::install_keras(tensorflow = &quot;cpu&quot;) After that, it should be enough to library(keras) The MNIST database is already divided in a training and a test set mnist &lt;- dataset_mnist() x_train &lt;- mnist$train$x y_train &lt;- mnist$train$y x_test &lt;- mnist$test$x y_test &lt;- mnist$test$y Let us see what the pictures look like. Figure 5.7: Examples from MNIST Each image is represented as a 28x28 matrix of pixel values between 0 and 255. We reshape each matrix in to a vector and scale the pixel value so that it is between 0 and 1. dim(x_train) &lt;- c(nrow(x_train), 784) dim(x_test) &lt;- c(nrow(x_test), 784) x_train &lt;- x_train / 255 x_test &lt;- x_test / 255 The \\(y\\) variables are given as an integer between 0 and 9. We transform it to a vector of dummy variables. y_train &lt;- to_categorical(y_train, 10) y_test &lt;- to_categorical(y_test, 10) Now we specify a 2-layer NN with Relu activation in the hidden layer and softmax in the last layer. model &lt;- keras_model_sequential() model %&gt;% layer_dense(units = 50, activation = &quot;relu&quot;, input_shape = c(784)) %&gt;% layer_dense(units = 10, activation = &quot;softmax&quot;) We compile the model by specifying the loss and the optimization method. model %&gt;% compile( loss = &quot;categorical_crossentropy&quot;, optimizer = optimizer_rmsprop(), metrics = c(&quot;accuracy&quot;) ) Here, cross entropy loss is just the negative of a multinomial log likelihood. The optimizer, RMSprop, is a way of choosing the learning rate adaptively. Now we train the NN. history &lt;- model %&gt;% fit( x_train, y_train, epochs = 10, batch_size = 128, validation_split = 0.2 ) Here we use 20â€° as a validation set. Usually NN does not include a regularization term and so there is a risk of overfitting. Instead one usually restricts the number of epochs and the optimization algorithm is not run until convergence. This is called early stopping. Figure 5.8: Training and validation loss/accuracy for each epoch We see that the validation accuracy is still increasing, so we could probably run more epochs. Let us evaluate the model on the test set. model %&gt;% evaluate(x_test, y_test,verbose = 0) ## loss accuracy ## 0.115221 0.966300 The accuracy is 97%, which is not too bad. Let us make predictions on the test set and plot some of them. Figure 5.9: Predictions on the test set "],["5-6-review-questions-4.html", "5.6 Review questions", " 5.6 Review questions What is the optimization problem that is solved for smoothing splines? What is the hypothesis set for smoothing splines? What is the hypothesis set for generalized additive models? What is an activation function? What is an ReLU? What is the softmax function? What is a layer? What is gradient descent? What is the trade-off in choosing the learning rate? What is a mini-batch? What is an epoch? What is backpropagation? How are NNs usually regularized? "]]
