[["5-beyond-linearity-draft.html", "Chapter 5 Beyond linearity (draft)", " Chapter 5 Beyond linearity (draft) In this module we cover some state-of-the-art non-linear models. Decision trees, bagging, random forest and boosting is covered well in ISLR, so we will no repeat it here. But it is part of the course. The new version of ISLR also includes a chapter on deep learning. The first part of this chapter is also part of the course Readings for this chapter is therefore: ISLR 8 ISLR 10.1-3 and 10.6-7 "],["5-1-an-application-3.html", "5.1 An application", " 5.1 An application Let us see an example of how to implement a neural network classifier. We will use Keras, which is just a wrapper for the machine learning library Tensorflow. You may find the documentation useful Our goal is to classify hand-written digits from the MNIST database, which is conveniently included in Keras. The first time you install Keras, you do. install.packages(&quot;keras&quot;) keras::install_keras() If you get a prompt asking to install miniconda, you should choose “Yes.” If this installation fails another option is to follow the instructions at the ISLR webpage After that, it should be enough to library(keras) The MNIST database is already divided in a training and a test set mnist &lt;- dataset_mnist() x_train &lt;- mnist$train$x y_train &lt;- mnist$train$y x_test &lt;- mnist$test$x y_test &lt;- mnist$test$y Let us see what the pictures look like. Figure 5.1: Examples from MNIST Each image is represented as a 28x28 matrix of pixel values between 0 and 255. We reshape each matrix in to a vector and scale the pixel value so that it is between 0 and 1. dim(x_train) &lt;- c(nrow(x_train), 784) dim(x_test) &lt;- c(nrow(x_test), 784) x_train &lt;- x_train / 255 x_test &lt;- x_test / 255 The \\(y\\) variables are given as an integer between 0 and 9. We transform it to a vector of dummy variables. y_train &lt;- to_categorical(y_train, 10) y_test &lt;- to_categorical(y_test, 10) Now we specify a 2-layer NN with Relu activation in the hidden layer and softmax in the last layer. model &lt;- keras_model_sequential() model %&gt;% layer_dense(units = 50, activation = &quot;relu&quot;, input_shape = c(784)) %&gt;% layer_dense(units = 10, activation = &quot;softmax&quot;) We compile the model by specifying the loss and the optimization method. model %&gt;% compile( loss = &quot;categorical_crossentropy&quot;, optimizer = optimizer_rmsprop(), metrics = c(&quot;accuracy&quot;) ) Here, cross entropy loss is just the negative of a multinomial log likelihood. The optimizer, RMSprop, is a way of choosing the learning rate adaptively. Now we train the NN. history &lt;- model %&gt;% fit( x_train, y_train, epochs = 10, batch_size = 128, validation_split = 0.2 ) Here we use 20‰ as a validation set. Usually NN does not include a regularization term and so there is a risk of overfitting. Instead one usually restricts the number of epochs and the optimization algorithm is not run until convergence. This is called early stopping. Figure 5.2: Training and validation loss/accuracy for each epoch We see that the validation accuracy is still increasing, so we could probably run more epochs. Let us evaluate the model on the test set. model %&gt;% evaluate(x_test, y_test,verbose = 0) ## loss accuracy ## 0.1103705 0.9698000 The accuracy is 97%, which is not too bad. Let us make predictions on the test set and plot some of them. Figure 5.3: Predictions on the test set "],["5-2-review-questions-4.html", "5.2 Review questions", " 5.2 Review questions What is an activation function? What is an ReLU? What is the softmax function? What is a layer? What is gradient descent? What is the trade-off in choosing the learning rate? What is a mini-batch? What is an epoch? What is backpropagation? How are NNs usually regularized? "]]
