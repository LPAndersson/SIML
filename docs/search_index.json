[["index.html", "Lecture notes for Statistical Inference and Machine Learning Preface", " Lecture notes for Statistical Inference and Machine Learning Patrik Andersson 2021-09-08 Preface These lecture notes are for the course 2ST126 at Uppsala University. The course litterature is All of Statistics (AOS) by Wasserman and Introduction to Statistical Learning (ISLR) by James et al. These lecture notes complement those books. That is, there are many things that are similar in the books and these notes. Some things are only in the books and some things are only in these notes. Some things are in both of the books and in these lectures notes, but presented differently. The course is therefore defined as the union of all three. At the beginning of each chapter, the readings from the books that are relevant for the chapter is given. You can find the source code for these lecture notes at GitHub "],["1-ch-likelihood.html", "Chapter 1 Likelihood-based methods", " Chapter 1 Likelihood-based methods This chapter is about methods for statistical inference based on the likelihood function. We start by discussing how we can find a point estimate of a parameter by using the maximum likelihood estimate. Once we have a point estimate we want to know how certain or uncertain this estimate is. This can be done by either simply calculating the standard deviation of the estimate or by calculating a confidence interval or by doing a hypothesis test. We then discuss the likelihood ratio test which is a general way of testing hypothesis based on the likelihood function. However, in many situations the distribution of the likelihood ratio under the null hypothesis is difficult to find, so we turn to asymptotic results. I.e. results that are valid when the sample size is large. It turns out that maximum likelihood estimates in general have an asymptotic normal distribution, with the true parameter value as expected value and variance given by the Fisher information, which can be calculated from the log-likelihood function. Based in this we find three different types of asymptotic hypothesis tests. Finally we will see how this can be applied to the binary regression model. Readings for this chapter is: AOS 6 AOS 9.1, 9.3, 9.4, 9.6, 9.7, 9.9 AOS 10.0-3, 10.6 "],["1.1-the-likelihood-function.html", "1.1 The likelihood function", " 1.1 The likelihood function In this section we introduce the likelihood function, the tool we will use in this chapter for doing inference. Let us begin with a simple example. Imagine that we toss a coin with an unknown probability \\(p\\) of landing on heads. Each time the coin lands on heads we record a 1 and each time it lands on tails a 0. We repeat the experiment 10 times and observe the sequence: ## [1] 0 0 1 0 0 1 0 1 0 0 We can calculate the probability of observing this exact sequence. The probability of observing 1 is \\(p\\) and of observing 0 is \\(1-p\\). Since the observations are independent, the total probability will be the product of each probability and so the probability is \\[ p^3(1-p)^7. \\] We can think of this as a function of \\(p\\) that tells us the probability of observing what we actually observed. For example, if \\(p=0.4\\) the probability is \\(1.79 \\cdot 10^{-3}\\), or if \\(p=0.5\\) the probability is \\(9.77 \\cdot 10^{-4}\\). The principle that we will follow to estimate parameters is that since \\(p=0.4\\) gives a higher probability to our observation than \\(p=0.5\\) we will prefer \\(0.4\\) over \\(0.5\\) as an estimate of \\(p\\). We call this function the likelihood function and denote it \\(L\\). That is \\[ L(p) := p^3(1-p)^7. \\] Figure 1.1: Likelihood of the sample In the picture we see that the likelihood function attains its highest value at \\(p=0.3\\). Using the above principle, we therefore prefer \\(0.3\\) as an estimate of \\(p\\) over any other value. This is the maximum likelihood estimate (MLE) of \\(p\\). Let us define the likelihood function in general. We have a parametric model of our observations, that is we assume that we observe random variables that are independent identically distributed, with a probability function (or density) \\(p_\\theta(x)\\). Here \\(\\theta\\) is an unknown parameter. For a random sample \\(x_1,\\ldots , x_n\\) of size \\(n\\) the likelihood function is \\[ L_n(\\theta) = \\prod_{i=1}^n p_\\theta(x_i). \\] That is, the likelihood is the probability (or density) of our observation, as a function of the unknown parameter. As a second example, say that we observe random variables \\(X_i\\) that are iid \\(\\mathsf N(\\mu,\\sigma^2)\\), where \\(\\mu\\) and \\(\\sigma^2\\) are unknown parameters. In other words, the joint density of an iid sample is \\[ p(x_1,\\ldots, x_n) = \\prod_{i=1}^n \\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-\\frac{1}{2\\sigma^2}(x_i-\\mu)^2}. \\] The likelihood function is then \\[ L_n(\\mu,\\sigma^2) = \\prod_{i=1}^n \\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-\\frac{1}{2\\sigma^2}(x_i-\\mu)^2}. \\] That is, the density and the likelihood function are written using the same formulas, but the density is a function of \\(x_i\\), while the likelihood is a function of the unknown parameters \\(\\mu\\) and \\(\\sigma^2\\). "],["1.2-maximum-likelihood-estimation.html", "1.2 Maximum likelihood estimation", " 1.2 Maximum likelihood estimation In this section we discuss how to find the maximum likelihood estimate. As in the previous section we consider an example: We will assume that time until something happens is exponentially distributed with parameter \\(\\lambda\\). Therefore the probability density of observing \\(T=t\\) is \\[ p_T(t) = \\lambda e^{-\\lambda t}. \\] On the other hand, we can also think of this as the likelihood function of the parameter \\(\\lambda\\), \\[ L(\\lambda) = \\lambda e^{-\\lambda t}. \\] If we have \\(n\\) independent observations, the joint density of these observations is the product of the marginal densities. Therefore the likelihood is also the product of the marginal likelihoods. That is, if we write \\(L_n(\\lambda)\\) for the likelihood of \\(n\\) observations, \\[ L_n(\\lambda) = \\prod_{i=1}^n \\lambda e^{-\\lambda t_i}. \\] Many times it is easier to instead consider the logarithm of the likelihood, the log-likelihood, \\[ l_n(\\lambda):=\\ln L_n(\\lambda) = \\sum_{i=1}^n( \\ln \\lambda - \\lambda t_i ) = n\\ln\\lambda - \\lambda\\sum_{i=1}^n t_i= n(\\ln\\lambda - \\lambda \\bar t) , \\] where we as usual let \\(\\bar t = \\frac{1}{n}\\sum_{i=1}^n t_i\\). Let us say we observe a sample of size 100. Figure 1.2: Histogram of the sample We define a function in R that calculates the log-likelihood logLn &lt;- function(param, sample){ n &lt;- length(sample) tbar &lt;- mean(sample) n*(log(param) - param*tbar) } Then we may calculate the log-likelihood of, for example, \\(\\lambda = 0.1\\), logLn(0.1, t) ## [1] -316.1482 Here the variable t is a vector that contains the observations. Let us plot the log-likelihood for a range of \\(\\lambda\\)-values. Figure 1.3: Log likelihood of the sample We can find the maximum likelihood estimate as the \\(\\lambda\\) that maximizes the likelihood (or log likelihood). That is: \\[ \\hat\\lambda = \\operatorname*{argmax}_\\lambda L_n(\\lambda) = \\operatorname*{argmax}_\\lambda l_n(\\lambda). \\] Since the likelihood is simple, we can find \\(\\hat \\lambda\\) directly: \\[ \\partial_\\lambda l_n(\\lambda)\\Big|_{\\lambda = \\hat\\lambda} = n\\left(\\frac{1}{\\hat \\lambda} - \\bar t\\right) = 0. \\] With solution \\(\\hat\\lambda =1/ \\bar t\\). For this sample: lambdaHat &lt;- 1/mean(t) lambdaHat ## [1] 0.1164284 We may also find the estimate using numerical optimization. optimResult &lt;- optim(1.0, function(lambda){ logLn(lambda,t)}, method = &quot;Brent&quot;, lower = 0.01, upper = 10.0, control = list(fnscale = -1.0)) optimResult$par ## [1] 0.1164284 "],["1.3-hypothesis-testing.html", "1.3 Hypothesis testing", " 1.3 Hypothesis testing In the previous section we saw how to estimate unknown parameters using maximum likelihood. While this is all well and good, we would like to also be able to test hypotheses regarding parameters. Consider Figure 1.4. Figure 1.4: Log likelihood of the sample There we have an estimate of \\(\\theta\\) which is \\(\\hat\\theta = 0.9\\). We would like to test \\(H_0: \\theta=\\theta_0 = 1\\) against \\(\\theta \\neq \\theta_0\\). This should be based on how far away, in some sense, the maximum likelihood estimate is from \\(\\theta_0\\). Looking at the figure, we can see three different ways of measuring how far away \\(\\hat\\theta\\) and \\(\\theta_0\\) are from each other. One way would be to measure the vertical distance between the log-likelihood function in \\(\\hat\\theta\\) and \\(\\theta_0\\). I.e. we would calculate: \\[ l(\\hat\\theta) - l(\\theta_0). \\] This is know as the likelihood ratio test. Another option is to calculate the horizontal distance between \\(\\hat\\theta\\) and \\(\\theta_0\\). I.e. to calculate the distance \\[ |\\hat\\theta - \\theta_0|. \\] This is known as the Wald test. Lastly we know that \\(\\partial_\\theta l(\\theta)|_{\\theta=\\hat\\theta} = 0\\). So we could calculate \\[ |\\partial_\\theta l(\\theta)|_{\\theta=\\theta_0}|, \\] and see how close it is to 0. This is known as the Score test. In the following sections we examine each test in detail. "],["1.4-likelihood-ratio-test.html", "1.4 Likelihood ratio test", " 1.4 Likelihood ratio test We are interested in testing the following hypotheses \\[ H_0:\\theta \\in \\Theta_0\\text{ vs. } H_1:\\theta \\in \\Theta_0^\\complement. \\] Here \\(\\Theta_0\\) is some set of parameter values. It could for example be that \\(\\Theta_0 = (-\\infty,\\theta_0)\\) or simply \\(\\Theta_0 = \\theta_0\\). We define the likelihood ratio as \\[ \\lambda_{\\text{LR}} := 2(l(\\hat\\theta) - l(\\hat\\theta_0)), \\] where \\[ l(\\hat\\theta_0) = \\sup_{\\theta\\in\\Theta_0}l(\\theta) \\] and \\[ l(\\hat\\theta) = \\sup_{\\theta\\in\\Theta}l(\\theta). \\] Since \\(l(\\hat\\theta)\\geq l(\\hat\\theta_0)\\), we have that \\(\\lambda_{\\text{LR}} \\geq 0\\) and data agrees well with \\(H_0\\) if \\(\\lambda_{\\text{LR}}\\) is small. Therefore the rejection region will be of the form \\(\\lambda_{\\text{LR}} &gt; k\\), where \\(k\\) is determined to get the correct size of the test. We can not say more in general, the continuation depends on the particular problem and tends to become complicated for anything but simple models. As before let us say that we have an iid sample from an exponential distribution and wish to test \\(H_0:~\\lambda = \\lambda_0\\) against \\(H_1:~\\lambda \\neq \\lambda_0\\), with \\(\\lambda_0=0.1\\). We have already seen that \\[ l(\\lambda) = n(\\ln \\lambda - \\lambda \\bar t) \\] and that \\(\\hat\\lambda = 1/\\bar t\\). Therefore \\[ l(\\lambda) = n \\left( \\ln\\lambda - \\frac{\\lambda}{\\hat\\lambda} \\right). \\] Now, we can write \\(l(\\hat\\lambda) = n\\left(\\ln\\hat\\lambda - 1\\right)\\). The likelihood ratio is then: \\[ \\lambda_{\\text{LR}} = 2(l(\\hat\\lambda) - l(\\lambda_0)) = 2n\\left( \\ln\\hat\\lambda - 1 - \\ln \\lambda_0 + \\frac{\\lambda_0}{\\hat\\lambda} \\right) = 2n\\left(\\ln \\frac{\\hat\\lambda}{\\lambda_0} + \\frac{\\lambda_0-\\hat\\lambda}{\\hat\\lambda}\\right). \\] Recall that we should reject \\(H_0\\) if \\(\\lambda_{\\text{LR}}&gt;k\\), and that \\(k\\) is set to get the correct size. But to do this we need to know the distribution of \\(\\lambda_{\\text{LR}}\\) and looking at the formula above, this seems difficult. Instead we search for something which is equivalent to \\(\\lambda_{\\text{LR}}&gt;k\\), but with a known distribution. Towards this we plot \\(\\lambda_{LR}(\\hat\\lambda)\\): Figure 1.5: Illustration of the likelihood ratio We see that \\(\\lambda_{LR}\\) is decreasing for \\(\\hat\\lambda &lt; \\lambda_0\\) and increasing for \\(\\hat\\lambda &gt; \\lambda_0\\) with minimum at \\(\\hat\\lambda = \\lambda_0\\). Therefore \\(\\lambda_{LR}&gt;k\\) is equivalent to \\(\\hat\\lambda &lt; k_L\\) or \\(\\hat\\lambda &gt; k_U\\), for some choices of \\(k_L\\) and \\(k_U\\). These should be determined so that the test gets the correct size. The size of the test is \\[\\begin{align} &amp;1- P_{\\lambda_0} \\left(k_L&lt; \\hat\\lambda &lt; k_U \\right) = 1- P_{\\lambda_0} \\left(k_L&lt; \\frac{1}{\\bar t} &lt; k_U \\right) = 1- P_{\\lambda_0} \\left(1/k_U&lt; \\bar t &lt; 1/k_L \\right)\\\\ =&amp; 1- P_{\\lambda_0} \\left(\\tilde k_L&lt; \\sum_{i=1}^n T_i&lt; \\tilde k_U \\right), \\end{align}\\] with \\(\\tilde k_L := n/k_U\\) and similarly for \\(\\tilde k_U\\). This probability can be calculated since we know that \\(\\sum_{i=1}^n T_i \\sim \\Gamma(n,\\lambda)\\). If we let \\(\\Gamma_{\\alpha}(n,\\lambda)\\) be the \\(\\alpha\\)-quantile of the gamma distribution, i.e. the number such that \\[ \\alpha = P\\left( \\Gamma(n,\\lambda )&gt; \\Gamma_\\alpha(n,\\lambda) \\right), \\] we see that the rejection region for a size \\(\\alpha\\) test is \\[ \\left\\{ T_i\\mid \\sum_{i=1}^n T_i &gt; \\Gamma_{\\alpha/2}(n,\\lambda_0) \\text{ or } \\sum_{i=1}^n T_i &lt; \\Gamma_{1-\\alpha/2}(n,\\lambda_0) \\right\\}. \\] or equivalently \\[ \\left\\{ \\hat \\lambda \\mid \\hat\\lambda &lt; \\frac{n}{\\Gamma_{\\alpha/2}(n,\\lambda_0)} \\text{ or } \\hat\\lambda &gt; \\frac{n}{\\Gamma_{1-\\alpha/2}(n,\\lambda_0)} \\right\\}. \\] Let us implement this: alpha &lt;- 0.05 lambda0 &lt;- 0.1 n &lt;- 100 upperCriticalValue &lt;- n / qgamma(alpha/2, shape = n, rate = lambda0) lowerCriticalValue &lt;- n / qgamma(1-alpha/2, shape = n, rate = lambda0) upperCriticalValue ## [1] 0.1229045 lowerCriticalValue ## [1] 0.08296762 In this case we had \\(\\hat\\lambda =\\) 0.1164 and so we would not reject \\(\\lambda \\neq 0.1\\). Another option is to calculate the p-value. Recall that the p-value is the smallest level for which \\(H_0\\) is rejected. That is, it is the \\(\\alpha\\) that solves \\(\\Gamma_{1-\\alpha/2}(n,\\lambda_0) = n/\\hat\\lambda\\). But, by definition \\[ \\alpha = 2 P \\left( \\Gamma(n,\\lambda_0) &lt; \\Gamma_{1-\\alpha/2}) \\right) \\] and therefore the p-value is \\[ 2P\\left( \\Gamma(n,\\lambda_0) &lt; \\frac{n}{\\hat\\lambda} \\right). \\] 2* pgamma(n/lambdaHat, shape = n, rate = lambda0) ## [1] 0.1471398 Again we see that we would not reject \\(H_0\\) on the 5%-level. "],["1.5-mathematical-aside-taylor-expansion.html", "1.5 Mathematical aside: Taylor expansion", " 1.5 Mathematical aside: Taylor expansion In this section we see how to approximate functions by polynomials. This will be useful when we search for the asymptotic distribution of the MLE. We would like to approximate a function \\(f(x)\\) by a polynomial \\(p(x)\\) of degree \\(n\\), around a point \\(x_0\\). That is, if \\(x\\approx x_0\\) we would like \\(f(x)\\approx p(x)\\). How should we choose \\(p(x)\\)? Let us write \\[ p(x) = c_0 + c_1(x-x_0) + c_2(x-x_0)^2 + \\cdots + c_n(x-x_0)^n. \\] To ensure that \\(f(x)\\approx p(x)\\) close to \\(x_0\\), we first require that \\[ f(x_0)=p(x_0)=c_0, \\] so that we have found the first parameter. To make the approximation better, we further require that the first derivatives are the same at \\(x_0\\), \\[ f&#39;(x_0) = p&#39;(x_0) = c_1 + 2c_2(x-x_0) + 3c_3(x-x_0)^2+\\cdots + nc_n(x-x_0)^{n-1}|_{x=x_0} = c_1, \\] and so the second parameter of \\(p(x)\\) has been determined. Continuing, we want \\[ f&#39;&#39;(x_0) = p&#39;&#39;(x_0) = 2c_2 + 2\\cdot 3c_3(x-x_0) + 3\\cdot 4c_4(x-x_0)^2 + \\cdot (n-1)nc_n(x-x_0)^{n-2}|_{x=x_0} = 2c_2, \\] so that \\(c_2 = f&#39;&#39;(x_0)/2\\). For the \\(k\\)th derivative, \\[ f^{(k)}(x_0) = p^{(k)}(x_0) = 2\\cdot 3\\cdots kc_k = k!c_k, \\] so that \\(c_k = f^{(k)}(x_0)/k!\\). To summarize, the order \\(n\\) polynomial approximation of \\(f(x)\\) close to \\(x_0\\) is \\[ f(x) \\approx f(x_0) + f&#39;(x_0)(x-x_0) + \\frac{f&#39;&#39;(x_0)}{2!}(x-x_0)^2 + \\cdots \\frac{f^{(n)}(x_0)}{n!}(x-x_0)^n. \\] This is known as a Taylor series. In fact, we can do even better by giving a formula for the error. Taylor’s theorem says that If \\(p(x)\\) is the order \\(n\\) polynomial approximation of \\(f(x)\\) as given above, then the approximation error is \\[ f(x) -p(x) = \\frac{f^{(n+1)}(\\xi)}{(n+1)!}(x-x_0)^{n+1}, \\] where \\(\\xi\\) is some number between \\(x\\) and \\(x_0\\). As an example, let us consider the approximation of \\(\\ln x\\) around 1. We begin by calculating the derivatives, \\[\\begin{align} \\ln 1 &amp;= 0 ,\\\\ \\partial_x\\ln x|_{x=1} &amp;= \\frac{1}{x}|_{x=1} = 1 \\\\ \\partial_x^2 \\ln x|_{x=1} &amp;= -\\frac{1}{x^2}|_{x=1} = -1,\\\\ \\partial_x^3 \\ln x|_{x=1} &amp;= \\frac{2}{x^3}|_{x=1} = 2. \\end{align}\\] Therefore the 3rd order polynomial approximation of \\(\\ln x\\) is \\[ \\ln x \\approx (x-1) - \\frac{1}{2}(x-1)^2 + \\frac{1}{3}(x-1)^3. \\] Figure 1.6: Taylor series approximation of ln x "],["1.6-asymptotic-distribution-of-the-mle.html", "1.6 Asymptotic distribution of the MLE", " 1.6 Asymptotic distribution of the MLE Here we examine the asymptotic properties of maximum likelihood estimators. But first let us recall some properties of the normal distribution, that will be used repeatedly in this chapter. If \\(X\\sim \\mathsf N(\\mu,\\sigma^2)\\) then \\(a+bX\\) also has a normal distribution and \\[\\begin{align*} E[a+bX] &amp;= a+bE[X] = a+b\\mu,\\\\ \\operatorname{Var}(a+bX) &amp;= b^2\\operatorname{Var}(X) = b^2\\sigma^2. \\end{align*}\\] That is \\(a+bX\\sim \\mathsf N(a+b\\mu,b^2\\sigma^2)\\). In particular, \\[ \\frac{X-\\mu}{\\sigma} \\sim \\mathsf N(0,1). \\] Now let us turn to the maximum likelhood estimators. To start, we imagine that we observe a random variable \\(X\\), from a parameterized distribution with density \\(p_\\theta\\). Our discussion will also be valid if we have a discrete random variable with a probability function \\(p_\\theta\\). Then we have a log-likelihood \\(l_x(\\theta):=\\ln p_\\theta(x)\\). Here we will calculate the expected value and variance of the random variable \\(l_X&#39;(\\theta):=\\partial_\\theta \\ln p_\\theta(X)\\). First note that since \\[ \\int p_\\theta(x)\\mathrm{d} x = 1, \\] therefore \\[ 0 = \\partial_\\theta \\int p_\\theta(x)\\mathrm{d} x = \\int \\partial_\\theta p_\\theta(x)\\mathrm{d} x = \\int \\partial_\\theta (\\ln p_\\theta(x))p_\\theta(x)\\mathrm{d} x = \\int l&#39;_x(\\theta) p_\\theta(x)\\mathrm{d} x = E[l_X&#39;(\\theta)]. \\] This means, \\[ E[l_X&#39;(\\theta)] =0. \\] To find the variance, we instead consider, \\[\\begin{align*} 0 &amp;= \\partial^2_\\theta \\int p_\\theta(x)\\mathrm{d} x = \\int \\partial_\\theta(l&#39;_x(\\theta) p_\\theta(x))\\mathrm{d} x =\\int (l&#39;&#39;_x(\\theta) p_\\theta(x) + (l&#39;_x(\\theta) )^2p_\\theta(x))\\mathrm{d} x \\\\ &amp;= E[l&#39;&#39;_X(\\theta)] + E[(l&#39;_X(\\theta))^2]. \\end{align*}\\] Therefore, \\[ \\operatorname{Var}(l&#39;_X(\\theta)) = E[(l&#39;_X(\\theta))^2] = -E[l&#39;&#39;_X(\\theta)] =: I(\\theta). \\] Here, \\(I(\\theta)\\) is called the Fisher information. This calculation was for a sample \\(X\\) of size 1. If we have an independent sample of size \\(n\\), we define the log-likelihood as \\[ l_n(\\theta) = \\sum_{i=1}^n l_{X_i}(\\theta). \\] Then we can also calculate \\[\\begin{align*} E[l&#39;_n(\\theta)] &amp;= E[\\partial_\\theta \\sum_{i=1}^n l_{X_i}(\\theta)] =\\sum_{i=1}^n E[\\partial_\\theta l_{X_i}(\\theta)] = 0,\\\\ Var(l&#39;_n(\\theta)) &amp;= Var(\\partial_\\theta \\sum_{i=1}^n l_{X_i}(\\theta)) = \\sum_{i=1}^n Var(l_{X_i}&#39;(\\theta)) = n I(\\theta) =:I_n(\\theta). \\end{align*}\\] Let us now recall the law of large numbers and the central limit theorem. They state that if \\(X_1,\\ldots X_n\\) are iid random variables with mean \\(\\mu\\) and finite variance \\(\\sigma^2\\), then for large \\(n\\) \\[\\begin{align*} \\frac{1}{n}\\sum_{i=1}^n X_i&amp; \\overset{asym.}{\\sim} \\mu,\\\\ \\frac{1}{\\sqrt{n}}\\Big(\\sum_{i=1}^n X_i-\\mu\\Big)&amp; \\overset{asym.}{\\sim} \\mathsf N(0,\\sigma^2). \\end{align*}\\] These asymptotic results in practice mean that we approximate the distribution of the left side with the right side if \\(n\\) is large. For example \\[ P\\left(\\frac{1}{\\sqrt{n}}\\Big(\\sum_{i=1}^n X_i-\\mu\\Big) \\leq x \\right) \\approx P\\left(\\sigma Z\\leq x \\right), \\] with \\(Z\\sim \\mathsf N(0,1)\\). Since \\(l&#39;_n(\\theta)\\) is a sum of random variables \\(l_{X_i}&#39;(\\theta)\\), and we now know the expected value and variance, we can apply the law of large numbers and central limit theorem to get the following asymptotics: \\[\\begin{align*} \\frac{1}{n}l&#39;_n(\\theta) &amp;\\overset{asym.}{\\sim} 0, \\\\ \\frac{1}{\\sqrt{n}}l&#39;_n(\\theta) &amp;\\overset{asym.}{\\sim} \\mathsf{N}(0,I(\\theta)), \\\\ -\\frac{1}{n}l&#39;&#39;_n(\\theta) &amp;\\overset{asym.}{\\sim} I(\\theta). \\end{align*}\\] Now we are ready to find the asymptotic distribution of \\(\\hat\\theta_n\\). We make a first-order Taylor expansion of \\(l_n&#39;(\\hat\\theta_n)\\) around \\(\\theta\\), \\[ l&#39;_n(\\hat\\theta_n) \\approx l&#39;_n(\\theta) + (\\hat\\theta_n-\\theta)l&#39;&#39;_n(\\theta). \\] Then use that \\(l&#39;_n(\\hat\\theta_n)=0\\) and rewrite as \\[ \\sqrt{n} (\\hat\\theta_n-\\theta) \\approx -\\frac{l&#39;_n(\\theta)/\\sqrt{n}}{l&#39;&#39;_n(\\theta)/n}. \\] Now, using the above asymptotics we arrive at: \\[ \\sqrt{n} (\\hat\\theta_n-\\theta) \\overset{asym.}{\\sim} \\frac{\\mathsf N(0,I(\\theta))}{I(\\theta)} \\overset{d}{=} \\mathsf N(0,I^{-1}(\\theta)). \\] In particular, this implies that \\(\\hat\\theta_n - \\theta \\overset{asym.}{\\sim } 0\\), or in other words that \\(\\hat\\theta_n\\) is a consistent estimator of \\(\\theta\\). One problem with the above is however that \\(I(\\theta)\\) is often difficult to calculate. However \\(-l&#39;&#39;_n(\\theta)/n=:\\hat I(\\theta)\\) is a consistent estimator of \\(I(\\theta)\\). Then we may also write: \\[ \\sqrt{n \\hat I(\\theta)}(\\hat\\theta_n - \\theta) \\overset{asym.}{\\sim} \\mathsf N(0,1). \\] A further problem is that \\(\\theta\\) is in general unknown. But since \\(\\hat\\theta_n\\) is consistent, we may simply replace \\(\\theta\\) by \\(\\hat\\theta_n\\), \\[ \\sqrt{n \\hat I(\\hat\\theta_n)}(\\hat\\theta_n - \\theta) \\overset{asym.}{\\sim} \\mathsf N(0,1). \\] This is an amazing result. Without knowing in detail how \\(\\hat\\theta\\) is determined from the sample; perhaps from some numerical optimization, we can say what the large-sample distribution is. We can rewrite this as \\[ \\hat \\theta_n \\overset{asym.}{\\sim} \\mathsf N\\left(\\theta, \\frac{1}{n \\hat I(\\hat\\theta_n)}\\right). \\] Now let us apply this to the example of the exponential distribution. We had that \\(\\hat\\lambda_n = 1/\\bar t\\). Further \\[ l&#39;&#39;(\\lambda) = -\\frac{1}{\\lambda^2}. \\] Thus, the Fisher information is simply \\[ I(\\lambda) = -E[l&#39;&#39;_T(\\lambda)] = \\frac{1}{\\lambda^2}. \\] The asymptotic distribution of \\(\\hat \\lambda_n\\) is therefore: \\[ \\hat\\lambda_n \\approx \\mathsf N\\left(\\lambda, \\frac{\\hat\\lambda_n^2}{n} \\right). \\] "],["1.7-the-delta-method.html", "1.7 The delta method", " 1.7 The delta method In this section we discuss how to find the asymptotic distribution of a function of the estimate. Let us assume that we already know that \\[ \\sqrt{n} (\\hat\\theta_n - \\theta) \\overset{asym.}{\\sim} N(0,\\sigma^2). \\] This might be because \\(\\hat\\theta_n\\) is the MLE and we have used the results from the previous section or we have applied some central limit theorem. We have a function \\(f\\) and we would like to know the asymptotic distribution of \\(f(\\hat\\theta_n)\\). Let us again write a Taylor expansion \\[ f(\\hat\\theta_n) \\approx f(\\theta) + f&#39;(\\theta)(\\hat\\theta_n - \\theta). \\] Rearranging and multiplying by \\(\\sqrt n\\) gives, \\[ \\sqrt{n}(f(\\hat\\theta_n) - f(\\theta)) \\approx f&#39;(\\theta)\\sqrt{n}(\\hat\\theta_n - \\theta). \\] The right side is asymptotically normal, by our assumption. We have then arrived at the first order delta method: \\[ \\sqrt{n}(f(\\hat\\theta_n) - f(\\theta)) \\overset{asym.}{\\sim} N(0,\\sigma^2f&#39;(\\theta)^2) . \\] For this to make sense we need that \\(f&#39;(\\theta)\\neq 0\\). If this is not the case we can instead do a second order Taylor expansion \\[ f(\\hat\\theta_n) \\approx f(\\theta) + f&#39;(\\theta)(\\hat\\theta_n - \\theta) + \\frac{f&#39;&#39;(\\theta)}{2}(\\hat\\theta_n - \\theta)^2 = f(\\theta) + \\frac{f&#39;&#39;(\\theta)}{2}(\\hat\\theta_n - \\theta)^2. \\] Rearranging gives, \\[ n(f(\\hat\\theta_n) - f(\\theta)) \\approx \\frac{f&#39;&#39;(\\theta)}{2}(\\sqrt{n}(\\hat\\theta_n-\\theta))^2. \\] We assumed that \\(\\sqrt{n} (\\hat\\theta_n - \\theta) \\overset{asym.}{\\sim} N(0,\\sigma^2)\\). The continuous mapping theorem (not covered here) states that if \\(f\\) is a continuous function and if \\(X_n \\overset{asym.}{\\sim} X\\), then \\(f(X_n) \\overset{asym.}{\\sim} f(X)\\). Therefore, if we let \\(Z\\sim N(0,1),\\) we can write our assumption as \\(\\sqrt{n} (\\hat\\theta_n - \\theta) \\overset{asym.}{\\sim} \\sigma Z\\) and thus, recalling that the square of a standard normal random variable has a \\(\\chi^2_1\\)-distribution, \\[ (\\sqrt{n}(\\hat\\theta_n-\\theta))^2 \\overset{asym.}{\\sim} \\sigma^2Z^2 \\overset{d}{=} \\sigma^2 \\chi^2_1. \\] With that we get the second order Delta method: \\[ n(f(\\hat\\theta_n) - f(\\theta)) \\overset{asym.}{\\sim} \\frac{f&#39;&#39;(\\theta)}{2}\\sigma^2\\chi_1^2. \\] Now let us apply the delta method to the exponential distribution. We would like to estimate the probability that the time until the next event is larger than 10. That is the probability \\[ p = P(T&gt;10) = e^{-10\\lambda}. \\] The MLE follows from the invariance principle of maximum likelihood, i.e. \\(\\hat p = e^{-10\\hat\\lambda}\\). The distribution of \\(\\hat p\\) can be found by the delta method if we let \\(p=f(\\lambda) = e^{-10\\lambda}\\). Then \\(f&#39;(\\lambda) = -10e^{-10\\lambda} = -10p\\). In this case \\(f&#39;(\\lambda)\\neq 0\\), so we may apply the first order delta method. Recall from the previous section that \\[ \\sqrt n\\left( \\hat\\lambda_n - \\lambda \\right) \\overset{asym.}{\\sim} \\mathsf N(0, \\lambda^2), \\] that is, the \\(\\sigma^2\\) appearing in the delta method is \\(\\lambda^2\\). Now, applying the delta method gives \\[ \\sqrt n(\\hat p_n- p) \\overset{asym.}{\\sim} \\mathsf N(0,100p^2\\lambda^2). \\] As usual, we may replace unknown parameters with a consistent estimate, i.e. \\(p\\) with \\(\\hat p\\) and \\(\\lambda\\) with \\(\\hat\\lambda\\). Therefore, \\[ \\sqrt n(\\hat p_n- p) \\overset{asym.}{\\sim} \\mathsf N(0,100\\hat p^2\\hat\\lambda^2) = \\mathsf N(0,0.132), \\] or \\[ \\hat p_n \\overset{asym.}{\\sim} \\mathsf N(p,0.132/n). \\] "],["1.8-wilks-test.html", "1.8 Wilks’ test", " 1.8 Wilks’ test In the previous sections we found the asymptotic distribution of \\(\\hat\\theta_n\\). Here we seek the asymptotic distribution of the likelihood ratio \\(-2(l_n(\\theta_0) - l_n(\\hat\\theta_n)))\\). The reason is that, as we have seen, finding the exact distribution is difficult. If we have the approximate, asymptotic, distribution, we can use that to do for example hypothesis testing. For ease of notation we suppress the \\(n\\) and write \\(l\\) and \\(\\hat\\theta\\). We will use the following results, that we have seen before: \\[\\begin{align*} l&#39;(\\hat\\theta) &amp;= 0,\\\\ -\\frac{1}{n}l&#39;&#39;(\\hat\\theta) &amp;\\overset{asym.}{\\sim} -\\frac{1}{n}l&#39;&#39;(\\theta) \\overset{asym.}{\\sim} I(\\theta),\\\\ \\sqrt{n}(\\hat\\theta - \\theta) &amp;\\overset{asym.}{\\sim} I^{-1/2}(\\theta )Z, \\end{align*}\\] with \\(Z\\sim \\mathsf N(0,1)\\). Just as in the delta method, we now do a Taylor expansion of \\(l(\\theta)\\) around \\(\\hat\\theta\\): \\[\\begin{align} l(\\theta) \\approx&amp; l(\\hat \\theta) + l&#39;(\\hat\\theta)(\\theta-\\hat\\theta) + \\frac{l&#39;&#39;(\\hat\\theta)}{2}(\\theta-\\hat\\theta) ^2\\\\ =&amp; l(\\hat\\theta) + \\frac{l&#39;&#39;(\\hat\\theta)}{2}(\\theta-\\hat\\theta) ^2 = l(\\hat \\theta) +\\frac{1}{2} \\frac{1}{n}l&#39;&#39;(\\hat\\theta)n(\\theta-\\hat\\theta) ^2\\\\ \\overset{asym.}{\\sim} &amp; l(\\hat\\theta) -\\frac{1}{2} I(\\theta)I^{-1}(\\theta)Z^2 \\overset{d}{=} l(\\hat\\theta) - \\frac{1}{2}\\chi_1^2. \\end{align}\\] In other words, for a large sample, \\[ \\lambda_{LR} = 2( l_n(\\hat\\theta)-l_n(\\theta_0))\\overset{asym.}{\\sim} \\chi_1^2. \\] Which is Wilks’ theorem. Let us again apply this to the exponential distribution. Of course, we have already found the exact likelihood ratio test, so we would in reality not use an asymptotic test in this case. Nonetheless, we can calculate it as: lrStatistic &lt;- 2*(logLn(optimResult$par, t) - logLn(0.1, t)) lrStatistic ## [1] 2.200653 Recall that we reject \\(H_0\\) if \\(\\lambda_{LR}\\) is large. Therefore the p-value is 1 - pchisq(lrStatistic, 1) ## [1] 0.1379523 "],["1.9-walds-test.html", "1.9 Wald’s test", " 1.9 Wald’s test Another way to measure if \\(\\hat\\theta\\) agrees with the null hypothesis is to calculate \\(\\hat\\theta - \\theta_0\\). If this is large, in absolute value, the test should reject the null hypothesis. We have already seen that, under the assumption of \\(H_0\\) \\[ \\frac{\\hat\\theta- \\theta_0}{\\textrm{Sd}(\\hat\\theta)} \\overset{asym.}{\\sim} \\mathsf{N}(0,1), \\] where the standard deviation can be calculated as \\[ Sd(\\hat\\theta) = (n\\hat I(\\hat\\theta))^{-1/2}. \\] Therefore the test can be done by comparing the left hand-side with the appropriate quantile of the Normal distribution. Note that this implies also that \\[ \\frac{(\\hat \\theta - \\theta_0)^2}{Var(\\hat\\theta)} \\overset{asym.}{\\sim} \\chi^2_1, \\] which is similar to Wilks’ test. Let us apply this again to the exponential distribution. We have already seen that \\(I(\\lambda) = 1/\\lambda^2\\) and so the standard deviation is: \\[ Sd(\\hat \\lambda) = \\frac{\\hat\\lambda}{\\sqrt n}. \\] Since we have a two-sided test, the test statistic is: \\[ \\frac{\\left| \\hat\\lambda - \\lambda_0 \\right|}{\\hat\\lambda / \\sqrt n }. \\] waldStatistics &lt;- abs(lambdaHat - lambda0)/(lambdaHat/sqrt(n)) waldStatistics ## [1] 1.41103 This is now compared to \\(z_{\\alpha/2}=\\) 1.96 if \\(\\alpha = 0.05\\) and so we do not reject \\(H_0\\). The p-value is: 2*(1-pnorm(waldStatistics)) ## [1] 0.1582359 Here we found the standard deviation of \\(\\hat \\lambda\\) by knowing the asymptotic distribution of the MLE. It is also possible to calculate this directly from the delta method. That is, we know that \\[\\begin{align} E\\left[ T\\right] &amp;= \\frac{1}{\\lambda},\\\\ Var\\left( T \\right) &amp;= \\frac{1}{\\lambda^2}. \\end{align}\\] So by the central limit theorem (\\(\\bar t\\) is a sum of random variables): \\[ \\sqrt n (\\bar t - 1/\\lambda) \\overset{asym.}{\\sim} N(0,1/\\lambda^2). \\] If \\(f(x)=1/x\\), then \\(\\hat\\lambda = f(\\bar t) = 1/\\bar t\\) and applying the delta method gives, \\[ \\sqrt n (\\hat\\lambda - \\lambda) \\overset{asym.}{\\sim} N(0,f&#39;(\\bar t)^2/\\lambda^2) = N(0,f&#39;(1/\\lambda)^2/\\lambda^2) = N(0,\\lambda^2). \\] This agrees with what we obtained previously. "],["1.10-score-test.html", "1.10 Score test", " 1.10 Score test In this section we discuss the score test, sometimes called the Rao test or the Lagrange multiplier (LM) test. If \\(\\hat\\theta\\) is close to \\(\\theta_0\\) then we should have that \\(l&#39;(\\theta_0)\\approx 0\\). The score test is therefore that we reject \\(H_0\\) if \\(\\left| l&#39;(\\theta_0)\\right|&gt;k\\), for some \\(k\\) chosen depending on the size of the test. We can use the asymptotics we already calculated, that is \\[ \\frac{l_n&#39;(\\theta_0)}{\\sqrt{I_n(\\theta_0)}} \\overset{asym.}{\\sim} \\mathsf N(0,1). \\] So the score test of size \\(\\alpha\\) is to reject \\(H_0\\) if \\[ \\frac{\\left|l_n&#39;(\\theta_0)\\right|}{\\sqrt{I_n(\\theta_0)}}&gt;z_{\\alpha/2}. \\] Note that this test statistic does not require us to calculate the MLE \\(\\hat\\theta\\). By squaring the test statistic we get, as in the previous section, a test statistic that is \\(\\chi^2_1\\)-distributed, similar to Wilks’ test. We apply this to the exponential distribution. We have already calculated everything we need so it is just a matter of putting it together: lp &lt;- n*( 1/lambda0 - mean(t) ) fisherInfo &lt;- n/lambda0^2 scoreStatistic &lt;- lp / sqrt(fisherInfo) scoreStatistic ## [1] 1.41103 With p-value: 2*(1-pnorm(scoreStatistic)) ## [1] 0.1582359 In this particular case the score test and the Wald test are exactly the same. This is not true in general. "],["1.11-confidence-intervals.html", "1.11 Confidence intervals", " 1.11 Confidence intervals We have derived a number of different tests. In principle all of them can be turned into confidence intervals since there is a correspondence between hypothesis tests and confidence intervals. Let us first examine how we can use the Wald test to construct confidence intervals. The Wald test is based on that for large \\(n\\): \\[ \\frac{\\hat\\theta- \\theta}{\\textrm{Sd}(\\hat\\theta)} \\overset{asym.}{\\sim} \\mathsf{N}(0,1), \\] Therefore we can write \\[ 1-\\alpha = P\\left( -z_{\\alpha/2} &lt; \\frac{\\hat\\theta- \\theta}{\\textrm{Sd}(\\hat\\theta)} \\leq z_{\\alpha/2} \\right) = P\\left( \\hat\\theta -\\textrm{Sd}(\\hat\\theta) z_{\\alpha/2}\\leq \\theta \\leq \\hat\\theta + \\textrm{Sd}(\\hat\\theta) z_{\\alpha/2}\\right). \\] Which means that \\(\\left[\\hat\\theta -\\textrm{Sd}(\\hat\\theta) z_{\\alpha/2}, \\hat\\theta +\\textrm{Sd}(\\hat\\theta) z_{\\alpha/2}\\right]\\) is a \\(1-\\alpha\\) confidence interval for \\(\\theta\\). Let us derive the same CI in a slightly different way. In the Wald test we would accept \\(H_0: \\theta=\\theta_0\\) if \\(\\left| \\hat\\theta - \\theta_0 \\right|/Sd(\\hat\\theta)&lt;z_{\\alpha/2}\\). Solving this for \\(\\theta_0\\) gives \\[ \\hat\\theta -\\textrm{Sd}(\\hat\\theta) z_{\\alpha/2}\\leq \\theta_0 \\leq \\hat\\theta + \\textrm{Sd}(\\hat\\theta) z_{\\alpha/2}. \\] If we replace \\(\\theta_0\\) with \\(\\theta\\) we obtain the CI above. Therefore we may think of the CI as being the set of \\(\\theta_0\\) that we would accept in a hypothesis test. The same principle can be applied to convert any hypothesis test to a corresponding CI. For example the score test accepts \\(H_0\\) if \\[ \\frac{\\left| l&#39;_n(\\theta_0)\\right |}{\\sqrt{I_n(\\theta_0)}}&lt; z_{\\alpha/2}, \\] and so solving this for \\(\\theta_0\\) gives a CI. However, in most cases it is not possible to obtain a closed form solution and we have solve it numerically. Let us as an example do it for the exponential distribution: \\[\\begin{align} l&#39;_n(\\lambda) &amp;= n\\left( \\frac{1}{\\lambda} - \\frac{1}{\\hat\\lambda} \\right),\\\\ I_n(\\lambda) &amp;= - l&#39;&#39;_n(\\lambda) = \\frac{n}{\\lambda^2}. \\end{align}\\] We plot the score statistic as a function of \\(\\lambda\\): Figure 1.7: Score statistic and confidence intervall To find the CI we need to solve \\[ \\frac{\\left| l&#39;_n(\\lambda)\\right |}{\\sqrt{I_n(\\lambda)}} = z_{\\alpha/2}, \\] in terms of \\(\\lambda\\). Looking at the figure, this has two solutions, one for \\(\\lambda &gt; \\hat\\lambda\\) and one for \\(\\lambda &lt; \\hat\\lambda\\). These will be the left and right endpoints of the CI. alpha = 0.05 scoreStatistic &lt;- function(lambda){ abs( n*(1/lambda - 1/lambdaHat) ) / sqrt( n/lambda^2 ) } f &lt;- function(lambda){ scoreStatistic(lambda) - qnorm(1-alpha/2) } rootResults &lt;- uniroot(f, interval = c(lambdaHat-0.05,lambdaHat)) leftCILimit &lt;- rootResults$root rootResults &lt;- uniroot(f, interval = c(lambdaHat+0.05,lambdaHat)) rightCILimit &lt;- rootResults$root leftCILimit ## [1] 0.09360885 rightCILimit ## [1] 0.1392479 We can compare this to the Wald based CI: alpha = 0.05 z = qnorm(1-alpha/2) leftCILimit &lt;- lambdaHat - qnorm(1-alpha/2)*lambdaHat/sqrt(n) rightCILimit &lt;- lambdaHat + qnorm(1-alpha/2)*lambdaHat/sqrt(n) leftCILimit ## [1] 0.09360885 rightCILimit ## [1] 0.1392479 Again, in this particular example, the two intervals are the same. "],["1.12-an-application.html", "1.12 An application", " 1.12 An application Here we present an application of what we have learned in this chapter. Consider the binary regression model, where we observe random variables \\(Y_i\\) that take on the values 0 or 1. The distribution of \\(Y_i\\) depends on the value of a covariate \\(X_i\\), \\[ P(Y_i=1\\mid X_i=x_i) = s(\\beta x_i), \\] where \\(\\beta\\) is a parameter and \\[ s(x) = \\frac{e^{x}}{1+e^{x}} \\] is the logistic function. We have a sample of size \\(n=\\) 1000 and we would like to do inference on \\(\\beta\\). First we plot our data. Figure 1.8: Observed sample We will estimate \\(\\beta\\) by maximum likelihood, so we begin by writing the likelihood of observation \\(i\\), \\[ L_i(\\beta) = s(\\beta x_i)^{y_i}\\left( 1-s(\\beta x_i)\\right)^{(1-y_i)} . \\] So that the log-likelihood of the \\(i\\)th observation is \\[ l_i(\\beta) = y_i\\ln s(\\beta x_i) + (1-y_i)\\ln ( 1-s(\\beta x_i)). \\] Since we assume that our observations are iid, the total log-likelihood is then \\[ l(\\beta) = \\sum_{i=1}^n l_i(\\beta). \\] Let us implement what we have so far. s &lt;- function(x) { exp(x) / (exp(x) + 1) } logLn &lt;- function(beta){ x &lt;- data.df$x y &lt;- data.df$y s &lt;- s(x * beta) sum(y * log(s) + (1 - y) * log(1 - s)) } To maximize the likelihood there are now two options. Either we ask the computer to solve \\[ \\underset{\\beta}{\\text{argma}x}~ l(\\beta), \\] or we calculate and solve \\(l&#39;(\\beta)=0\\). For practice we do both ways here. optimResult &lt;- optim( 1.0, logLn, method = &quot;Brent&quot;, lower = 0.0, upper = 3.0, control = list(fnscale = -1.0) ) betahat &lt;- optimResult$par betahat ## [1] 1.79158 For the second way we need \\[\\begin{align} s&#39;(x) &amp;= \\frac{e^x}{\\left(1+e^x\\right)^2},\\\\ l&#39;(\\beta) &amp;= \\frac{x y s&#39;(x \\beta )}{s(x \\beta )}-\\frac{x (1-y) s&#39;(x \\beta )}{1-s(x \\beta )}. \\end{align}\\] In R: sp &lt;- function(x){ exp(x)/(1+exp(x))^2 } logLp &lt;- function(beta){ x &lt;- data.df$x y &lt;- data.df$y s &lt;- s(x * beta) sp &lt;- sp(x * beta) sum( -x * (1-y) * sp / (1-s) + x * y * sp / s ) } rootResults &lt;- uniroot( logLp, interval = c(0,3) ) rootResults$root ## [1] 1.791589 Both methods giving the same result. To confirm that we indeed found the MLE we plot the log-likelihood. Figure 1.9: Log likelihood of the sample Figure 1.10: Observed sample and fitted model Now we turn to hypothesis testing. Let us say we want to test \\(H_0: \\beta = 2\\) against \\(H_1:\\beta \\neq 2\\). First we do the asymptotic likelihood ratio test. So we need to calculate \\(\\lambda_{\\text{LR}}\\): lr &lt;- function(beta0){ 2*(logLn(betahat) - logLn(beta0)) } lr(2.0) ## [1] 2.97212 If \\(H_0\\) is true, this is an observation of a \\(\\chi_1^2\\)-distributed random variable. Therefore the p-value is 1 - pchisq(lr(2.0), 1) ## [1] 0.0847108 Next we do a Wald’s test. For this we need an estimate of the standard deviation of the MLE. Perhaps the easiest way is to calculate the Fisher information, that is \\(-l&#39;&#39;(\\beta)\\). Here there are again two options, we can do it numerically or exactly. First we calculate it numerically: observedFisherInfo &lt;- function(beta){ drop(-pracma::hessian(logLn, beta)) } observedFisherInfo(betahat) ## [1] 72.64601 Calculating the second derivative exactly involves more work but is preferable whenever possible. We get, \\[ l&#39;&#39;(\\beta)=(1-y) \\left(-\\frac{x^2 \\sigma &#39;(x \\beta )^2}{(1-\\sigma (x \\beta )^2}-\\frac{x^2 \\sigma &#39;&#39;(x \\beta )}{1-\\sigma (x \\beta )}\\right)+y \\left(-\\frac{x^2 \\sigma &#39;(x \\beta )^2}{\\sigma (x \\beta )^2}+\\frac{x^2 \\sigma &#39;&#39;(x \\beta )}{\\sigma (x \\beta )}\\right) \\] Implemented in R: spp &lt;- function(x){ -exp(x)*(exp(x)-1)/(exp(x)+1)^3 } logLpp &lt;- function(beta){ x &lt;- data.df$x y &lt;- data.df$y s &lt;- s(x*beta) sp &lt;- sp(x*beta) spp &lt;- spp(x*beta) sum( (1-y)*(-x^2*sp^2/(1-s)^2 - x^2*spp/(1-s))+y*(-x^2*sp^2/s^2 + x^2*spp/s) ) } observedFisherInfo &lt;- function(beta){ -logLpp(beta) } observedFisherInfo(betahat) ## [1] 72.64589 Recall that Wald’s test statistic is standard normal under \\(H_0\\). So we may calculate the p-value: zWald &lt;- function(beta0){ abs(betahat- beta0)*sqrt(observedFisherInfo(betahat)) } 2 * ( 1 - pnorm( zWald(2.0) ) ) ## [1] 0.07566359 We might also do a Score test. Here, all we need is \\(l&#39;\\) and \\(l&#39;&#39;\\), which we have already calculated. The score statistic is again standard normal under \\(H_0\\). zScore &lt;- function(beta0){ abs(pracma::grad(logLn,beta0)/sqrt(observedFisherInfo(beta0))) } 2 * ( 1 - pnorm(zScore(2.0) ) ) ## [1] 0.0754018 Lastly, we might calculate a CI on \\(\\beta\\). Using the Wald’s statistic, this would be: alpha &lt;- 0.05 leftCILimit &lt;- betahat - qnorm(1-alpha/2) / sqrt(observedFisherInfo(betahat)) rightCILimit &lt;- betahat + qnorm(1-alpha/2) / sqrt(observedFisherInfo(betahat)) leftCILimit ## [1] 1.561625 rightCILimit ## [1] 2.021534 For a score based CI we first plot the score statistic. Figure 1.11: Observed sample and fitted model We need to find the points where the score statistic is \\(z_{\\alpha/2}\\), which are the limits of the CI. alpha = 0.05 f &lt;- function(beta){ zScore(beta) - qnorm(1-alpha/2) } rootResults &lt;- uniroot(f, interval = c(betahat-1,betahat)) leftCILimit &lt;- rootResults$root rootResults &lt;- uniroot(f, interval = c(betahat+1,betahat)) rightCILimit &lt;- rootResults$root leftCILimit ## [1] 1.561916 rightCILimit ## [1] 2.021279 "],["1.13-summary.html", "1.13 Summary", " 1.13 Summary Since there is a lot of new material in this chapter, in this section we summarize what we have learned. First, recall that the following is equivalent \\[\\begin{align} X &amp; \\sim \\mathsf N(\\mu,\\sigma^2),\\\\ \\frac{X-\\mu}{\\sigma} &amp; \\sim \\mathsf N(0,1). \\end{align}\\] Also recall from the B-course that for a large sample \\[ \\bar X \\sim \\mathsf N(\\mu,\\sigma^2/n). \\] The variance is like this because of \\[ Var(\\bar X) = Var\\left( \\frac{1}{n}\\sum_{i=1}^n X_i \\right) = \\frac{1}{n^2}\\sum_{i=1}^n Var(X_i) = \\frac{\\sigma^2}{n}. \\] If we use the above, we get that \\[ \\frac{\\bar X-\\mu}{\\sigma/\\sqrt{n}} \\sim \\mathsf N(0,1). \\] This we used to construct what we called the large sample test. That is, we should accept \\(\\mu=\\mu_0\\) as opposed to \\(\\mu\\neq \\mu_0\\) if \\[ \\frac{|\\bar X-\\mu_0|}{\\sigma/\\sqrt{n}}&lt;z_{\\alpha/2}. \\] In this chapter we have seen that this is true in more generallity, whenever we have an MLE. That is \\[ \\hat\\theta \\overset{asym.}{\\sim} \\mathsf N(\\theta,\\sigma^2_{\\hat\\theta}), \\] the same as \\[ \\frac{\\hat\\theta - \\theta}{\\sigma_{\\hat\\theta}} \\overset{asym.}{\\sim} \\mathsf N(0,1). \\] So we can again use this for hypothesis testing or CI, the only remaining challenge is finding \\(\\sigma_{\\hat\\theta}\\). We have a couple of different tools to do this. Let us consider yet another example. We have a sample of size \\(n\\) from \\(\\mathsf{Be}(p)\\). We could also say that we have a sample of size 1 from \\(\\mathsf{Bin}(n,p)\\), the analysis will be the same. But let us stick with \\(\\mathsf{Be}(p)\\). We want to do inference on \\(p\\). The MLE of \\(p\\) is \\(\\hat p = \\bar x\\) and we know that \\[ \\hat p \\overset{asym.}{\\sim} \\mathsf N(p,\\sigma^2_{\\hat p}). \\] Here we can find \\(\\sigma^2_{\\hat p}\\) by direct calculation. \\[ \\sigma^2_{\\hat p} = Var(\\hat p) = Var(\\bar X) = \\frac{1}{n}Var(X_i) = \\frac{p(1-p)}{n}. \\] Therefore, \\[ \\hat p \\overset{asym.}{\\sim} \\mathsf N\\left(p,\\frac{p(1-p)}{n}\\right). \\] If we write this on the form of a test statistic, \\[ \\frac{\\hat p - p}{\\sqrt{\\frac{p(1-p)}{n}}}\\overset{asym.}{\\sim} \\mathsf N\\left(0,1\\right). \\] The problem is that the denominator contains the unkown \\(p\\). Since \\(\\hat p\\) is the MLE, it is consistent, and we may therefore also say that, \\[ \\hat p \\overset{asym.}{\\sim} \\mathsf N\\left(p,\\frac{\\hat p(1-\\hat p)}{n}\\right). \\] Or, \\[ \\frac{\\hat p - p}{\\sqrt{\\frac{\\hat p(1-\\hat p)}{n}}} \\overset{asym.}{\\sim} \\mathsf N\\left(0,1\\right), \\] which can be used for constructing the Wald test. What this then means is that, if \\(n\\) is large, and we where to estimate \\(p\\) with \\(\\hat p\\) for many different samples, the distribution of the estimates would be approximately distributed as \\(\\mathsf N(0,1)\\). Let us verify this with a simulation. set.seed(42) n &lt;- 100 p0 &lt;- 0.5 replications &lt;- 1000 p.hats &lt;- array(dim = replications) for (i in seq_len(replications)) { x &lt;- sample(c(0,1), size = n, replace = TRUE) p.hats[i] &lt;- mean(x) } wald &lt;- (p.hats - p0)/sqrt(p.hats*(1-p.hats)/n) Figure 1.12: Simulated density of the Wald statistic and the standard normal density In this case we were lucky that we could calculate \\(Var(\\hat p)\\) directly. Another path is to use the Fisher information. We found that \\[ \\hat \\theta \\overset{asym.}{\\sim} \\mathsf N\\left(\\theta, \\frac{1}{nI(\\theta)}\\right). \\] The Fisher information is \\[ I(\\theta) = -E[l&#39;&#39;(\\theta)]. \\] Note that here \\(I(\\theta)\\) is the Fisher information of a sample of size 1, and so also \\(l(\\theta)\\) is the log-likelihood of a sample of size 1. Here, \\[\\begin{align} L(p) &amp;= p^{x}(1-p)^{1-x},~0\\leq p \\leq 1,\\\\ l(p) &amp;= x\\ln p + (1-x)\\ln (1-p),\\\\ l&#39;(p) &amp;= \\frac{x}{p} - \\frac{1-x}{1-p},\\\\ l&#39;&#39;(p) &amp;= -\\frac{x}{p^2} -\\frac{1-x}{(1-p)^2},\\\\ I(p) &amp;= -E[l&#39;&#39;(p)] = E\\left[ \\frac{X}{p^2} + \\frac{1-X}{(1-p)^2} \\right]\\\\ &amp;= \\frac{p}{p^2} + \\frac{1-p}{(1-p)^2} = \\frac{1}{p(1-p)}. \\end{align}\\] So that we recover the same variance as above. Another option is to use a Score test. The score statistic is \\[ \\frac{l_n&#39;(\\theta_0)}{\\sqrt{I_n(\\theta_0)}}. \\] Here \\(l_n(\\theta_0)\\) is the log-likelihood of the sample of size \\(n\\). In our case \\[ l_n&#39;(p) = \\sum_{i=1}^n l_i&#39;(p) = \\sum_{i=1}^n \\left( \\frac{x_i}{p} - \\frac{1-x_i}{1-p} \\right) = \\frac{n\\bar x}{p} - \\frac{n-\\bar x}{1-p} = \\frac{n(\\bar x -p)}{p(1-p)} = \\frac{n(\\hat p -p)}{p(1-p)}. \\] Also, \\(I_n(\\theta) = nI(\\theta)\\) is the Fisher information of the sample of size \\(n\\). For us \\[ I_n(p) = \\frac{n}{p(1-p)}. \\] The score statistic is then \\[ \\frac{l_n&#39;(p_0)}{\\sqrt{I_n(p_0)}} = \\frac{n(\\hat p -p_0)}{p_0(1-p_0)}\\sqrt{\\frac{p_0(1-p_0)}{n}} = \\frac{\\hat p -p_0}{\\sqrt{\\frac{p_0(1-p_0)}{n}}}, \\] which is asymptotically distributed as \\(\\mathsf N(0,1)\\) if \\(p=p_0\\). This is similar to the Wald statistic. The difference is that \\(\\hat p\\) in the denominator is replaced by \\(p_0\\). The final test is the asymptotic likelihood ratio test. It states that, if \\(p=p_0\\), \\[ 2(l_n(\\hat p)-l_n(p_0)) \\overset{asym.}{\\sim} \\chi^2_1. \\] This is perhaps the easiest test to perform, since it is only a matter of evaluating the log-likelihood and comparing the the appropriate quantile of \\(\\chi^2_1\\). "],["1.14-review-questions.html", "1.14 Review questions", " 1.14 Review questions What is the likelihood function? How is the maximum likelihood estimate calculated? How can one use numerical optimisation to calculate the maximum likelihood estimate? What is the likelihood ratio? What is the likelihood ratio test? How is the Taylor expansion calculated? What is the Fisher information? What is the asymptotic distribution of the MLE? What is the delta method? What is the difference between the first and second order delta methods? What is Wilk’s test? What is Wald’s test? What is the score test? How does one construct a confidence interval based on the Wald test? "],["2-bayesian-statistics.html", "Chapter 2 Bayesian statistics", " Chapter 2 Bayesian statistics In this chapter we introduce a different way of thinking about statistical inference, Bayesian statistics. The type of statistics that we have seen so far in this course, and also in previous courses, is called frequentist statistics. The idea is to analyse the properties of estimators, confidence intervals etc., thinking of the sample as random. For example, we say that an estimator is unbiased if the estimator applied to many different samples give the correct value of the parameter, on average. A possible critique of this is that we typically only have one sample and it is therefore not relevant to rely on the average behavior of an estimator. Bayesian statistics considers an alternative viewpoint. Here the sample is considered fixed and instead the unknown parameter is considered random. The distribution of the unknown parameter before making any observations is called the prior distribution. After making the observations, that distribution is updated, using Bayes’ formula, and we call the updated distribution the posterior distribution. The posterior distribution can theb be used to construct point estimates of the parameter, intervals etc. The aim of the chapter is to explain the basics, learn how to do Bayesian inference by hand in simple models and see how one can handle more complicated models with simulation methods. Readings for this chapter is: AOS 11.1, 2, 4, 6, 7, 9. AOS 12.1-3. AOS 24.1, 2, 4, 5 (not Gibbs). "],["2.1-some-basic-decision-theory.html", "2.1 Some basic decision theory", " 2.1 Some basic decision theory In this section we introduce statistical decision theory, a different way of thinking about what the task of the statistician is, compared to what we have seen so far. Consider a statistical model with a parameter \\(\\theta\\in\\Theta\\). Based on data \\(x\\), we are going to make a decision \\(\\delta(x)\\). For example, the decision might be a point estimate \\(\\delta(x)=\\bar x\\) or some interval estimator. To evaluate if our decision is good we have a loss function \\(l(\\theta,\\delta(x))\\). It is the penalty of making decision \\(\\delta(x)\\) when the true parameter is \\(\\theta\\). If \\(\\theta\\) is a real-valued parameter we might take squared loss \\[ l(\\theta,\\delta(x)) = (\\theta-\\delta(x))^2, \\] and in the discrete case, a 0-1 loss \\[ l(\\theta,\\delta(x)) =\\begin{cases} 0\\text{ if } \\theta=\\delta(x)\\\\ 1\\text{ if } \\theta\\neq \\delta(x). \\end{cases} \\] We would like to choose \\(\\delta\\) such that the loss is as small as possible. However, the loss depends on both the data \\(x\\) and the parameter \\(\\theta\\) and thus a comparison between different decisions becomes complicated. Instead we calculate the risk associated with a particular decision. The risk is defined as \\[ R(\\theta,\\delta) = E_\\theta\\left[ l(\\theta,\\delta(X)) \\right]. \\] Here the expectation is taken of a random sample \\(X\\), while \\(\\theta\\) is held fixed. Still the comparison of different \\(\\delta\\) is not trivial. It may be that for \\(\\delta_1\\) the risk is lower than the risk of \\(\\delta_2\\) for some values of \\(\\theta\\), while for some other \\(\\theta\\), the risk is higher. As a trivial example, say that we have the squared loss and \\(\\delta(x) \\equiv 0\\). Then, if the the true \\(\\theta\\) is 0, clearly this is a good decision. But if the true \\(\\theta\\) is something else, it is a bad decision. Note that the distinguishing feature of the above risk function is that the expectation is taken over a random sample \\(X\\). This is known as frequentist statistics. This makes sense in some situations, for example if you have some software that should work well in most situations, i.e. for most data \\(x\\). However in many situations the statistician is given one data set and the task is to make valid inference for that particular data set. In that situation, one might argue that, it does not make sense to consider any other data set. The Bayesian way is to instead consider \\(\\theta\\) as random, with a distribution \\(p(\\theta)\\). Even if we consider \\(\\theta\\) to have a fixed unknown value it could make sense to consider it having a distribution. For example, if we let \\(\\theta\\) be 1 if it rained in Uppsala exactly 10 years ago, and 0 otherwise, clearly this is not random. Either it rained or it did not. However, for us, since we do not know which is true, it seems natural to assign probabilities to each alternative. The Bayesian statistician would call \\(p(\\theta)\\) the prior distribution on \\(\\theta\\). Then, after observing the data \\(x\\), calculate the posterior distribution, using Bayes’ formula, \\[ p(\\theta \\mid x) = \\frac{p(x\\mid \\theta)p(\\theta)}{p(x)}. \\] Here \\(p(x\\mid \\theta)\\) is the model of how data is generated. That is, the probability or density of obtaining a particular sample, conditioned on knowing \\(\\theta\\). Also, \\(p(x)\\) is the marginal density of the observed data. We can write it formally as \\[ p(x) = \\int p(x,\\theta)d\\theta = \\int p(x\\mid \\theta)p(\\theta)d\\theta. \\] The posterior risk is then defined as \\[ \\rho(p(\\theta),\\delta(x)) = E\\left[ l(\\theta,\\delta(x)) \\right] := \\int l(\\theta,\\delta(x))p(\\theta\\mid x)d\\theta. \\] Here the random quantity is \\(\\theta\\) while the data \\(x\\) is held constant. The Bayes action \\(\\delta^\\star\\) is the decision \\(\\delta\\) that minimizes the posterior risk. For example, if the decision is a point estimate of \\(\\theta\\) and \\(l(\\theta,\\delta) = (\\theta - \\delta)^2\\). Then \\[\\begin{align} \\rho(p(\\theta),\\delta) &amp;= \\int (\\theta - \\delta)^2 p(\\theta\\mid x)d\\theta\\\\ &amp;= \\delta^2 - 2\\delta \\int \\theta p(\\theta\\mid x)d\\theta + \\int \\theta^2p(\\theta\\mid x)d\\theta,\\\\ \\implies \\partial_\\delta \\rho(p(\\theta),\\delta) &amp;= 2\\delta -2\\int \\theta p(\\theta\\mid x)d\\theta = 0\\\\ \\implies \\delta^\\star &amp;=\\int \\theta p(\\theta\\mid x)d\\theta. \\end{align}\\] That is, the Bayes action is to choose the mean of the posterior distribution. A hybrid between the frequentist and the Bayesian approach is the average risk (with respect to \\(p(\\theta)\\)): \\[ r(p(\\theta),\\delta) = \\int R(\\theta,\\delta)p(\\theta)d\\theta. \\] The Bayes risk is the minimum of the average risk, taken over all \\(\\delta\\) and the decision \\(\\delta(x)\\) that achieves the minimum is called the Bayes rule. Since the risk here is the frequentist risk, this is fundamentally a frequentist quantity. However, we can write \\[\\begin{align} r(p(\\theta),\\delta) &amp;= \\int \\int l(\\theta,\\delta(x))p(x\\mid\\theta)dxp(\\theta)d\\theta\\\\ &amp; = \\int\\int l(\\theta,\\delta(x)) p(\\theta\\mid x)d\\theta p(x)dx\\\\ &amp; = \\int \\rho(p(\\theta),\\delta(x)) p(x)dx. \\end{align}\\] This is minimized by minimizing \\(\\rho(p(\\theta),\\delta(x))\\) for each \\(x\\). That is, we can find the Bayes rule by taking the Bayes action for each \\(x\\). "],["2.2-bayesian-statistics-1.html", "2.2 Bayesian statistics", " 2.2 Bayesian statistics In the previous section we saw that the difference between the frequentist methods and the Bayesian methods is that the frequentist averages over different samples while the Bayesian averages over different parameter values. The Bayesian method is therefore to assign a prior distribution \\(p(\\theta)\\) to the unknown parameter. This distribution reflects our belief about the parameter before we see the data. We model how data is generated by \\(p(x\\mid \\theta)\\). That is, the probability or density of obtaining a particular sample, conditioned on knowing \\(\\theta\\). We then calculate the posterior distribution of the parameter, given the observations \\[ p(\\theta \\mid x) = \\frac{p(x\\mid \\theta)p(\\theta)}{p(x)}. \\] Here \\(p(x)\\) is the marginal density of the observed data. We can write it formally as \\[ p(x) = \\int p(x,\\theta)d\\theta = \\int p(x\\mid \\theta)p(\\theta)d\\theta. \\] However, note that since \\(x\\) is fixed and we are interested in the distribution of \\(\\theta\\), \\(p(x)\\) can be regarded as a constant. Also, \\(p(x\\mid \\theta)\\) is the likelihood, \\(L(\\theta)\\). We may therefore write \\[ p(\\theta\\mid x) = c L(\\theta)p(\\theta) \\propto L(\\theta)p(\\theta). \\] That is, the posterior is proportional to the likelihood times the prior. Another way to write this is by applying the logarithm. \\[ \\ln p(\\theta\\mid x) = l(\\theta) + \\ln p(\\theta) + c \\] We see that the total information regarding \\(\\theta\\), expressed as the posterior distribution, is a combination of the likelihood obtained from the observations and the knowledge before the observations, expressed in the prior distribution. Once we have the posterior, we can keep it as it is. Perhaps \\(\\theta\\) is one part of a larger model and by using the full distribution we are able to take full account of the uncertainty. But we might also summarize the posterior in the posterior mean \\[ \\bar \\theta = \\int \\theta p(\\theta\\mid x)d\\theta, \\] or construct a \\(1-\\alpha\\) posterior interval That is \\(a\\) and \\(b\\) such that \\[ P\\left( a\\leq \\theta \\leq b \\right) = \\int_a^b p(\\theta\\mid x)d\\theta = 1-\\alpha. \\] We might also want to predict a new observation \\(x^\\text{new}\\), \\[ p(x^\\text{new}\\mid x) = \\int p(x^\\text{new}\\mid \\theta) p(\\theta\\mid x)d\\theta. \\] Let us examine a simple example: We flip a coin \\(n\\) times and we want to make Bayesian inference regarding the probability of heads, \\(\\theta\\). The first step is to decide on the data generating model. It seems natural to assume that \\(X_1,\\ldots X_n \\overset{iid}\\sim \\mathsf{Bernoulli}(\\theta)\\). The second step is to decide on a prior distribution of \\(\\theta\\). Since \\(\\theta\\) represents a probability, the prior distribution should be confined to \\([0,1]\\). A popular choice of prior distribution on probabilities is the beta distribution. It has density, \\[ p(\\theta) = \\frac{\\theta^{\\alpha-1}(1-\\theta)^{\\beta-1}}{B(\\alpha,\\beta)},\\quad 0\\leq \\theta\\leq 1, \\] where \\(\\alpha&gt;0\\) and \\(\\beta&gt;0\\) are parameters. Also, \\(B(\\alpha,\\beta)\\) is the beta function, defined simply such that the density integrates to one. We plot the density for a few choices of \\(\\alpha\\) and \\(\\beta\\). Figure 2.1: Density of the beta distribution We see from the figure that by changing \\(\\alpha\\) and \\(\\beta\\) the beta prior can reflect different kinds of prior information. For example, \\(\\alpha = \\beta = 1\\) is a uniform distribution, that is a prior that gives equal probability to any value of \\(\\theta\\). On the other hand, \\(\\alpha =3\\), \\(\\beta = 2\\) gives small probability to \\(\\theta\\) close to 0 and 1 and puts more probability to \\(\\theta&gt;0.5\\) than \\(\\theta &lt; 0.5\\). Now let us calculate the posterior distribution of \\(\\theta\\) after observing \\(x=(x_1,\\ldots, x_n)\\). First we need the likelihood of the observation \\[ L(\\theta) = \\prod_{i=1}^n\\theta^{x_i}(1-\\theta)^{1-x_i} = \\theta^{n\\bar x}(1-\\theta)^{n-n\\bar x}, \\] where \\(n \\bar x = \\sum_i x_i\\). Then the posterior is \\[ p(\\theta\\mid x) \\propto L(\\theta)p(\\theta) \\propto \\theta^{n\\bar x}(1-\\theta)^{n-n\\bar x} \\theta^{\\alpha-1}(1-\\theta)^{\\beta-1} = \\theta^{\\alpha + n\\bar x -1}(1-\\theta)^{\\beta + n - n\\bar x -1}, \\] we recognize this as a \\(\\mathsf{Beta}(\\alpha+n\\bar x, \\beta+n-n\\bar x)\\) distribution. In this situation, when the prior and posterior happen to be in the same family of distributions, the prior is said to be conjugate with respect to the model. From here we easily get that the mean of the posterior is \\[ E\\left[ \\theta \\mid x \\right] = \\frac{\\alpha + n\\bar x}{\\alpha+\\beta + n}. \\] It can be interesting to note here that as \\(n\\to \\infty\\), the above converges to \\(\\bar x\\), i.e. for a large sample, the influence of the prior becomes small and the mean of the posterior is simply the MLE. If we wish to predict a new sample, we would calculate \\[ p(x^\\text{new} = 1 \\mid x^n) = \\int p(x^\\text{new} = 1\\mid \\theta) p(\\theta\\mid x^n)d\\theta = \\int \\theta p(\\theta \\mid x^n)d\\theta = E\\left[ \\theta \\mid x^n \\right]. \\] To make things more concrete, let us say that we observe \\(\\bar x = 0.3\\). Note how the likelihood, and therefore also the posterior, is a function of data only through \\(\\bar x\\). This is because \\(\\bar x\\) is a sufficient statistic. As an illustration, let us also choose \\(\\mathsf{Beta}(5.0, 2.0)\\) as the prior. Below we plot the prior, likelihood and posterior for \\(n=10\\) and \\(n=100\\). Note how the posterior becomes more like the likelihood as the sample size increase. Figure 2.2: Prior, likelihood and posterior when n = 10 (top) and n=100 (bottom) "],["2.3-choosing-prior.html", "2.3 Choosing prior", " 2.3 Choosing prior It is clear that the choice of prior will influence the inference and so the choice of prior distribution becomes important. If we have a subjective belief about what values the parameter is likely to take we can choose a prior that reflects this belief. For example we might know from experience that coins have a probability close to 0.5 of landing on heads and so we would choose a prior with most of the probability around 0.5. However if the goal is to make scientific inference, for example convince the government that a particular drug is safe, choosing a subjective prior could face criticism. In such cases an alternative is to choose a non-informative prior. One way is to use a flat prior, \\(p(\\theta)\\propto \\text{constant}\\). For example in the Bernoulli example choosing \\(p(\\theta)=1\\) for \\(0\\leq \\theta \\leq 1\\) seems reasonable. Consider the model \\(X\\sim \\mathsf N(\\theta,\\sigma^2)\\), \\(\\sigma^2\\) known. Here the flat prior would be \\(p(\\theta)=c&gt;0\\). Note however that \\(p\\) can never be a proper density since \\[ \\int_{-\\infty}^\\infty p(\\theta)d\\theta = \\infty, \\] for every choice of \\(c\\). Then \\(p\\) is said to be an improper prior. Although a little curious, we can still perform the Bayesian inference program. The posterior distribution is \\[ p(\\theta \\mid x) \\propto L(\\theta)p(\\theta) \\propto L(\\theta). \\] So that with this choice the posterior distribution is just the likelihood. Continuing the example, let us say that we have a sample of size \\(n\\) with sample mean \\(\\bar x\\). Then the likelihood is \\[ L(\\theta) \\propto \\prod_{i=1}^n e^{-\\frac{1}{2\\sigma^2}(x_i-\\theta)^2} = e^{-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n(x_i-\\theta)^2} \\propto e^{-\\frac{n}{2\\sigma^2}(\\theta-\\bar x)^2}, \\] which is the density of \\(\\mathsf N(\\bar x, \\sigma^2/n)\\). Therefore the point estimates and interval estimators will be the same as in likelihood based inference. A problem with flat priors is however that they are only non-informative in a particular parameterisation. Take again the Bernoulli example. If we say that we do not have any information about \\(\\theta\\), then it would be reasonable to say that we also do not have any information about some function of \\(\\theta\\). Take for example \\(\\Psi = \\ln (\\theta/(1-\\theta))\\). If \\(p(\\theta)=1\\) for \\(0\\leq\\theta\\leq 1\\), then the density of \\(\\Psi\\) is \\[\\begin{align} p(\\psi) &amp; = \\partial_\\psi P(\\Psi \\leq \\psi) = \\partial_\\psi P\\left( \\ln (\\theta/(1-\\theta)) \\leq \\psi \\right)\\\\ &amp;= \\partial_\\psi P\\left( \\theta \\leq \\frac{e^\\psi}{1+e^\\psi} \\right) = \\partial_\\psi \\frac{e^\\psi}{1+e^\\psi} = \\frac{e^\\psi}{(1+e^\\psi)^2}, \\end{align}\\] which is clearly no longer non-informative. We say that flat priors are not transformation invariant. One way to construct a prior that does not change under a change of parameterisation is Jeffreys’ prior: \\[ p(\\theta)\\propto \\sqrt{I(\\theta)}, \\] where \\(I\\) is the Fisher information. Recall that in the \\(\\mathsf{Bernoulli}(\\theta)\\) case \\(I(\\theta)=(\\theta(1-\\theta))^{-1}\\) and so the Jeffreys prior is \\[ p(\\theta) \\propto \\theta^{-1/2}(1-\\theta)^{-1/2}, \\] which is a \\(\\mathsf{Beta}(1/2,1/2)\\). "],["2.4-multiparameter-problems.html", "2.4 Multiparameter problems", " 2.4 Multiparameter problems If there is more than one parameter, the previous discussion applies without any major changes. What might complicate things is if we wish to do inference on just one of the parameters. Then we need to calculate the marginal posterior distribution of the parameter, which may not be practical. One remedy is to simply draw samples from the joint posterior distribution and from the samples pick the parameters that are of interest. Let us consider the example of comparing two binomials. The data generating model is \\[\\begin{align} X_1 &amp;\\sim \\mathsf{Bin}(n_1,\\theta_1),\\\\ X_2 &amp;\\sim \\mathsf{Bin}(n_2,\\theta_2). \\end{align}\\] The parameter that we are interested in is \\(\\tau:=\\theta_2-\\theta_1\\). If we assume a flat prior on \\(\\theta_1\\) and \\(\\theta_2\\) the posterior becomes, \\[ p(\\theta_1,\\theta_2\\mid x) \\propto L(\\theta_1,\\theta_2) \\propto \\theta_1^{x_1}(1-\\theta_1)^{n_1-x_1}\\theta_2^{x_2}(1-\\theta_2)^{n_2-x_2}. \\] Note that \\(\\theta_1\\mid x_1\\sim \\mathsf{Beta}(x_1+1,n_1-x_1+1)\\) is independent of \\(\\theta_2\\mid x_2\\sim \\mathsf{Beta}(x_2+1,n_2-x_2+1).\\) Therefore we can simulate from \\(\\tau\\mid x\\) by drawing \\(\\theta_1^\\star\\) and \\(\\theta_2^\\star\\) from the respective distribution and setting \\(\\tau^\\star = \\theta_2^\\star - \\theta_1^\\star\\). library(ggplot2) library(latex2exp) n1 &lt;- 10 n2 &lt;- 10 theta1 &lt;- 0.5 theta2 &lt;- 0.3 nPost &lt;- 2000 set.seed(42) #Generate data x1 &lt;- rbinom(n = 1, size = n1, prob = theta1) x2 &lt;- rbinom(n = 1, size = n2, prob = theta2) #Posterior for theta theta1.post &lt;- rbeta(n = nPost, shape1 = x1+1, shape2 = n1-x1+1) theta2.post &lt;- rbeta(n = nPost, shape1 = x2+1, shape2 = n2-x2+1) tau.post &lt;- theta2.post - theta1.post ggplot(data.frame(tau.post), aes(x=tau.post)) + geom_histogram(aes(y=..density..), colour=&quot;black&quot;, fill=&quot;white&quot;)+ geom_density(alpha=.2, fill=&quot;black&quot;) + theme_minimal() + xlab(TeX(&quot;$\\\\tau$&quot;)) Figure 2.3: Posterior distribution "],["2.5-markov-chain-monte-carlo.html", "2.5 Markov chain Monte Carlo", " 2.5 Markov chain Monte Carlo So far we have been using that the posterior distribution is proportional to the likelihood times the prior distribution. We were also careful to choose the prior as the conjugate distribution so that we could recognize the posterior distribution. In general we have to calculate the normalizing constant: \\[ \\int p(x\\mid \\theta)p(\\theta)d\\theta. \\] Many times, and in particular if \\(\\theta\\) is high dimensional, it is not possible to calculate this integral. In this section we will see how we can, instead of calculating it, generate samples from the posterior distribution. This is the method that is used in modern Bayesian statistics. Here we are only able to scratch the surface. Both in terms of theory, since explaining why the method works would require first learning about Markov chains, and in terms of complexity of the methods, since the methods that are used in practice are usually more advanced version of what we present here. We assume that we are able to write down the likelihood of our data generating model and the prior on our parameters. Therefore we are also able to write down the posterior distribution up to a multiplying constant. The method we will discuss is called the Metropolis-Hastings algorithm and it is an example of a Markov chain Monte Carlo (MCMC) method. Markov chain here means that we will obtain a sequence of samples from the posterior distributions and that the distribution of each sample only depends on the previous sample, and not on samples before that. Monte Carlo means that is is an algorithm that uses on random sampling. The Metropolis-Hastings algorithm is as follows: Suppose we want to sample from the posterior \\(p(\\theta\\mid x).\\) Suppose also that this is not possible directly, but that we are able to generate samples from some other distribution \\(q(\\theta^\\star \\mid \\theta).\\) Then do the following: Choose \\(\\theta_0\\) arbitrarily. For \\(i\\geq 1\\) do: Generate a proposal \\(\\theta^\\star\\sim q(\\theta^\\star\\mid \\theta_{i-1})\\) Calculate \\[ r = \\min\\left\\{ \\frac{p(\\theta^\\star\\mid x)}{p(\\theta_{i-1}\\mid x)}\\frac{q(\\theta_{i-1}\\mid \\theta^\\star)}{q(\\theta^\\star\\mid\\theta_{i-1})}, 1 \\right\\}. \\] Set \\[ \\theta_i = \\begin{cases} \\theta^\\star\\quad \\text{with probability } r\\\\ \\theta_{i-1}\\quad \\text{with probability } 1-r. \\end{cases} \\] Theory then tells us that as \\(i\\) becomes large, the distribution of \\(\\theta_i\\) will be approximately that of \\(p(\\theta\\mid x).\\) Note that here the normalizing constant in \\(p(\\theta\\mid x)\\) is cancelled since it appears both in the numerator and denominator. It remains to choose \\(q(\\theta^\\star\\mid x)\\). One common choice is called random walk Metropolis Hastings and consists of letting \\[ \\theta^\\star = \\theta_{i-1} + \\varepsilon_{i-1}, \\] where \\(\\varepsilon_{i-1}\\) is and independent random variable. For example \\(\\varepsilon_{i-1}\\overset{iid}{\\sim} \\mathsf N(0,b^2)\\) for some variance \\(b^2\\). In this case \\(q(\\theta_{i-1}\\mid \\theta^\\star)=q(\\theta^\\star\\mid \\theta_{i-1})\\) and so the acceptance probability becomes \\[ r=\\min\\left\\{ \\frac{p(\\theta^\\star\\mid x)}{p(\\theta_{i-1}\\mid x)} ,1\\right\\} \\] Here it is important to choose \\(b\\) such that the correlation between \\(\\theta_i\\) and \\(\\theta_{i-1}\\) is small. If \\(b\\) is small, almost all proposals will be accepted, but since \\(\\varepsilon_i\\) also tends to be small, \\(\\theta_{i+1}\\) will be highly correlated with \\(\\theta_{i}\\). On the other hand, if \\(b\\) is large, the proposal will rarely be accepted, and \\(\\theta_i=\\theta_{i-1}\\). As a rule of thumb, \\(b\\) should be set such that about \\(50\\%\\) of the proposals are accepted. Another class of proposal distributions is \\(q(\\theta^\\star\\mid \\theta_{i-1})\\equiv q(\\theta^\\star)\\), that is \\(\\theta^\\star\\) is independent of \\(\\theta_{i-1}\\). The acceptance probability is then \\[ r = \\min\\left\\{ \\frac{p(\\theta^\\star\\mid x)}{p(\\theta_{i-1}\\mid x)}\\frac{q(\\theta_{i-1})}{q(\\theta^\\star)}, 1 \\right\\}. \\] This is known as independence Metropolis Hastings. Here, since each time the proposal is accepted \\(\\theta_i\\) will change, we aim for as high an acceptance probability as possible. As an example let us see how one can generate samples from the exponential distribution using Metropolis Hastings. To be clear, this is not the best way to generate exponentially distributed random numbers, it is just an example. The density is \\[ p(x) = \\lambda e^{-\\lambda x}\\propto e^{-\\lambda x}. \\] Here we know that the normalizing constant is \\(\\lambda\\), but the idea of MCMC is that we do not need to know it, so let us proceed as if we did not know it. Also note that here we see \\(x\\) as random, in the typical Bayesian setting we would think of \\(\\lambda\\) as random, but is is just a matter of notation. First we implement the density. We do it on a log-scale. This is because we need to divide two densities in the algorithm. On a log-scale this becomes subtraction, which is numerically more stable. logDensity &lt;- function(x) { lambda &lt;- 1 if (x &gt;= 0) { -lambda * x } else{ -Inf } } Next implement the random-walk Metropolis-Hastings algorithm with normally distributed steps. mcmc.iter &lt;- function(x, logDensity, sigma, n.iter){ #Random walk Metropolis Hastings MCMC res &lt;- matrix(NA, n.iter+1, length(x)) #Create empty matrix res[1,] &lt;- x logD &lt;- logDensity(x) accProb &lt;- 0 #keep track of the proportion of proposals that are accepted for (i in seq_len(n.iter)){ #New proposal xProp &lt;- x + rnorm(length(x), 0, sigma) #Log density of proposal logDProp &lt;- logDensity(xProp) #Acceptance probability r &lt;- min( c(1, exp(logDProp - logD) ) ) if(r&gt;runif(1)){ #Accept with probability r, else keep old x x &lt;- xProp logD &lt;- logDProp accProb &lt;- accProb + 1 } res[i+1,] &lt;- x } list(sample = res, accProb = accProb/n.iter) } Now we can run the algorithm. First for 1000 step that we throw away. This is since the distribution of the samples are correct only when the number of steps are large. Then we run it for as many steps as we need samples. x.init &lt;- 1 #Initial value nIter &lt;- 100000 #Number of MC steps set.seed(42) exp.mcmc &lt;- mcmc.iter(x.init, logDensity, 0.2, 1000) exp.mcmc &lt;- mcmc.iter(exp.mcmc$sample[nrow(exp.mcmc$sample),], logDensity, 0.2, nIter) We can check for example that the mean and standard deviation of the samples are as expected, i.e. in our case 1. mean(exp.mcmc$sample) ## [1] 1.072045 sd(exp.mcmc$sample) ## [1] 1.114397 We can plot the histogram of the samples against the density. "],["2.6-an-application-1.html", "2.6 An application", " 2.6 An application Here we consider the Bayesian probit model as an example. The data generating model is \\[\\begin{align} Y_i&amp;\\sim \\mathsf{Bin}(n_i,\\pi_i),\\\\ \\pi_i &amp;= \\Phi(\\beta_0 + \\beta_1 z_{i1} + \\beta_2 z_{i2} + \\beta_3 z_{i3}), \\end{align}\\] where \\(z_{ij}\\) are indicators and \\(\\Phi\\) is the \\(\\mathsf N(0,1)\\) distribution function. The data is shown in the table. Table 2.1: Data for the Bayesian probit model y n z1 z2 z3 11 84 1 1 1 5 61 0 1 1 0 29 0 0 1 21 36 1 1 0 35 69 0 1 0 2 4 1 0 0 4 42 0 0 0 The likelihood is \\[ L(\\beta)\\propto \\prod_{i=1}^n\\Phi(\\beta_0 + \\beta_1 z_{i1} + \\beta_2 z_{i2} + \\beta_3 z_{i3})^{y_i}(1-\\Phi( \\beta_0 + \\beta_1 z_{i1} + \\beta_2 z_{i2} + \\beta_3 z_{i3}))^{n_i-y_i}. \\] We choose a normal distribution as prior for each \\(\\beta_i\\), \\[ \\beta_i \\overset{iid}\\sim \\mathsf N(0,\\lambda), \\] with \\(\\lambda=10\\). That is, the prior can be written as \\[ p(\\beta) \\propto \\exp\\left( -\\frac{1}{2\\lambda}\\sum_{j=0}^3 \\beta_j^2 \\right). \\] We will use a random walk Metropolis Hastings algorithm with \\(\\varepsilon\\overset{iid}\\sim \\mathsf N(0,\\sigma^2)\\). Now, we implement this in R. First we load the data. data.df &lt;- read.csv(&quot;data/bayesProbit.dat&quot;, header=TRUE) n &lt;- nrow(data.df) data.df$z0 &lt;- rep(1, n) col_order &lt;- c(&#39;y&#39;, &#39;n&#39;,&#39;z0&#39;,&#39;z1&#39;,&#39;z2&#39;,&#39;z3&#39;) data.df &lt;- data.df[, col_order] Then implement the likelihood function and prior density. We do this on a log-scale. logL &lt;- function(beta){ y &lt;- data.df$y n &lt;- data.df$n z0 &lt;-data.df[3] z1 &lt;-data.df[4] z2 &lt;-data.df[5] z3 &lt;-data.df[6] p &lt;- pnorm(as.matrix(z0 * beta[1] + z1 * beta[2] + z2 * beta[3] + z3 * beta[4])) sum &lt;- 0 for (i in seq_len(nrow(data.df))) { sum &lt;- sum + y[i]*log(p[i]) + (n[i]-y[i])*log(1-p[i]) } sum } lambda &lt;- 10 logPrior &lt;- function(beta){ -sum(beta^2)/(2*lambda) } logPosterior &lt;- function(beta){ logL(beta) + logPrior(beta)} Now run the Markov chain. First a burn-in of 1000 steps, that we then discard. After that a longer run. beta &lt;- c(0,0,0,0) #Initial value nIter &lt;- 100000 #Number of MC steps set.seed(42) beta.mcmc &lt;- mcmc.iter(beta, logPosterior, 0.08, 1000) beta.mcmc &lt;- mcmc.iter(beta.mcmc$sample[nrow(beta.mcmc$sample),], logPosterior, 0.08, nIter) Now we do some diagnostics of the simulation. First check that the acceptance probability is reasonable. beta.mcmc$accProb ## [1] 0.53389 Then we plot the trajectories of the parameters. We want to see that the trajectories appear stationary and that they are not stuck in one state. We only plot the first 1000 steps. Figure 2.4: Trajectories of the Markov chain Then we calculate the cumulative mean. We want to see that the simulation is long enough so that the law of large numbers have come in to effect. Figure 2.5: Cumulative mean of the Markov chain Then we plot the posterior distribution of the parameters. Figure 2.6: Posterior distribution From this we can get point estimates, the mean of the posterior distribution, and credible intervals. #Point estimates apply(beta.mcmc$sample, 2, mean) ## [1] -1.1829531 0.3175842 1.1576232 -1.4132245 #95% CI apply(beta.mcmc$sample, 2, quantile, probs = c(0.05,0.95)) ## [,1] [,2] [,3] [,4] ## 5% -1.5791352 0.01517242 0.7428645 -1.736278 ## 95% -0.8206941 0.63006163 1.6063078 -1.101808 As a final check, let us verify that our estimates makes sense by comparing our data to our predictions. beta.fit &lt;- apply(beta.mcmc$sample, 2, mean) p &lt;- pnorm(as.matrix(data.df[3:6])%*%beta.fit) y.pred &lt;- data.df$n*p y.pred ## [,1] ## [1,] 11.0166258 ## [2,] 4.5834439 ## [3,] 0.1366877 ## [4,] 22.1383420 ## [5,] 33.8028200 ## [6,] 0.7736728 ## [7,] 4.9733827 This is similar to the data. "],["2.7-summary-1.html", "2.7 Summary", " 2.7 Summary In this section we summarize the chapter by doing a simple example that can be solved by direct calculation. In the 2020 season of the Swedish football league Allsvenskan there was 488 goals scored in 240 games. Assume that \\(X_1, \\ldots X_{488}\\), the number of goals scored in each game, are i.i.d and distributed as \\(\\mathsf{Po}(\\lambda)\\). We wish to do Bayesian inference on the unknown parameter \\(\\lambda\\). First we find the likelihood. Since the probability function of a Poisson distributed random variable is \\[ p(x) = \\frac{\\lambda^x e^{-\\lambda}}{x!},~x = 0,1,2\\ldots, \\] if we denote the number of goals in game \\(i\\) by \\(x_i\\), the total likelihood is \\[ L(\\lambda) = \\prod_{i=1}^{n} \\frac{\\lambda^{x_i} e^{-\\lambda}}{x_i!} \\propto \\lambda^{\\sum x_i}e^{-n\\lambda}. \\] Then we decide on an appropriate prior distribution. Since \\(\\lambda &gt;0\\) we should choose a distribution with all probability on the positive real numbers. A convenient choice, it turns out, is the gamma distribution. Let us choose the parameters of the gamma distribution based on our subjective (prior) belief. As prior belief we will say that the mean of \\(\\lambda\\) should be 3, and that it is unlikely that the average number of goals is larger than 5. Since \\(\\lambda \\sim \\mathsf{Gamma}(\\alpha,\\beta)\\) implies that \\[ E\\left[ \\lambda \\right] = \\frac{\\alpha}{\\beta}, \\] we will choose \\(\\beta = \\alpha/3\\). It remains to choose \\(\\alpha\\). To do this we plot the density for different choices of \\(\\alpha\\). Figure 2.7: Density of the gamma distribution Based on this picture we decide that \\(\\alpha = 20\\) fits with our prior beliefs. Now we shall calculate the posterior distribution. The prior distribution has the density (from a textbook) \\[ p(\\lambda) = \\frac{\\beta^\\alpha}{\\Gamma(\\alpha)} \\lambda^{\\alpha-1} e^{-\\beta\\lambda}. \\] Therefore the posterior distribution has density \\[ p(\\lambda \\mid x) \\propto L(\\lambda)p(\\lambda) \\propto \\lambda^{\\sum x_i}e^{-n\\lambda} \\lambda^{\\alpha-1} e^{-\\beta\\lambda} = \\lambda^{\\alpha + \\sum x_i - 1}e^{-\\lambda(\\beta + n)}. \\] That is, the posterior distribution is \\(\\mathsf{Gamma}(\\alpha + \\sum x_i, \\beta + n)\\). With our prior we therefore get the posterior \\(\\mathsf{Gamma}(518, 246.7)\\), illustrated in the figure. Figure 2.8: Density of the posterior distribution when prior is subjective The mean of the posterior, and our point estimate of \\(\\lambda\\) is \\(518/246.7 \\approx 2.1\\) and a 95% credibility interval is given by lower &lt;- qgamma(0.025, shape = 518, 20/3 + 240) upper &lt;- qgamma(0.975, shape = 518, 20/3 + 240) c( lower, upper ) ## [1] 1.923027 2.284652 We could instead use a flat prior \\(p(\\lambda) = c, \\quad \\lambda&gt;0\\). Note that this corresponds to a \\(\\mathsf{Gamma}(1,0)\\) distribution. So we may recycle the above calculations which gives the posterior \\(\\mathsf{Gamma}(489,240)\\). Figure 2.9: Density of the posterior distribution when prior is flat Now our point estimate of \\(\\lambda\\) is \\(489/240 \\approx 2\\) and a 95% credibility interval is given by lower &lt;- qgamma(0.025, shape = 489, 240) upper &lt;- qgamma(0.975, shape = 489, 240) c( lower, upper ) ## [1] 1.860889 2.222002 We could also use Jeffreys prior which is proportional to the square root of the Fisher information. The Fisher information can be calculated, for example, as \\[ I(\\lambda) = Var(l&#39;(\\lambda)). \\] The log-likelihod and its derivative is \\[\\begin{align} l(\\lambda) &amp;= \\sum_{i=1}^n\\left( -\\ln x_i! + x_i\\ln \\lambda - \\lambda \\right)\\\\ l&#39;(\\lambda) &amp;= \\sum_{i=1}^n\\left( \\frac{x_i}{\\lambda} -1 \\right) = n\\left( \\frac{\\bar x}{\\lambda} -1 \\right). \\end{align}\\] Which gives \\[ I(\\lambda) = Var\\left(\\frac{n\\bar X}{\\lambda} \\right) = \\frac{n^2}{\\lambda} Var\\left(\\bar X\\right) = \\frac{n^2}{\\lambda}\\frac{\\lambda}{n} = \\frac{n}{\\lambda}. \\] Then, finally, Jeffreys prior is \\(p(\\lambda) \\propto \\lambda^{-1/2}\\), corresponding to \\(\\mathsf{Gamma}(1/2,0)\\) and then the procedure follows the same steps as above. "],["2.8-review-questions-1.html", "2.8 Review questions", " 2.8 Review questions What is the main difference between frequentist and Bayesian inference? What is the risk associated with a decision? What is the posterior risk? What is the Bayes action? What is the Bayes rule? What is the relationship between the prior, the likelihood and the posterior? What is a point estimate in Bayesian statistics? What is a posterior interval? When is the Beta distribution a good choice as a prior distribution? What is a subjective prior? What is a flat prior? What is a possible critisism of flat priors? What is Jeffreys prior? What is the computational problem that Markov chain Monte Carlo solves? What is the difference between random walk Metropolis Hastings and independence Metropolis Hastings? "],["3-ch-bootstrap.html", "Chapter 3 Bootstrap", " Chapter 3 Bootstrap This chapter’s main topic is a method for measuring the accuracy of sample estimates. Recall that in frequentist statistics we consider the variability of an estimate as we imagine that we draw new samples from the same distribution. In reality we only have one sample and so the solution has been to assume a distribution of the observations and theoretically derive the sampling distribution of the estimator. For example this is what we did using asymptotic approximations in the first chapter. In this chapter we introduce the Bootstrap as an alternative way of finding the accuracy of an estimator. Instead of assuming a distribution of the observations, we will let the sample represent the whole sampling distribution. In that way we can get repeated approximate samples from the sampling distribution by resampling with replacement from the sample. This is a powerful method that lets us estimate standard deviations and construct confidence intervals even for complicated estimators, with very little effort. But first we will discuss the distinction between parametric and non-parametric statistics. Readings for this chapter is: AOS 7, 8 ISLR 5.2 "],["3.1-parametric-vs-non-parametric.html", "3.1 Parametric vs non-parametric", " 3.1 Parametric vs non-parametric What we have seen so far has been parametric models. For example we could have a normal model, \\(X\\sim \\mathsf N(\\mu,\\sigma^2)\\), and the goal would be to make some statement regarding the parameters \\(\\mu\\) and \\(\\sigma^2\\). For a non-parametric model we only say that our sample is an observation from some distribution, with distribution function \\(F\\) and our goal is to make statements about some property of that distribution. For example we might want to estimate the mean of that distribution. We call these properties functionals of the distribution. Since if we knew the distribution, they could be calculated. For example the mean, \\[ \\mu = T(F) = \\int x dF(x). \\] Here, \\(T(F)\\) is just expressing the fact that \\(\\mu\\) is calculated from the distribution function \\(F\\). The integral is a convenient way of writing integration with respect to a distribution in a general way. For example, if the distribution is continuous, recall that the density is \\[ f(x) = \\frac{dF(x)}{dx}. \\] Written in another way: \\[ dF(x) = f(x)dx. \\] Then the formula above just becomes \\[ \\mu = \\int xf(x)dx, \\] which we recognize as the mean of a distribution. If \\(F\\) is the distribution function of a discrete distribution, \\(dF(x)\\) can be thought of as the probability function, and the mean is \\[ \\mu = \\int xdF(x) = \\sum_x xp(x). \\] Another example is the variance, \\[ T(F) = \\int x^2dF(x) - \\left( \\int xdF(x) \\right)^2. \\] Also for example the median can of course be calculated, at least in principle, if one knows the distribution and can therefore be written as a functional. Recall that the median is the number \\(m\\) that satisfies, at least if \\(F\\) is continuous, \\[ F(m) \\equiv P(X\\leq m) = 1/2. \\] So that, \\[ F^{-1}(1/2) = F^{-1}(F(m)) = m. \\] More generally, assuming that \\(F\\) is strictly increasing, the \\(p\\)th quantile is \\(T(F) = F^{-1}(p).\\) Again, this is a functional of \\(F\\). "],["3.2-non-parametric-estimation.html", "3.2 Non-parametric estimation", " 3.2 Non-parametric estimation We start by defining the empirical distribution function. The empirical distribution function \\(\\hat F_n\\) is the distribution function that puts mass \\(1/n\\) at each data point \\(x_i\\). That is, \\[ \\hat F_n(x) = \\frac{\\sum_{i=1}^n I(x_i\\leq x)}{n}, \\] where \\[ I(x_i\\leq x) = \\begin{cases} 1 &amp; \\text{if } x_i \\leq x \\\\ 0 &amp; \\text{otherwise } \\end{cases} \\] In the example below, we draw a sample of size 100 from the \\(\\mathsf{N}(0,1)\\) distribution and plot the empirical distribution function of the sample. This can be done easily using gg-plot. set.seed(42) data.df &lt;- data.frame( x = rnorm(100) ) library(ggplot2) ## Warning: package &#39;ggplot2&#39; was built under R version 4.1.1 cbp1 &lt;- c(&quot;#999999&quot;, &quot;#E69F00&quot;, &quot;#56B4E9&quot;, &quot;#009E73&quot;, &quot;#F0E442&quot;, &quot;#0072B2&quot;, &quot;#D55E00&quot;, &quot;#CC79A7&quot;) ggplot(data.df, aes(x)) + stat_function(fun = pnorm, color = cbp1[2], size = 1) + stat_ecdf(geom = &quot;step&quot;, size = 1) + labs( y = &quot;F(x)&quot;, x = &quot;x&quot;) + theme_minimal() Figure 3.1: Empirical and true distribution function Now we may estimate any parameter that is a functional of \\(F\\) by simply replacing \\(F\\) by \\(\\hat F_n\\). The plug-in estimator of \\(\\theta = T(F)\\) is defined by \\[ \\hat\\theta_n = T(\\hat F_n). \\] Let us consider functionals that can be written as \\[ T(F) = \\int r(x) dF(x), \\] for some function \\(r\\). Clearly the mean and the variance are examples of this. Then the plug-in estimator is \\[ T(\\hat F_n) = \\int r(x)d\\hat F_n(x). \\] Since the empirical distribution is a discrete distribution that puts mass \\(1/n\\) at each point \\(x_i\\), the corresponding empirical probability function is, \\[ \\hat p_n(x) = \\begin{cases} 1/n &amp; \\text{if } x = x_i \\text{ some } i \\\\ 0 &amp; otherwise \\end{cases} \\] The function can then be written: \\[ T(\\hat F_n) = \\int r(x) d\\hat F_n(x) = \\sum_{i=1}^n r(x_i) \\hat p_n(x_i) = \\frac{1}{n} \\sum_{i=1}^n r(x_i). \\] The plug-in estimates of such functionals is therefore simple to calculate. For example, \\[\\begin{align} \\hat \\mu &amp;= \\mu(\\hat F_n) =\\int x d\\hat F_n(x) = \\frac{1}{n} \\sum_{i=1}^n x_i,\\\\ \\hat \\sigma^2 &amp;= \\sigma^2(\\hat F_n) = \\int x^2 d\\hat F_n(x) - \\left(\\int xd\\hat F_n(x) \\right)^2 = \\frac{1}{n}\\sum_{i=1}^n x_i^2 + \\left( \\frac{1}{n}\\sum_{i=1}^n x_i\\right)^2 = \\frac{1}{n}\\sum_{i=1}^n (x_i -\\bar x)^2. \\end{align}\\] The plug in estimator of the \\(p\\)th quantile becomes the \\(p\\)th sample quantile: \\[ \\hat F_n^{-1}(p) = \\inf \\left\\{ x \\mid \\hat F_n(x)\\geq p \\right\\}. \\] That is, it is the smallest \\(x\\) such that \\(\\hat F_n(x)\\geq p\\). Looking at the picture it seems as in our sample, for example, \\(\\hat F_n^{-1}(0.75)\\approx 0.7\\). Calculating precisely: quantile(data.df$x, probs = c(0.75), type = 1) ## 75% ## 0.6556479 "],["3.3-bootstrap.html", "3.3 Bootstrap", " 3.3 Bootstrap In this section we discuss how to calculate standard errors and confidence intervals for a statistic \\(T(X_1,X_2,\\ldots, X_n)\\). But first let us take a step back and consider the basics of what we are doing. We are given a sample \\((x_1,\\ldots, x_n)\\). We have some function, \\(T\\), that we apply to the sample and call a statistic. This may be a simple function like the sample mean, or some more complicated function like a parameter in a regression problem. We apply the function to the data and get a number \\(t=T(x_1,\\ldots,x_n)\\). To do inference we also need to know the variability of that number if we were to repeat the same experiment. We therefore consider the random variable \\(T_n := T(X_1,\\ldots, X_n)\\), where \\(X_i\\overset{iid}{\\sim }F\\). The distribution of this random variable is called the sampling distribution. In some cases, for a particular \\(T\\) and \\(F\\), we may be able to find the sampling distribution either exactly or approximately. For example, we previously found the asymptotic sampling distribution of the maximum likelihood estimate. Once we have that, we can do hypothesis testing, construct confidence intervals or simply state the standard deviation. The procedure can be summarized as: \\[ F\\overset{sample}{\\to} x \\overset{T}{\\to} t. \\] However, in many cases the calculation of the sampling distribution is not possible and one way to instead approximate the distribution is through simulation. For now, let us assume that we know the distribution \\(F\\). Then it may be possible to draw \\(n\\) iid copies of the random variable \\(X_1^\\star,\\ldots X^\\star_n \\overset{iid}{\\sim} F\\) using a computer. Then calculate \\(t^\\star = T(X_1^\\star,\\ldots X^\\star_n)\\). Repeat this \\(B\\) times to obtain \\(t_1^\\star,\\ldots, t_B^\\star\\). If we have \\(B\\) such copies, by the large of large numbers, as \\(B\\to \\infty\\), \\[\\begin{equation} \\frac{1}{B}\\sum_{i=1}^B f(t_i^\\star)\\overset{asym.}\\sim E\\left[ f(T_n) \\right]. \\tag{3.1} \\end{equation}\\] So that in practice, we would approximate the right hand side with the left hand side. The most important example is the variance: \\[ Var\\left( T_n \\right) = E\\left[T_n^2\\right] - E\\left[T_n\\right]^2 \\approx \\frac{1}{B}\\sum_{i=1}^B (t_i^\\star)^2 - \\left( \\frac{1}{B}\\sum_{i=1}^B t_i^\\star \\right)^2 = \\frac{1}{B}\\sum_{i=1}^B \\left( t_i^\\star - \\bar{t^\\star} \\right)^2 \\] As an example, let us consider \\(X_1,\\ldots, X_{n}\\overset{iid}\\sim \\mathsf{Exp}(1)\\). We would like to know the distribution of the sample median and in particular the expected value and variance. n &lt;- 100 B &lt;- 1000 T &lt;- median tstar &lt;- array(dim = B) for (i in seq_len(B)) { x &lt;- rexp(n, rate = 1) tstar[i] &lt;- T(x) } mean(tstar) ## [1] 0.6964009 var(tstar) ## [1] 0.01063886 In practice this is not helpful since we rarely know \\(F\\). What we have is \\(\\hat F_n\\), so let us approximate \\(F\\approx \\hat F_n\\) and therefore also \\(Var_F(T_n) \\approx Var_{\\hat F_n}(T_n)\\). The right hand side here is then the variance of \\(T(X_1,\\ldots , X_n)\\), where \\(X_i\\) is iid and distributed as \\(\\hat F_n\\). Then we proceed as above: Draw \\(n\\) iid copies of the random variable \\(X^\\star_1,\\ldots X^\\star_n \\sim \\hat F_n\\) using a computer. Then calculate \\(t^\\star = T(X_1^\\star,\\ldots X^\\star_n)\\). Repeat this \\(B\\) times to obtain \\(t_1^\\star,\\ldots, t_B^\\star\\). Here, since \\(\\hat F_n\\) is the empirical distribution, drawing from \\(\\hat F_n\\) means simply to draw an observation at random from the original data set. In contrast to the above, this method can be summarized as: \\[ \\hat F_n\\overset{sample}{\\to} x^\\star \\overset{T}{\\to} t^\\star. \\] This method is called the bootstrap. Bootstrap variance estimation: Choose \\(B\\) as a large number, then for \\(b=1,\\ldots, B\\) Draw \\(X_1^\\star,\\ldots, X_n^\\star \\overset{iid}{\\sim} \\hat F_n\\). Compute \\(t_b^\\star = T(X_1^\\star,\\ldots, X_n^\\star)\\). Approximate \\(Var_F(T_n)\\) by \\[ v_{boot} = \\frac{1}{B}\\sum_{b=1}^B\\left( t^\\star_{b} - \\frac{1}{B}\\sum_{r=1}^B t^\\star_{r} \\right)^2 \\] To be clear, step 1 above, draw \\(X_1^\\star,\\ldots, X_n^\\star \\overset{iid}{\\sim} \\hat F_n\\), simply means to draw one of the observations in the sample randomly and call it \\(X_1^\\star\\). Put the observation back in the sample and independently draw another observation, call it \\(X_2^\\star\\) and so on. Let us implement this method to calculate the standard error of the median using the normally distributed data set from the previous section: T &lt;- median n &lt;- nrow(data.df) B &lt;- 1000 for (i in 1:B) { indices &lt;- sample(seq_len(n), size = n, replace = TRUE) tstar[i] &lt;- T(data.df$x[indices]) } sd(tstar) ## [1] 0.1564483 In practice we would rather use the boot library. library(boot) boot(data = data.df, statistic = function(data, index){ T(data$x[index]) }, R = 1000) ## ## ORDINARY NONPARAMETRIC BOOTSTRAP ## ## ## Call: ## boot(data = data.df, statistic = function(data, index) { ## T(data$x[index]) ## }, R = 1000) ## ## ## Bootstrap Statistics : ## original bias std. error ## t1* 0.08979677 0.03632407 0.1475442 Note that the two bootstrap functions do not give exactly the same result. This is because of the Monte Carlo error, i.e. the error in approximating the sample mean with the expected value as in Equation (3.1). Since here we know \\(F\\) we can calculate the standard deviation by simulating from \\(F\\), an alternative not available in practice. n &lt;- nrow(data.df) B &lt;- 1000 Tsim &lt;- array(dim = B) for (i in 1:B) { simData.df &lt;- data.frame( x = rnorm(n) ) Tsim[i] &lt;- T(simData.df$x) } sd(Tsim) #Standard error of median ## [1] 0.1253581 It is also possible to construct confidence intervals using bootstrap. Here we present bootstrap pivotal confidence intervals, sometimes known as basic bootstrap intervals. Let us call \\(T(F) = \\theta\\) and \\(T(\\hat F_n )=\\hat\\theta_n\\) and define the pivot \\(R_n = \\hat\\theta_n - \\theta\\). Write the distribution function of \\(R_n\\) as: \\[ H(r) := P(R_n\\leq r). \\] Then, \\[\\begin{align} &amp;P\\left(\\hat\\theta_n - H^{-1}(1-\\alpha/2)\\leq \\theta \\leq \\hat\\theta_n - H^{-1}(\\alpha/2)\\right) \\\\ =&amp; P\\left( H^{-1}(\\alpha/2) \\leq \\hat\\theta_n - \\theta \\leq H^{-1}(1-\\alpha/2) \\right)\\\\ =&amp; P\\left( H^{-1}(\\alpha/2) \\leq R_n \\leq H^{-1}(1-\\alpha/2) \\right)\\\\ =&amp; H\\left( H^{-1}(1-\\alpha/2) \\right) - H\\left( H^{-1}(\\alpha/2) \\right)\\\\ =&amp; 1-\\frac{\\alpha}{2}-\\frac{\\alpha}{2} = 1-\\alpha, \\end{align}\\] proving that \\[ \\left[\\hat\\theta_n - H^{-1}(1-\\alpha/2) , \\hat\\theta_n - H^{-1}(\\alpha/2) \\right] \\] is a \\(1-\\alpha\\) CI for \\(\\theta\\). Now, since \\(H\\) is unknown this is not practical. We can however construct a bootstrap estimate of \\(H\\) by drawing \\(R^\\star_{n,b} = \\hat\\theta^\\star_{n,b}- \\hat\\theta_n\\) and defining: \\[ \\hat H(r) = \\frac{1}{B}\\sum_{b=1}^B I(R^\\star_{n,b}\\leq r). \\] The quantity \\(H^{-1}(\\alpha)\\) is the \\(\\alpha\\) quantile of \\(H\\). In the CI we replace \\(H^{-1}\\) by \\(\\hat H^{-1}\\), i.e. the \\(\\alpha\\) quantile of \\(R^\\star_{n,b}\\). Since \\(\\hat \\theta_n\\) is fixed in the bootstrap sample, the \\(\\alpha\\) quantile of \\(R^\\star_{n,b}\\) is simply \\(\\theta_\\alpha^\\star - \\hat\\theta_n\\), where \\(\\theta_\\alpha^\\star\\) denotes the \\(\\alpha\\) quantile of \\(\\hat\\theta^\\star_{n,b}\\). Finally we get that the \\(1-\\alpha\\) bootstrap pivotal CI is \\[ C_n = \\left[ \\hat\\theta_n - (\\theta^\\star_{1-\\alpha/2} - \\hat\\theta_n) , \\hat\\theta_n - (\\theta^\\star_{\\alpha/2} - \\hat\\theta_n) \\right] = \\left[ 2\\hat\\theta_n - \\theta^\\star_{1-\\alpha/2} , 2\\hat\\theta_n -\\theta^\\star_{\\alpha/2} \\right] \\] Let us implement this on the same data set as above. alpha &lt;- 0.05 T &lt;- median n &lt;- nrow(data.df) B &lt;- 1000 tstar &lt;- array(dim = B) for (i in seq_len(B)) { indices &lt;- sample(seq(1:n), size = n, replace = TRUE) tstar[i] &lt;- T(data.df$x[indices]) } q &lt;- unname( quantile(tstar, probs = c(1-alpha/2, alpha/2)) ) lowerCI &lt;- 2*T(data.df$x) - q[1] lowerCI ## [1] -0.2250317 upperCI &lt;- 2*T(data.df$x) - q[2] upperCI ## [1] 0.3515109 Even simpler is to use the boot library. library(boot) boot.result&lt;- boot(data = data.df, statistic = function(data, index) T(data$x[index]), R = 1000) boot.ci(boot.result, type = &quot;basic&quot;) ## BOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS ## Based on 1000 bootstrap replicates ## ## CALL : ## boot.ci(boot.out = boot.result, type = &quot;basic&quot;) ## ## Intervals : ## Level Basic ## 95% (-0.2389, 0.3129 ) ## Calculations and Intervals on Original Scale A better alternative which we do not cover in this course is the bias-corrected CI. It is however just as easy to use. library(boot) boot.result&lt;- boot(data = data.df, statistic = function(data, index) T(data$x[index]), R = 1000) boot.ci(boot.result, type = &quot;bca&quot;) ## BOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS ## Based on 1000 bootstrap replicates ## ## CALL : ## boot.ci(boot.out = boot.result, type = &quot;bca&quot;) ## ## Intervals : ## Level BCa ## 95% (-0.1333, 0.3701 ) ## Calculations and Intervals on Original Scale "],["3.4-parametric-bootstrap.html", "3.4 Parametric bootstrap", " 3.4 Parametric bootstrap Here we discuss an alternative to the (non-parametric) bootstrap from the previous section. The main idea of the bootstrap was that the empirical distribution \\(\\hat F\\) is (hopefully) close to the actual distribution \\(F\\). Another method of finding an approximation to \\(F\\) is to take some family of distributions, parametrised by a parameter \\(\\theta\\), \\(F_\\theta\\), and find the \\(\\theta\\) that agrees the best with the observed sample. For example by choosing \\(\\theta = \\hat\\theta_{ML}\\). Then we proceed as before. We may now sample from \\(\\hat F = F_{\\hat\\theta}\\) and estimate variances and construct confidence intervals as before. As an example, let us say that we have a random sample from \\(\\mathsf N(\\mu,\\sigma^2)\\) and wish to estimate the variance of the median of the sample. We then estimate \\(\\mu\\) and \\(\\sigma^2\\) with the ML-estimates and let \\(\\hat F = \\mathsf N(\\hat \\mu,\\hat \\sigma^2)\\). Implementing this is not complicated. T &lt;- median n &lt;- nrow(data.df) B &lt;- 1000 mu.hat &lt;- mean(data.df$x) sigma.hat &lt;- sd(data.df$x) for (i in 1:B) { bootstrap.sample &lt;- rnorm(n, mean = mu.hat, sd = sigma.hat) tstar[i] &lt;- T(bootstrap.sample) } sd(tstar) ## [1] 0.1272958 "],["3.5-an-application-2.html", "3.5 An application", " 3.5 An application Here we present an application of what we have learned in this chapter. The application is based on an example in Efron and Hastie (2016). The data set consists of measurements on the kidney function of 157 individuals. We fit a smoothing spline to the data and plot. kidney.df &lt;- read.table(&quot;data/kidney.dat&quot;, header = TRUE) kidney.spline &lt;- smooth.spline(kidney.df$age, kidney.df$tot, df = 10) kidney.df$spline &lt;- predict(kidney.spline, x = kidney.df$age)$y library(ggplot2) cbp1 &lt;- c(&quot;#999999&quot;, &quot;#E69F00&quot;, &quot;#56B4E9&quot;, &quot;#009E73&quot;, &quot;#F0E442&quot;, &quot;#0072B2&quot;, &quot;#D55E00&quot;, &quot;#CC79A7&quot;) ggplot(kidney.df, aes(x = age, y = tot)) + geom_point() + geom_line(aes(y = spline), color = cbp1[2], size = 1) + labs( y = &quot;kidney function&quot;, x=&quot;age&quot;) + theme_minimal() Figure 3.2: Kidney function vs age Now we can predict the kidney function of a new individual of age 50. predict(kidney.spline, x = 50)$y ## [1] -0.7232874 Let us use bootstrap to find the standard deviation of this prediction and construct a CI. library(boot) T &lt;- function(data,index){ s &lt;- smooth.spline(data$age[index], data$tot[index], df = 10) predict(s, x = 50)$y } boot.result&lt;- boot(data = kidney.df, statistic = T, R = 1000) boot.result ## ## ORDINARY NONPARAMETRIC BOOTSTRAP ## ## ## Call: ## boot(data = kidney.df, statistic = T, R = 1000) ## ## ## Bootstrap Statistics : ## original bias std. error ## t1* -0.7232874 0.0455268 0.6037417 boot.ci(boot.result, type = &quot;bca&quot;) ## BOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS ## Based on 1000 bootstrap replicates ## ## CALL : ## boot.ci(boot.out = boot.result, type = &quot;bca&quot;) ## ## Intervals : ## Level BCa ## 95% (-2.0496, 0.3520 ) ## Calculations and Intervals on Original Scale References "],["3.6-summary-2.html", "3.6 Summary", " 3.6 Summary Perhaps the most complicated section in this chapter is the derivation of the form of the basic bootstrap interval. In this section we see that it is actually just a more general case of the normally distributed CIs that we have seen in previous courses. We wish to form a confidence interval for the parameter \\(\\theta\\) that we estimate by \\(\\hat \\theta\\). We will assume that \\(R:= \\hat \\theta - \\theta \\sim \\mathsf N(0,\\sigma^2)\\). Recall that the usual \\(1-\\alpha\\) confidence interval is \\[ \\left[ \\hat\\theta - \\sigma z_{\\alpha/2}, \\hat\\theta + \\sigma z_{\\alpha/2} \\right]. \\] Let, as for the bootstrap confidence, \\(H\\) be the distribution function of \\(R\\), that is \\[ H(z) = P(R\\leq z) = P(\\hat\\theta - \\theta \\leq z) = P\\Big(\\frac{\\hat\\theta - \\theta}{\\sigma}\\leq \\frac{z}{\\sigma}\\Big) = \\Phi(z/\\sigma), \\] where \\(\\Phi\\) denotes the distribution function of the standard normal distribution. Now, if we choose \\(z = H^{-1}(\\alpha)\\), then \\[ \\alpha = H(H^{-1}(\\alpha)) = \\Phi\\left( \\frac{H^{-1}(\\alpha)}{\\sigma} \\right). \\] Now apply \\(\\Phi^{-1}\\) on both sides, \\[ \\Phi^{-1}(\\alpha) = \\frac{H^{-1}(\\alpha)}{\\sigma}. \\] The number \\(z_\\alpha\\) is by definition the number such that \\(\\Phi(-z_\\alpha) = \\alpha\\), or stated differently \\(\\Phi^{-1}(\\alpha) = -z_\\alpha\\). Then we have that \\[ \\sigma z_{\\alpha/2} = -\\sigma\\Phi^{-1}(\\alpha/2) = -H^{-1}(\\alpha/2). \\] Therefore the upper limit of the CI can be written \\[ \\hat \\theta + \\sigma z_{\\alpha/2} = \\hat \\theta -H^{-1}(\\alpha/2). \\] We conclude that the basic bootstrap interval is of the same form as the usual normal CI. The lower limit can be obtained with similar calculations. In the bootstrap interval the next step was to replace the unknown \\(H\\) with a bootstrap estimate. "],["3.7-review-questions-2.html", "3.7 Review questions", " 3.7 Review questions When writing \\(\\int x dF(x)\\), what does \\(dF(x)\\) mean? In terms of the distribution function \\(F\\), what is the median of a distribution? What is the empirical distribution function? What is a plug-in estimator? In bootstrap, what is \\(X_1^*\\)? What does it mean, in practice, when we write \\(X_1^*,\\ldots,X_n^*\\sim \\hat F_n\\)? How is the basic bootstrap interval calculated? What is the difference between parametric and non-parametric bootstrap? "],["4-ch-statLearn.html", "Chapter 4 Statistical learning", " Chapter 4 Statistical learning In this chapter we will discuss statistical learning from a mostly theoretical perspective. That is, we will discuss when and why statistical learning can be expected to work. We begin by defining what we mean by statistical learning and the different types of statistical learning. Then we will display some examples that motivate our theoretical discussion and after that we will see two different theoretical perspectives on learning. We begin by defining two spaces: \\[\\begin{align} \\mathcal X &amp;= \\text{ Input space}\\\\ \\mathcal Y &amp;= \\text{ Output space}. \\end{align}\\] We assume that there is a distribution on \\(\\mathcal X \\times \\mathcal Y\\), that we call \\(P_{X,Y}\\). This means for example that we can calculate, at least in principle, expectations like, \\[ E\\left[ f(X,Y)\\right] = \\int f(x,y)p_{X,Y}(x,y)dxdy. \\] There is also a loss function \\(l:\\mathcal Y \\times \\mathcal Y \\mapsto \\mathbb R\\) that tells how close two points in \\(\\mathcal Y\\) are to each other. The problem in statistical learning is to find a function \\(h:\\mathcal X \\mapsto \\mathcal Y\\) that takes an input \\(x\\in \\mathcal X\\) and gives and output \\(y\\in\\mathcal Y\\). For a given \\(h\\) we define the out-of-sample error as \\[ E_{out}(h) = E_{X,Y}\\left[l(h(X),Y)\\right]. \\] This tells us how the function \\(h\\) performs on average, on random observation from \\(P_{X,Y}\\). We would like to choose the function \\(h\\) so that this error is as small as possible. The fundamental problem of learning is that the distribution of \\(X,Y\\) is unknown and so direct minimization of \\(E_{out}\\) is not possible. Methods in statistical learning can be partitioned in various ways. The two main ways is in supervised and unsupervised learning. In supervised learning we are given a set of \\(n\\) examples \\((x_1,y_1),\\ldots,(x_n,y_n)\\) and the task is to predict \\(y\\) for a previously unseen point \\(x\\). In unsupervised learning we are only given \\(x_1,\\ldots, x_n\\) and the goal is to describe the associations and patterns among a set of input measures. A third scenario is reinforcement learning, that we will not cover in this course. In supervised learning the two main tasks are classification and regression. In classification the task is to assign a class to each item. If it is binary classification then, in the terms defined above, \\(\\mathcal Y = \\{-1,1\\}\\). Many times the loss is taken to be the 0-1 loss, \\[l(y_1,y_2)=1_{y_1\\neq y_2}(y_1,y_2):=\\begin{cases} 0\\text{ if } y_1=y_2,\\\\ 1\\text{ otherwise }. \\end{cases} \\] In regression the task is to predict a real value for each item, \\(\\mathcal Y = \\mathbb R\\). Many times in regression the squared error is used, \\(l(y_1,y_2)=(y_1-y_2)^2\\). Readings for this chapter is: ISLR 2 ISLR 4.1-2 ISLR 5 ISLR 6.1-2 ISLR 9 The sections on Hoeffding’s inequality, generalization error and VC-dimension is not in the course literature. So it is enough to read these notes. "],["4.1-classification.html", "4.1 Classification", " 4.1 Classification In this section we discuss the theory of classification, without going in to detail about any particular classification method. We are given training data \\((x_i,y_i)\\quad i=1,\\ldots, n\\) that we assume are a random sample from an unknown distribution \\(P_{X,Y}\\). The goal is to find a classifier, that is a function \\(h:\\mathcal X \\mapsto \\mathcal Y= \\left\\{-1,1\\right\\}\\), based on the training data. There is a loss function \\(l:\\mathcal Y\\times \\mathcal Y \\mapsto \\mathbb R\\) and most commonly this is the 0-1 loss, \\[ l(h(x),y) :=1_{h(x)\\neq y}(x,y). \\] Then the out-of-sample error is \\[ E_{out}(h) = E_{X,Y}\\left[ l(h(X),Y) \\right] = P(h(X)\\neq Y). \\] That is, it is the probability of classifying a random observation \\(X\\) incorrectly, when using the classifier \\(h\\). To simplify notation we also define the conditional class probability, \\[ \\eta(x):=P\\left( Y=1\\mid X=x \\right). \\] In the hypothetical case where we know the distribution of \\(X,Y\\), the \\(h\\) that minimizes this out-of-sample error is called the Bayes classifier. We claim that it is \\[\\begin{align*} h^\\star(x) &amp;= \\begin{cases} 1 &amp; \\text{if } \\eta(x)\\geq 1/2\\\\ -1 &amp; \\text{if } \\eta(x)&lt; 1/2 \\end{cases}\\\\ &amp;= \\text{sign} (\\eta(x) - 1/2). \\end{align*}\\] That is, we should classify to the class that has the highest probability, conditioned on \\(X\\). The minimal out-of-sample error is called the Bayes risk. Let us prove this. The claim is that for any other classifier \\(h(x)\\), the out-of-sample error is at least as large, that is \\[ E_{out}(h) = P\\left( h(X)\\neq Y \\right) \\geq P\\left( h^\\star(X)\\neq Y \\right) = E_{out}(h^\\star). \\] Or, equivalently, \\(P\\left( h^\\star(X)= Y \\right)\\geq P\\left( h(X) = Y \\right)\\). First note that if we condition on \\(X=x\\), then either \\(h(x)=-1\\) or \\(h(x)=1\\). Therefore, \\[\\begin{align*} P\\left( h(X) = Y \\mid X=x\\right) &amp;= 1_{h(x) = 1}(x)P\\left(Y=1 \\mid X=x\\right) + 1_{h(x) = -1}(x)P\\left(Y=-1 \\mid X=x\\right)\\\\ &amp;= 1_{ h(x) = 1}(x)\\eta(x) + (1-1_{ h(x) = 1}(x))(1-\\eta(x))\\\\ &amp;= 1_{ h(x) = 1}(x)\\left( 2\\eta(x)-1 \\right) +1-\\eta(x). \\end{align*}\\] and the same is true if we replace \\(h\\) by \\(h^\\star\\). Then, \\[ P\\left( h^\\star(X) = Y \\mid X=x\\right) -P\\left( h = Y \\mid X=x\\right) = \\left(1_{ h^\\star(x) = 1}(x) - 1_{ h(x) = 1}(x)\\right)\\left( 2\\eta(x)-1 \\right). \\] Now if \\(x\\) is such that \\(\\eta(x)\\geq 1/2\\), then \\[ \\underbrace{\\left(\\underbrace{1_{ h^\\star(x) = 1}(x)}_{=1} - \\underbrace{1_{ h(x) = 1}(x)}_{= 0 \\text{ or } 1}\\right)}_{\\geq 0}\\underbrace{\\left( 2\\eta(x)-1 \\right)}_{\\geq 0}\\geq 0. \\] On the other hand, if \\(x\\) is such that \\(\\eta(x)&lt; 1/2\\), then \\[ \\underbrace{\\left(\\underbrace{1_{ h^\\star(x) = 1}(x)}_{=0} - \\underbrace{1_{ h(x) = 1}(x)}_{= 0 \\text{ or } 1}\\right)}_{\\leq 0}\\underbrace{\\left( 2\\eta(x)-1 \\right)}_{&lt; 0}\\geq 0. \\] In any case, this implies that \\[ P\\left( h^\\star(X) = Y \\mid X=x\\right) -P\\left( h = Y \\mid X=x\\right) \\geq 0. \\] Since this is true for any \\(x\\), it also holds that \\[ P\\left( h^\\star(X) = Y \\right) -P\\left( h(X) = Y \\right) \\geq 0, \\] as we claimed. Since the Bayes classifier is not available in practice, one needs to take another approach. One is to estimate the function \\(\\eta(x)\\) and then construct a classifier by plugging in to the Bayes classifier. That is, \\[ h(x) = \\text{sign} (\\hat\\eta(x) - 1/2). \\] Logistic regression can be thought of as an example of this. Another option is to find a function \\(h\\) that directly minimizes the in-sample error, \\[ E_{in} = \\frac{1}{n} \\sum_{i=1}^n I(h(x_i)\\neq y_i). \\] It would be done by specifying a class of candidate classifiers \\(\\mathcal H\\) from which we pick the best \\(h\\). For example, let us assume that \\(x=(x^1,x^2,\\ldots, x^p)\\in \\mathbb R^p\\) and we might then consider all the linear classifiers, \\[ \\mathcal H = \\left\\{ h(x) = \\text{sign}(\\beta_0 + x^1\\beta_1 + \\ldots + x^p\\beta_p) \\mid \\beta \\in \\mathbb R^p, \\beta_0\\in\\mathbb R \\right\\}. \\] Note that if \\(y_i(\\beta_0 + x^1\\beta_1 + \\ldots + x^p\\beta_p)&gt;0\\), then \\(y_i\\) is classified correctly and if \\(y_i(\\beta_0 + x^1\\beta_1 + \\ldots + x^p\\beta_p)&lt;0\\), then \\(y_i\\) is misclassified. We can therefore think of \\(y_i(\\beta_0 + x^1\\beta_1 + \\ldots + x^p\\beta_p)\\) as measuring how close \\(y_i\\) is to being classified correctly or incorrectly. In fact, we can write the 0-1 loss as \\[ l(h(x),y) = I_{y(\\beta_0 + x^1\\beta_1 + \\ldots + x^p\\beta_p)\\leq 0}(x,y). \\] More generally we can consider a function \\(f(x)\\), a classifier \\(h(x) = \\text{sign}(f(x))\\) and a loss function that depends on the margin \\(yf(x)\\). It turns out that \\(E_{in}\\), as defined above, is difficult to use for training. The reason for this can be understood in different ways. Mathematically, the 0-1 loss is non-convex, and non-convex functions are in general difficult to optimize. In terms of classification, consider the picture below. Two points are misclassified, but we can see that by moving the classification boundary, we can find a classifier that only misclassifies one point. However, the training algorithm will try to move the boundary a very small step, and see if that gives an improvement. If we use the 0-1 loss, the in-sample error will be the same as long as the boundary is not moved far enough. Therefore it is better to use a loss function that also measures how far away each point is from being classified correctly/incorrectly. Figure 4.1: Example of linear classification Below we discuss two alternative loss functions that produce two much used methods for classification, the hinge loss and the negative log-likelihood. They are plotted in the picture below. Figure 4.2: Loss functions for classification The hinge loss is \\[ l(f(x),y)=(1-yf(x))_+, \\] where \\((\\cdot)_+\\) indicates the positive part. This function takes care of our complaints about the 0-1 loss function. If a point \\(x_i\\) is correctly classified, and it is far away from being misclassified, so that \\(y_if(x_i)\\) is large and positive, the loss is 0. However if it is close to being misclassified it incurs a loss, even if it is correctly classified. It might however worry some that we are using a different function for training (e.g. the hinge loss) and evaluation (0-1). Let us therefore see what the population minimizer of the hinge loss is. The population minimizer for the 0-1 loss is the Bayes classifier, and for the hinge loss, \\[ f^\\star_{hinge} := \\underset{f}{\\text{argmin}}~ E\\left[(1-Yf(X))_+ \\right]. \\] Let us fix an arbitrary \\(x\\) and then we should find \\(f(x)\\) that minimizes \\[ E\\left[(1-Yf(x))_+ \\mid X=x\\right] = (1-f(x))_+\\eta(x) + (1+f(x))_+(1-\\eta(x)). \\] We should always have \\(-1\\leq f(x)\\leq 1\\), because otherwise we could truncate \\(f(x)\\) to get a smaller loss. So with that assumption, \\[ E\\left[(1-Yf(x))_+ \\mid X=x\\right] = (1-f(x))\\eta(x) + (1+f(x))(1-\\eta(x)) = 1+(1-2\\eta(x))f(x). \\] Then we realize that depending on the sign of \\(1-2\\eta(x)\\) we should choose \\(f(x)\\) to be as large positive or negative as possible, that is \\[ f^\\star_{hinge} = \\begin{cases} 1&amp;\\text{if } \\eta\\geq 1/2\\\\ -1&amp;\\text{if } \\eta&lt; 1/2, \\end{cases} \\] which is exactly the Bayes classifier. The other loss function we will discuss is the loss function of logistic regression. In logistic regression we model the conditional probability as \\[ \\eta(x) = \\frac{1}{1+e^{-f(x)}}. \\] The likelihood of the observation \\((x,y)\\) is therefore, \\[ L(x,y) = \\begin{cases} \\eta(x)&amp;\\text{if } y=1\\\\ 1-\\eta(x)&amp;\\text{if } y=-1. \\end{cases} \\] Then the negative log-likelihood (which should be minimized) is, \\[ -\\log_2(L(x,y)) = \\begin{cases} \\log_2\\left(1+e^{-f(x)}\\right)&amp;\\text{if } y=1\\\\ \\log_2\\left(1+e^{f(x)}\\right)&amp;\\text{if } y=-1 \\end{cases} = \\log_2(1+e^{-yf(x)})=:l(f(x),y) \\] Here we took base 2 logarithm since \\(\\log_2(1+e^0)=1\\) and then the loss function is on the same scale as the hinge and 0-1. Also for logistic regression you can show that if you classify according to the class with highest probability, the population minimizer is again the Bayes classifier. "],["4.2-support-vector-machines-i.html", "4.2 Support vector machines I", " 4.2 Support vector machines I In this section will discuss binary classification and in particular support vector machines (SVM). The approach taken here is different from the one in ISL. Our approach is easier to explain, generalizes to other method and perhaps also more modern. The approach in ISL however provides a different intuition and is also relevant when implementing the algorithms. We are given training examples \\((x_i,y_i)\\), where \\(x_i = (x_i^1,x_i^2,\\ldots,x_i^p) \\in \\mathbb R^p\\) and \\(y_i\\in \\left\\{-1,1\\right\\}\\). The equation \\[ f(x):=\\beta_0 + x^1\\beta + \\ldots + x^p\\beta_p = 0, \\] defines a hyperplane (e.g. a line in \\(\\mathbb R^2\\), a plane in \\(\\mathbb R^3\\)). We are going to classify as +1 if the point is on one side of the hyperplane, \\(\\beta_0 + x^1\\beta + \\ldots + x^p\\beta_p&gt;0\\), and -1 if it is on the other side, \\(\\beta_0 + x^1\\beta + \\ldots + x^p\\beta_p&lt;0\\). In other words, the classification rule is \\[ h(x) = \\text{sign}(\\beta_0 + x^1\\beta + \\ldots + x^p\\beta_p). \\] The value of \\(f(x_i)\\) tells us how far away from the hyperplane the point is and if \\(y_if(x_i)&gt;0\\) the point is classified correctly. That is, if \\(y_if(x_i)\\) is large and positive, the point \\(x_i\\) is classified correctly and with a safe margin. If \\(y_if(x_i)\\) is large and negative, the point is classified incorrectly and is far away from being correctly classified. We will consider the hinge loss \\[ l(y,f) = (1-yf)_+ , \\] here \\((\\cdot)_+\\) indicates the positive part and we will minimize the in-sample error \\[ \\underset{\\beta_0,\\beta}{\\text{minimize}}\\quad \\frac{1}{n}\\sum_{i=1}^n (1-y_if(x_i))_+. \\] As an example, we generate some training data from a mixture of normal distributions. library(mvtnorm) set.seed(42) mu.p &lt;- rmvnorm(10,mean = c(1,0), sigma = diag(2)) mu.n &lt;- rmvnorm(10,mean = c(0,1), sigma = diag(2)) n.samples &lt;- 100 data.matrix &lt;- matrix(nrow = 2*n.samples, ncol = 3) for (i in seq_len(n.samples)) { mu = mu.p[sample(x= nrow(mu.p), size = 1),] sample &lt;- rmvnorm(1, mean = mu, sigma = diag(2)/5) data.matrix[2*i-1,] &lt;- c(sample, 1) mu = mu.n[sample(x= nrow(mu.n), size = 1),] sample &lt;- rmvnorm(1, mean = mu, sigma = diag(2)/5) data.matrix[2*i,] &lt;- c(sample, -1) } data.df &lt;- data.frame(data.matrix) colnames(data.df) &lt;- c(&quot;x1&quot;,&quot;x2&quot;,&quot;y&quot;) data.df$y &lt;- as.factor(data.df$y) Then use R to calculate the hyperplane that minimizes the in-sample error. library(kernlab) svm.model &lt;- ksvm(y~x1+x2, data = data.df, type = &quot;C-svc&quot;, kernel = &quot;vanilladot&quot;, C = 1000) grid &lt;- expand.grid(x1 = seq(-5,5, length = 500), x2 = seq(-5,5, length = 500)) grid$predicted &lt;- as.factor(predict(svm.model, grid)) Figure 4.3: Training data and linear classification with hinge loss We can also calculate the in-sample error mean(data.df$y != predict(svm.model, data.df)) ## [1] 0.285 Not so bad, but let us try to improve it. We select some basis functions, \\(\\varphi_m(x)\\), \\(m=1,\\ldots, M\\) and use the same classifier but with input features \\(\\varphi(x) = (\\varphi_1(x),\\ldots, \\varphi_M(x))\\). We can for example choose \\(\\varphi_m\\) to be polynomials of increasing order. For order 2, we get the classifier below, an ellipsoid. library(kernlab) svm.model &lt;- ksvm(y~poly(x1, x2, degree = 2), data = data.df, type = &quot;C-svc&quot;, kernel = &quot;vanilladot&quot;, C = 1000) grid &lt;- expand.grid(x1 = seq(-5,5, length = 500), x2 = seq(-5,5, length = 500)) grid$predicted &lt;- as.factor(predict(svm.model, grid)) Figure 4.4: Training data and quadratic classification with hinge loss This time the in-sample error is mean(data.df$y != fitted(svm.model)) ## [1] 0.275 Better. Let us continue with increasing order polynomials, and calculate the error. library(kernlab) maxDegree &lt;- 20 error.df &lt;- data.frame(&quot;degree&quot; = 1:maxDegree, inError = NA, outError = NA) svm.model.list &lt;- vector(mode = &quot;list&quot;, length = maxDegree) for (degree in seq(1,maxDegree)) { svm.model.list[degree] &lt;- ksvm(y ~ poly(x1, x2, degree = degree), data = data.df, type = &quot;C-svc&quot;, kernel = &quot;vanilladot&quot;, C = 1000) error.df$inError[degree] &lt;- mean(data.df$y != predict(svm.model.list[[degree]], data.df)) } Figure 4.5: In sample error vs. degree of polynomial, using hinge loss In-sample error gets smaller as we increase the order of the polynomial. For degree 20, the in-sample error is error.df$inError[20] ## [1] 0.03 The classifier looks complex. Figure 4.6: Training data and degree 20 polynomial classification Since we know the data generating distribution, we can approximate the out-of-sample error for each classifier, by simulation. library(mvtnorm) set.seed(42) n.samples &lt;- 1e4 data.matrix &lt;- matrix(nrow = 2*n.samples, ncol = 3) for (i in seq_len(n.samples)) { mu = mu.p[sample(x= nrow(mu.p), size = 1),] sample &lt;- rmvnorm(1, mean = mu, sigma = diag(2)/5) data.matrix[2*i-1,] &lt;- c(sample, 1) mu = mu.n[sample(x= nrow(mu.n), size = 1),] sample &lt;- rmvnorm(1, mean = mu, sigma = diag(2)/5) data.matrix[2*i,] &lt;- c(sample, -1) } data.test.df &lt;- data.frame(data.matrix) colnames(data.test.df) &lt;- c(&quot;x1&quot;,&quot;x2&quot;,&quot;y&quot;) data.test.df$y &lt;- as.factor(data.test.df$y) for (degree in seq(1,maxDegree)) { error.df$outError[degree] &lt;- mean(data.test.df$y != predict(svm.model.list[[degree]], data.test.df)) } Figure 4.7: Out-of-sample error vs. degree of polynomial. The in-sample error is decreasing as the degree of the polynomial increases, and so also the complexity of the model. When the degree is small, the in-sample error is a close approximation of the out-of-sample error, but as the degree increases the difference increases and for large degrees the in-sample error provides little information about the out-of-sample error. The out-of-sample error is at first decreasing, but unlike the in-sample-error, starts increasing after degree 4. In the following sections we will investigate the connection between the in-sample and out-of-sample error. "],["4.3-hoeffdings-inequality.html", "4.3 Hoeffding’s inequality", " 4.3 Hoeffding’s inequality One tool to understand the connection between the in-sample and out-of-sample error is Hoeffding’s inequality. This is a result from probability theory and so we will present it as such. That is, in this section we do not discuss any application to statistical learning. Hoeffding’s inequality states that: Let \\(Y_1,\\ldots, Y_n\\) be iid with \\(E[Y_i]=\\mu\\) and \\(a\\leq Y_i \\leq b\\). Then for any \\(\\varepsilon&gt;0\\), \\[ P\\left( \\left| \\bar Y_n - \\mu \\right|&gt;\\varepsilon \\right) \\leq 2e^{-2n\\varepsilon^2/(b-a)^2}. \\] The inequality is true for any random variable satisfying the conditions. However, we will only prove the special case that if \\(Y_i\\overset{iid}{\\sim}\\mathsf{Be}(p)\\), then \\[ P\\left( \\left| \\bar Y_n - p \\right|&gt;\\varepsilon \\right) \\leq 2e^{-2n\\varepsilon^2}. \\] We start by deriving Markov’s inequality: Let \\(X\\) be a non-negative random variable. For the sake of this calculation we assume \\(X\\) is continuous with density \\(p(x)\\), although the inequality holds also when this is not the case. Then for any \\(t&gt;0\\), \\[\\begin{align} E[X] &amp;= \\int_0^\\infty xp(x)dx = \\int_0^t xp(x)dx + \\int_t^\\infty xp(x)dx \\\\ &amp;\\geq \\int_t^\\infty x p(x)dx \\geq t \\int_t^\\infty p(x)dx = tP(X&gt; t). \\end{align}\\] We usually write this as \\(P(X&gt;t)\\leq E(X)/t\\). Now we turn to Hoeffding’s inequality. Firstly, \\[ P\\left( \\left| \\bar Y_n - p \\right|\\geq \\varepsilon \\right) = P\\left( \\bar Y_n \\geq p+\\varepsilon \\right) + P\\left( \\bar Y_n \\leq p - \\varepsilon \\right). \\] Then we use that \\(\\exp\\) is an increasing function, together with Markov’s inequality, for any \\(t&gt;0\\), \\[\\begin{align*} P\\left( \\bar Y_n \\geq p + \\varepsilon \\right) &amp;= P\\left( t\\sum_{i=1}^n Y_i \\geq tn(p + \\varepsilon) \\right) = P\\left( e^{t\\sum_{i=1}^n Y_i }\\geq e^{tn(p +\\varepsilon) } \\right) \\\\ &amp;\\overset{\\text{Markov}}{\\leq} e^{-tn(p+\\varepsilon)}E\\left[ e^{t\\sum_{i=1}^n Y_i } \\right] \\overset{indep.}{=} e^{-tn(p+\\varepsilon)}\\prod_{n=1}^nE\\left[ e^{t Y_i } \\right] = e^{-tn(p+\\varepsilon)}E\\left[ e^{t Y_i } \\right]^n. \\end{align*}\\] Now we need to bound the last expression. Since \\(Y_i\\) is Bernoulli, \\[ E\\left[ e^{t Y_i } \\right] = e^{t\\cdot 1}P(Y_i=1) + e^{t\\cdot 0}P(Y_i=0) = pe^t + (1-p). \\] We would like to show that \\(pe^t+1-p\\leq e^{tp+t^2/8}\\). Since then, \\[ P\\left( \\bar Y_n \\geq p + \\varepsilon \\right) \\leq e^{-tn(p+\\varepsilon)}E\\left[ e^{t Y_i } \\right]^n \\leq e^{-tn(p+\\varepsilon)} e^{ntp+nt^2/8} = e^{-nt\\varepsilon + nt^2/8} \\leq e^{-2n\\varepsilon^2}, \\] which is what we want. In the last step we used that \\(-nt\\varepsilon + nt^2/8\\) has a maximum at \\(t=4\\varepsilon\\), which is easy to check. A similar argument will give that \\(P\\left( \\bar Y_n \\leq p - \\varepsilon \\right)\\) can be bounded in the same way and then we arrive at Hoeffding’s inequality. So, we need to study \\(pe^t+1-p\\) and \\(e^{tp+t^2/8}\\). First note that the inequality is true if and only if \\[ f(t):=\\ln (pe^t+1-p) \\leq tp+t^2/8, \\] since \\(\\ln\\) is increasing. By Taylor’s theorem, we can write \\[ f(t) = f(0) + f&#39;(0)t + f&#39;&#39;(\\zeta)\\frac{t^2}{2}, \\] for some \\(0\\leq \\zeta \\leq t.\\) However \\(f(0)=0\\) and \\(f&#39;(0) = \\frac{pe^0}{pe^0+1-p} = p\\) and \\[ f&#39;&#39;(t) = \\frac{pe^t}{pe^t+1-p} - \\frac{(pe^t)^2}{(pe^t+1-p)^2} = \\frac{pe^t}{pe^t+1-p}\\left( 1-\\frac{pe^t}{pe^t+1-p} \\right) = \\rho(1-\\rho), \\] with \\(\\rho:= \\frac{pe^t}{pe^t+1-p}.\\) Now, it is easy to see that \\(0\\leq\\rho\\leq 1\\) therefore that \\(\\rho(1-\\rho)\\leq 1/4\\) so that \\(f&#39;&#39;(t)\\leq 1/4.\\) All together, \\[ f(t) = tp + f&#39;&#39;(\\zeta)\\frac{t^2}{2} \\leq tp + \\frac{t^2}{8}, \\] and we are done. If we choose \\[ \\varepsilon = \\sqrt{\\frac{1}{2n}\\ln \\frac{2}{\\delta}}, \\] we get \\[ P\\left( \\left| \\bar Y_n - p \\right|\\leq \\sqrt{\\frac{1}{2n}\\ln \\frac{2}{\\delta}} \\right) \\geq 1-2\\exp\\left( -2n\\frac{1}{2n}\\ln \\frac{2}{\\delta} \\right) =1-\\delta. \\] Stated another way, with probability at least \\(1-\\delta\\), \\[ \\left| \\bar Y_n - p \\right|\\leq \\sqrt{\\frac{1}{2n}\\ln \\frac{2}{\\delta}}. \\] "],["4.4-generalization-error.html", "4.4 Generalization error", " 4.4 Generalization error In this section we will study the generalization error, that is the difference between \\(E_{out}(h)\\) and \\(E_{in}(h)\\). We will only consider binary classification and we will assume that there is a function \\(h\\) such that \\(Y=h(X)\\). This is a simplification since it means that we only need to consider the distribution of \\(X\\). To make things concrete we study an example where \\(\\mathcal X = \\mathbb R^2\\) and a linear classification, where observations are classified according to which side of a straight line they fall. Before looking at the data, let us choose a classification algorithm completely willy-nilly. Let us choose: \\[ h(x_1,x_2)=\\begin{cases} 1 \\text{ if } x_1 &gt;0\\\\ -1 \\text{ otherwise}. \\end{cases} \\] Now we generate some data: Figure 4.8: Training data and classification rule We can now calculate the in-sample error: H &lt;- function(a, b){ function(x1, x2){ if(a*x1 + b*x2 &gt; 0) &quot;1&quot; else &quot;-1&quot; } } h &lt;- H(1,0) error &lt;- function(data, classifier){ missClass &lt;- 0 for (i in seq_len(nrow(data))) { x1 &lt;- data[i,]$x1 x2 &lt;- data[i,]$x2 y &lt;- data[i,]$y if(classifier(x1,x2) != y ) missClass &lt;- missClass + 1 } missClass/nrow(data) } error(data.df, h) ## [1] 0.32 In this case, \\(h\\) is not a function of the training data so the in-sample error is an unbiased estimate of the out-of-sample error. Since we know the distribution of \\(X,Y\\) we can calculate the out-of-sample error by simulation. n.samples &lt;- 1e4 data.matrix &lt;- matrix(nrow = n.samples, ncol = 3) for (i in seq_len(n.samples)) { if (runif(1)&gt;0.5) { sample &lt;- rmvnorm(1, mean = mu.p, sigma = diag(sd.p)) data.matrix[i,] &lt;- c(sample, 1) } else{ sample &lt;- rmvnorm(1, mean = mu.n, sigma = diag(sd.n)) data.matrix[i,] &lt;- c(sample, -1) } } data.test.df &lt;- data.frame(data.matrix) colnames(data.test.df) &lt;- c(&quot;x1&quot;,&quot;x2&quot;,&quot;y&quot;) data.test.df$y &lt;- as.factor(data.test.df$y) error(data.test.df, h) ## [1] 0.3548 Using Hoeffding’s inequality, we can give a guarantee of the difference between the in-sample and the out-of-sample error. Let us review: A pair \\(X,Y\\) is drawn from some distribution. We then apply the function \\(h\\) and with some probability we make an error on the classification, this probability is \\(E_{out}(h)\\) and corresponds to the probability \\(p\\) in Hoeffding. What we have is a sample of size \\(n\\) and an estimate \\(E_{in}(h)\\) of the probability of an error. This corresponds to \\(\\bar Y_n\\) in Hoeffding. Therefore this situation is exactly like the Bernoulli experiment from the previous section. We can say that with probability at least \\(1-\\delta\\), \\[ E_{out}(h) \\leq E_{in}(h) + \\sqrt{\\frac{\\ln \\frac{2}{\\delta}}{2n}}. \\] Let us say we want to have confidence 95%, that is \\(\\delta = 0.05\\), we then have the generalization bound delta = 0.05 error(data.df,h) + sqrt(log(2/delta)/(2*nrow(data.df))) ## [1] 0.5120646 That is, with confidence 95%, the out-of-sample error is not larger than this. Above we picked an \\(h\\) without looking at the data and so it can not really be considered statistical learning. Therefore, for the generalization bound to be useful, we need to handle the situation where \\(h\\) is chosen from some collection \\(\\mathcal H\\). Let us first consider the case where \\(\\mathcal H\\) is finite, that is there is a finite number of functions \\(h\\) in the collection. Call this number \\(|\\mathcal H|\\). The statement of the learning bound then becomes Let \\(\\mathcal H\\) be finite. Then for any \\(\\delta&gt;0\\), with probability at least \\(1-\\delta\\): \\[ \\forall h\\in\\mathcal H,\\quad E_{out}(h)\\leq E_{in}(h) + \\sqrt{\\frac{ \\ln\\frac{2|\\mathcal H|}{\\delta}}{2n}}. \\] The proof of this is again an application of Hoeffding. Let \\(h_1,\\ldots,h_{|\\mathcal H|}\\) be the elements of \\(\\mathcal H\\). Then \\[\\begin{align} &amp;P\\left( \\exists h\\in\\mathcal H : \\left| E_{in}(h) - E_{out}(h) \\right|&gt;\\varepsilon \\right)\\\\ =&amp; P\\left( \\left\\{ \\left| E_{in}(h_1) - E_{out}(h_1) \\right| &gt; \\varepsilon \\right\\} \\cup \\ldots \\cup \\left\\{ \\left| E_{in}(h_{|\\mathcal H |}) - E_{out}( (h_{| \\mathcal H |}) \\right| &gt; \\varepsilon \\right\\} \\right)\\\\ \\leq &amp; \\sum_{i=1}^{|\\mathcal H|} P\\left(\\left| E_{in}(h_i) - E_{out}(h_i) \\right|&gt;\\varepsilon \\right)\\\\ \\leq &amp; 2 |\\mathcal H | e^{-2n\\varepsilon^2}. \\end{align}\\] Setting this equal to \\(\\delta\\) and solving for \\(\\varepsilon\\) gives the result. Now we can handle also the case with a finite number of \\(h\\). But it is still not very useful. Most classifications methods will have an infinite \\(\\mathcal H\\). For example, in the example above, we would consider all different straight lines and no only \\(x_1 = 0\\). The above argument will not work for infinite \\(\\mathcal H\\). There is however some hope. We used that, for two events \\(A\\) and \\(B\\), \\[ P(A\\cup B) = P(A) + P(B) - P(A\\cap B) \\leq P(A) + P(B). \\] But this inequality is not very tight. When \\(A\\) and \\(B\\) tend to happen at the same time \\(P(A \\cup B)\\approx P(A)\\) and so the right hand side will be roughly twice the left hand side. We can expect this to happen also for the generalization errors. That is, many \\(h\\) in \\(\\mathcal H\\) are similar, so if \\(\\left| E_{in}(h_1) - E_{out}(h_1) \\right|&gt;\\varepsilon\\) it is likely that also \\(\\left| E_{in}(h_2) - E_{out}(h_2) \\right|&gt;\\varepsilon\\). "],["4.5-vc-dimension.html", "4.5 VC-dimension", " 4.5 VC-dimension In this section we discuss a measure of the complexity of a hypothesis set \\(\\mathcal H\\), the Vapnik–Chervonenkis (VC) dimension. We call one possible way of labeling a set of points, \\(S\\), a dichotomy and we say that \\(S\\) is shattered by \\(\\mathcal H\\) when \\(\\mathcal H\\) can generate all possible dichotomies of \\(S\\). The VC-dimension of \\(\\mathcal H\\) is the size of the largest set that can be shattered by \\(\\mathcal H\\). First consider the example of the real line and \\(\\mathcal H\\) being the classifiers that classify to one class to the left of a point and to the other class to the right of the point. If \\(S\\) consists of two points, all four dichotomies can be realized and so \\(S\\) can be shattered by \\(\\mathcal H\\). However if \\(S\\) consists of three points, the dichotomy (1,0,1) cannot be realized and \\(S\\) cannot be shattered. Therefore the VC-dimension of \\(\\mathcal H\\) is \\(d_{VC}=2\\). Figure 4.9: Two points can be shattered Figure 4.10: Three points that can not be shattered Next consider points on \\(\\mathbb R^2\\) and \\(\\mathcal H\\) being the set of straight lines, as in the example in the previous section. Clearly three points can be shattered by a straight line, as long as the points are not colinear. However four points can never be shattered. So the VC-dimension is \\(d_{VC}=3\\). Similarly in \\(R^p\\), if \\(\\mathcal H\\) are the hyperplanes, \\(d_{VC} = p+1\\). Figure 4.11: Three points can be shattered but not four We see a pattern here, the VC-dimension is equal to the number of parameters of \\(\\mathcal H\\). This is however not always true. As a counterexample, take the functions \\(\\left\\{ x\\mapsto sin(\\omega x)\\mid \\omega\\in \\mathbb R \\right\\}\\). Then label a point \\(x\\) according to the sign of \\(sin(\\omega x)\\). There is only one parameter, but any \\(S\\) can be shattered and so \\(d_{VC}=\\infty\\). Figure 4.12: A one parameter function can shatter any number of points The usefulness of the VC-dimension comes from the following result. Let \\(\\mathcal H\\) have VC-dimension \\(d_{VC}\\). Then for any \\(\\delta&gt;0\\), with probability at least \\(1-\\delta\\), the following holds for all \\(h\\in \\mathcal H\\) (Mohri, Rostamizadeh, and Talwalkar 2018): \\[ E_{out}(h) \\leq E_{in}(h) +\\sqrt{\\frac{2d_{VC}\\ln \\frac{en}{d_{VC}}}{n}} + \\sqrt{\\frac{\\ln \\frac{1}{\\delta}}{2n} } \\] The important quantity here is \\(n/d_{VC}\\). If the sample size \\(n\\) is large in relation to the VC-dimension \\(d_{VC}\\), the out-of-sample error is guaranteed to be close to the in-sample error. The bound is loose, so the actual value is not useful in practice. Rather it provides a way of thinking about generalization from in-sample to out-of-sample error. In practice, it is usually the case that models with lower \\(d_{VC}\\) generalizes better than models with higher \\(d_{VC}\\). A popular rule of thumb is that \\(n\\) should be at least \\(10\\times d_{VC}\\). However, there are also algorithms with \\(d_{VC}=\\infty\\) that work well in practice. A different way of thinking of this results is \\[ E_{out}(h) = E_{in}(h) + \\Omega(n,\\mathcal H, \\delta), \\] where \\(\\Omega\\) is a penalty for model complexity. As we choose a more complex \\(\\mathcal H\\) (with higher \\(d_{VC}\\)) the in-sample error will become smaller. But, at the same time the penalty \\(\\Omega\\) gets larger. The penalty gets smaller when we have more samples. The optimal model, with smallest \\(E_{out}\\), is therefore a compromise that minimize the sum of the two terms. One commonly used way to estimate \\(E_{out}\\) is to split the data into one training set and one test set. The test set is not used for training and so when we calculate \\(E_{test}\\), we do this for only one \\(h\\), the one with minimal in-sample error. Therefore Hoeffding’s inequality applies as a generalization bound, which is much tighter than the VC-bound. In the figure below we visualize the partition of the out-of-sample error into in-sample error and generalization error. Figure 4.13: Sketch of the different components of the error References "],["4.6-support-vector-machines-ii.html", "4.6 Support vector machines II", " 4.6 Support vector machines II Based on the previous discussion, in this section we develop the final form of SVM. The main conclusion from the previous sections is that the in-sample error is not an accurate estimate of the out-of-sample error and if we train a model with high complexity we may overfit the model. One way of dealing with this problem is to limit the complexity of the model, for example in terms of the VC-dimension. In the SVM case this could mean to limit the degree of the polynomial. Another way, which we will explore in this section, is to recall from the previous section that \\[ E_{out} = E_{in} + \\Omega(N,\\mathcal H, \\delta). \\] Now instead of minimizing the in-sample error of our model, we add a term that represents \\(\\Omega\\), which is increasing in the complexity of the model. Then the learning problem instead becomes, \\[ \\min_{h\\in\\mathcal H}\\left[ E_{in}(h) + c(h)\\right], \\] where \\(c\\) is some function that increases with the complexity of the function \\(h\\). SVMs are a special case of this. If we have a linear classifier \\(f(x) = \\beta_0+\\beta_1x^1 + \\ldots \\beta_p x^p\\) the optimisation problem is \\[ \\min_{\\beta_0,\\beta_1,\\ldots,\\beta_p} \\sum_{i=1}^n \\left( 1-y_if(x_i) \\right)_+ + \\lambda \\sum_{i=1}^p{\\beta_j^2}, \\] where \\(\\lambda\\geq 0\\) is a tuning parameter. Here a large value of \\(\\lambda\\) will penalize complex models heavily and force the model to be simple, thereby preventing overfitting. A small value of \\(\\lambda\\) will allow more complex models, thereby making the in-sample error smaller, on the other hand risking overfitting. As usual, we may take some function of the inputs \\(\\varphi_i(x)\\) and consider a linear function of these new features. Let us see what happens when we vary \\(\\lambda\\), using polynomial \\(\\varphi\\) of maximum degree 5. Figure 4.14: Error vs. reciprocal of lambda. We see from the picture that \\(\\lambda\\) controls the complexity of the model. When \\(\\lambda\\) is large (\\(1/\\lambda\\) small), the in-sample and out-of-sample errors are similar and as \\(\\lambda\\) becomes smaller, the in-sample error becomes smaller, but the difference between the errors are larger. With this development, we do not need to limit the complexity of the class of function \\(\\mathcal H\\), since this is done by choosing \\(\\lambda\\). In fact, we can in some cases even choose \\(\\mathcal H\\) to be an infinite dimensional space of functions. It turns out that the solution to the optimization problem of SVM can be written as \\[ f(x) = \\beta_0 + \\sum_{i=1}^n\\alpha_i\\langle x,x_i \\rangle, \\] for some values \\(\\alpha_i\\), where we define the inner product of two points \\(x\\) and \\(\\tilde x\\) as, \\[ \\langle x,\\tilde x \\rangle = \\sum_{j=1}^p x^j\\tilde x^j. \\] If we now instead consider the input features \\(\\varphi(x) = (\\varphi_1(x),\\ldots,\\varphi_M(x))\\) and functions of the form \\[ f(x) = \\beta_0 + \\varphi_1(x)\\beta_1 + \\ldots \\varphi_M(x)\\beta_M , \\] in the same way as above, the SVM classifier is \\[ f(x) = \\beta_0 + \\sum_{i=1}^n\\alpha_i\\langle \\varphi(x),\\varphi(x_i) \\rangle. \\] We then see that the features \\(\\varphi\\) only appears in the inner product. So it is enough if we specify the function \\[ K(x,x_i):= \\langle \\varphi(x),\\varphi(x_i) \\rangle, \\] which is called a kernel. We then write \\[ f(x) = \\beta_0 + \\sum_{i=1}^n\\alpha_iK(x,x_i). \\] An important point here is that the dimension of \\(\\varphi\\), \\(M\\), does not appear here. The sum is over (at most) \\(n\\) terms, even if let \\(M\\to\\infty\\). In practice, we therefore specify the kernel and not the features. Two examples of kernels are the polynomial kernel \\[ K(x,\\tilde x) = (1+\\sum_{i=1}^px_i\\tilde x_i)^d, \\] and the radial basis kernel, \\[ K(x,\\tilde x) = \\exp\\left( -\\gamma \\sum_{i=1}^p (x_i-\\tilde x_i)^2\\right). \\] Let us try the radial basis function kernel on our data set. Figure 4.15: Training data a radial basis SVM classification This now has an in-sample error of 0.275 and out-of-sample error 0.255555 "],["4.7-bias-variance-decomposition.html", "4.7 Bias-Variance decomposition", " 4.7 Bias-Variance decomposition In this section we will see another approach for understanding how model complexity affects statistical learning. Instead of bounding the difference between \\(E_{out}\\) and \\(E_{in}\\) we will decompose \\(E_{out}\\) into two different error terms. We consider the regression problem and therefore we assume a quadratic loss function, \\[ E_{out}(h) = E\\left[ (Y- h(X))^2 \\right]. \\] If \\(h^{\\mathcal D}\\) is the final hypothesis that was learnt from the data \\(\\mathcal D\\), we also define \\[ \\bar h(x) := E\\left[h^{\\mathcal D}(x))\\right]. \\] This is the average function that we would learn, averaged over all different datasets. It is just a theoretical tool and nothing that we would be able to calculate in practice. First note that we can always write \\[ Y = f(X) + \\varepsilon, \\] where \\(E\\left[ \\varepsilon \\right] = 0\\). This is because if we take \\(f(X) = E[Y\\mid X]\\), \\[ E\\left[\\varepsilon\\right] = E\\left[ Y-f(X) \\right] = E\\left[ Y - E\\left[ Y\\mid X \\right] \\right] = E\\left[ Y\\right]- E\\left[ Y\\right] = 0. \\] First let us see what the population minimizer of the error is. That is, assuming that we know the distribution of \\(X,Y\\), we would like to find the optimal choice of \\(h\\). We do this by conditioning on \\(X\\) and decomposing the error into two parts: \\[\\begin{align*} &amp;E\\left[ (Y-h(X))^2 \\mid X \\right] = E\\left[ (Y - E[Y\\mid X] + E[Y\\mid X] -h(X))^2 \\mid X \\right] \\\\ =&amp;E\\left[ (Y- E[Y\\mid X])^2 \\mid X \\right] + E\\left[ ( E[Y\\mid X] - h(X))^2 \\mid X \\right] \\\\ &amp;+ \\underbrace{2E\\left[ (Y- E[Y\\mid X])( E[Y\\mid X] - h(X)) \\mid X \\right]}_{=0}\\\\ =&amp;E\\left[ (Y- E[Y\\mid X])^2 \\mid X \\right] + E\\left[ ( E[Y\\mid X] - h(X))^2 \\mid X \\right] \\end{align*}\\] The first part does not depend on \\(h\\) and the second part is minimized by choosing \\(h(x) = E[Y\\mid X=x]\\) for every \\(x\\), which therefore is the population minimizer of the error. Coming back slightly to reality, we do not know the distribution of \\((X,Y)\\) but we have a training set \\(\\mathcal D\\). So we now take the frequentist point of view and consider the squared difference between the function learnt from data, \\(h^\\mathcal D\\) and the true regression function \\(f(x)\\), averaged with respect to \\(\\mathcal D\\). \\[\\begin{align} &amp;E\\left[ (h^{\\mathcal D}(x) - f(x))^2\\right] = E\\left[ (h^{\\mathcal D}(x) - \\bar h(x) + \\bar h(x) - f(x) )^2\\right] \\\\ =&amp; E\\left[ (h^{\\mathcal D}(x) - \\bar h(x) )^2\\right] + E\\left[ (\\bar h(x) - f(x) )^2\\right] +2(\\bar h(x) - f(x)) E\\left[ (h^{\\mathcal D}(x) - \\bar h(x) )\\right]\\\\ =&amp; E\\left[ (h^{\\mathcal D}(x) - \\bar h(x) )^2\\right] + (\\bar h(x) - f(x) )^2 \\end{align}\\] The first term is the variance of \\(h^\\mathcal D\\) thinking of the data set \\(\\mathcal D\\) as random. The second term is the difference between the average function \\(\\bar h\\) and the true regression function \\(f(x)\\) and is called the bias. This is the Bias-variance decomposition. If we choose a large class of functions \\(\\mathcal H\\) from which we choose \\(h^\\mathcal D\\), it will be possible to closely approximate \\(f\\), on average. In this case the bias will be small. However, many times if \\(\\mathcal H\\) is large, \\(h^\\mathcal D\\) will be sensitive to data, that is we will overfit, and then the variance term is large. This is known as the trade-off between bias and variance. However it is important to understand that this trade-off is an empirical observation and not an theoretical fact. We can illustrate this with a picture similar to what we saw in the discussion of generalization error. Figure 4.16: Sketch of the bias-variance decomposition "],["4.8-regression-regularization.html", "4.8 Regression regularization", " 4.8 Regression regularization In this section we discuss regularization in the context of regression as a way of controlling overfit. Recall that in regression we have observations from \\(\\mathbb R^p\\) and our task is to predict a value in \\(\\mathbb R\\). We will do this by finding a function \\(h\\in \\mathcal H\\) that gives a small out-of-sample error \\[ E_{out} = E\\left[ (Y-h(X))^2 \\right]. \\] In linear regression, the collection of functions \\(\\mathcal H\\) are the functions of the form \\[ h(x) = \\beta_1x^1+ \\cdots + \\beta_p x^p, \\] and we are thus to choose the parameters \\(\\beta_1,\\ldots,\\beta_p\\) in a good way. The \\(\\beta\\) that minimises the in-sample error is the least squares estimate of \\(\\beta_1,\\ldots,\\beta_p\\) and can be calculated easily. As we have seen, we may specify some function \\(\\varphi_i\\) and use \\(\\varphi_1(x),\\ldots,\\varphi_M(x)\\) to predict \\(y\\) and we are still in the case of linear regression. However we need to be careful not to overfit. Let us see a simple example were \\(\\varphi\\) are polynomials. First we construct the functions that will generate our data: library(ggplot2) library(gridExtra) library(resample) gen_data &lt;- function(truth, n.obs = 100) { x &lt;- runif(n.obs, min = -2, max = 2) y &lt;- truth(x) + rnorm(n.obs) * 0.4 data.df &lt;- data.frame(y = y, x = x) } truth &lt;- function(x) { sin(x * 2 + 1) + x } For convenience, we write a function that fits three different polynomial regressions and returns the plot. fit_plot &lt;- function(){ data.train.df &lt;- gen_data(truth) x.grid &lt;- seq(-2, 2, 0.01) x.grid.df &lt;- data.frame(x = x.grid) fit1 &lt;- predict(lm(y ~ poly(x, 2), data.train.df), x.grid.df) fit2 &lt;- predict(lm(y ~ poly(x, 6), data.train.df), x.grid.df) fit3 &lt;- predict(lm(y ~ poly(x, 20), data.train.df), x.grid.df) cbp1 &lt;- c(&quot;#999999&quot;, &quot;#E69F00&quot;, &quot;#56B4E9&quot;, &quot;#009E73&quot;, &quot;#F0E442&quot;, &quot;#0072B2&quot;, &quot;#D55E00&quot;, &quot;#CC79A7&quot;) p &lt;- ggplot(data.frame( x = x.grid, fit1 = fit1, fit2 = fit2, fit3 = fit3 )) + geom_line(aes(x = x, y = fit1, color = &quot;1&quot;), size = 1) + geom_line(aes(x = x, y = fit2, color = &quot;2&quot;), size = 1) + geom_line(aes(x = x, y = fit3, color = &quot;3&quot;), size = 1) + scale_colour_manual( name = &quot;&quot;, values = c( &quot;train&quot; = cbp1[1], &quot;1&quot; = cbp1[2], &quot;2&quot; = cbp1[3], &quot;3&quot; = cbp1[4] ), labels = c(&quot;1&quot; = &quot;2nd&quot;, &quot;2&quot; = &quot;6th&quot;, &quot;3&quot; = &quot;20th&quot;) ) + geom_point(aes(x = x, y = y, color = &quot;train&quot;), data = data.train.df, show.legend = FALSE) + ylab(&quot;y&quot;) + coord_cartesian(ylim = c(-3, 2)) + theme_minimal() p } Next we fit the polynomial models to four different training sets, generated from the same distribution. set.seed(42) grid.arrange(fit_plot(), fit_plot(), fit_plot(), fit_plot(), nrow = 2) Figure 4.17: Polynomial fits to four different sets of training data, sampled from the same distribution We see from the pictures that the 2nd degree polynomial fit does not change very much on the different training sets, while the 20th degree polynomial looks very different in the four pictures. Now let us repeat this experiment many times, and only plot the mean and variance of \\(h\\). Figure 4.18: Mean fit and 95% CI for the different polynomial regressions From the pictures we see that the 2nd degree polynomial has a bias, but for 6th degree, the bias is close to 0. The error in the 2nd degree polynomial is a sum of bias and variance, but for 6th and 20th the error is dominated by the variance term. The variance becomes larger as the degree of the polynomial becomes larger, while the bias is already close to 0 for 6th degree. Therefore, the total error, the sum of bias and variance, is smallest for a 6th degree polynomial. As before, we see that a very flexible model will tend to overfit the data and may perform worse than a less flexible model. Another way to see the same phenomena is to choose one point, here \\(x=1.5\\), and plot the bias and variance as we vary the degree of the polynomial. We do this for sample size \\(n=100\\) and \\(n=10000\\). library(ggplot2) library(gridExtra) biasVariancePlot &lt;- function(n.obs, x0 = 1.5, n.sim = 1e3, seed = 42){ degrees &lt;- seq(2,20,2) predictions &lt;- matrix(nrow = n.sim, ncol = length(degrees)) x.df = data.frame(x = x0) for (sim in seq_len(n.sim)) { data.df &lt;- gen_data(truth, n.obs) for (i in seq(length(degrees))) { predictions[sim,i] &lt;- predict(lm(y ~ poly(x, degrees[i]), data.df), x.df) } } plot.df &lt;- data.frame(hbar = colMeans(predictions), variance = colVars(predictions), bias = (colMeans(predictions)-truth(x0))^2, degree = degrees, total = colVars(predictions) + (colMeans(predictions)-truth(x0))^2) cbp1 &lt;- c(&quot;#999999&quot;, &quot;#E69F00&quot;, &quot;#56B4E9&quot;, &quot;#009E73&quot;, &quot;#F0E442&quot;, &quot;#0072B2&quot;, &quot;#D55E00&quot;, &quot;#CC79A7&quot;) ggplot(plot.df, aes(x = degree)) + geom_line(aes(y = bias, color = &quot;bias&quot;), size = 1) + geom_line(aes(y = variance, color = &quot;variance&quot;), size = 1) + geom_line(aes(y = total, color = &quot;total&quot;), size = 1) + scale_colour_manual(name = &quot;&quot;, values = c(&quot;bias&quot; = cbp1[2], &quot;variance&quot; = cbp1[3], &quot;total&quot; = cbp1[4]), labels = c(&quot;bias&quot; = &quot;bias&quot;, &quot;variance&quot; = &quot;variance&quot;, &quot;total&quot; = &quot;total&quot;)) + scale_y_continuous(trans=&#39;log10&#39;) + ylab(&quot;Error&quot;) + xlab(&quot;Degree&quot;) + theme_minimal() } grid.arrange(biasVariancePlot(100), biasVariancePlot(10000), nrow = 1) Figure 4.19: Bias-variance decomposition when varying the degree of the polynomial. Left picture with n = 100, right with n = 10000 From the pictures we see that the bias is decreasing and the variance is increasing, as the degree increases. In the left picture, \\(n=100\\), the minimum total error is at a degree around 3. When the sample size increases, in the right picture, we can fit a more complex model, and the minimum error is at a degree around 8. We have seen that we also in the regression setting need to bound the model complexity so that we do not overfit the model. This can be done by choosing a small family of models \\(\\mathcal H\\). In the above case this would mean limiting the degree of the polynomial. Another option, that we will explore here, is to include a regularization term in the loss function. Recall that least squares regression minimizes the in-sample error: \\[ E_{in} = \\sum_{i=1}^n\\left( y_i - \\beta_0 - \\sum_{j=1}^p \\beta_j x_{ij} \\right)^2. \\] Ridge regression instead minimizes \\[ E_{in} + \\lambda\\sum_{j=1}^p \\beta_j^2, \\] and the lasso minimizes \\[ E_{in} + \\lambda\\sum_{j=1}^p | \\beta_j |. \\] Here \\(\\lambda\\) is a tuning parameter that needs to be determined separately. Ridge regression and lasso use different penalty terms of model complexity, but the main idea is the same. Models with many and large parameters will be penalized in favor of less complex models. Compared to least squares regression, the estimates of \\(\\beta_j\\) will be smaller in ridge and lasso. That is the estimates will shrink towards zero. These methods are therefore sometimes called shrinkage methods. We know that the estimates in least squares regression are unbiased and shrinkage methods therefore introduce a bias. The hope is that the variance will decrease sufficiently, so that the total prediction error is smaller. Note that neither of these methods are scale invariant, however usually software will take care of this automatically. Let us look at the same example above, this time with a degree 20 polynomial, but penalized with ridge and lasso. library(ggplot2) library(gridExtra) library(glmnet) data.df &lt;- gen_data(truth) model.ridge &lt;- glmnet(poly(data.df$x,20), data.df$y, alpha = 0 ) model.lasso &lt;- glmnet(poly(data.df$x,20), data.df$y, alpha = 1 ) Figure 4.20: Coefficents as lambda is varied for ridge regression Figure 4.21: Coefficients as lambda is varied for the lasso In the pictures we see that the fitted coefficients decrease towards 0 as \\(\\lambda\\) increases. The main difference between the two methods is that in the ridge regression all coefficients are strictly positive for any finite \\(\\lambda\\), while for the lasso, coefficients become 0 when \\(\\lambda\\) is large. This difference relates to model interpretability. That is, the lasso will produce a simple model where many variables can be disregarded. We also mention that the lasso and ridge regression can, unlike unregularized least squares, handle the case \\(p&gt;&gt;n\\). There is a generalization of ridge and lasso called elastic net, where a parameter \\(0\\leq \\alpha \\leq 1\\) is introduced and the loss function is instead, \\[ E_{in} + \\lambda\\left( \\alpha \\sum_{j=1}^p | \\beta_j | + (1-\\alpha) \\sum_{j=1}^p \\beta_j^2\\right). \\] "],["4.9-model-selection.html", "4.9 Model selection", " 4.9 Model selection We have seen that in statistical learning a model that is too flexible will lead to overfitting and a model that is not flexible enough will lead to underfitting. We can control the flexibility of our model by either choosing the model class, e.g. the degree of a polynomial, or by introducing regularization. In either case this introduces a new parameter that we can not learn from the in-sample error. In this section we discuss how choose this parameter. One approach is to use a validation set. We would take the available observations and divide them (randomly) into a training set and a validation set. The training set is used only for fitting the model(s) and the validation set is used for estimating the out-of-sample error. We could then choose the model that achieves the smallest error on the validation set. However note that the error of the chosen model is not a good estimate of the out-of-sample error, since again we have used the validation set for choosing a model. The drawback of this method is that the division into training and validation set is random and so if we are unlucky, the division does not represent the underlying distribution well. An approach that tries to address this drawback is leave-one-out cross-validation (LOOCV). Here we again divide the observations into a training set and a validation set. The difference is that the validation set will only contain one observation, call this \\((x_1,y_1)\\). We fit the model on the \\(n-1\\) observations in the training set and predict the observation in the validation set. We then calculate the prediction error and call this (in the regression case) \\(MSE_1\\). We now repeat this, instead keeping \\((x_2,y_2)\\) in the validation set and all other in the training set. The LOOCV estimate of the out-of-sample error is then \\[ CV_{(n)}= \\frac{1}{n}\\sum_{i=1}^n MSE_i. \\] The advantage compared to the validation set approach is that it always yields the same results and that there is less bias. This is because the size of the training set, \\(n-1\\), is almost the same as the full data set, \\(n\\). Therefore the fit should be almost as good as using the full data set. The main drawback of LOOCV is that if \\(n\\) is large, it could be expensive to implement. An alternative is therefore k-fold cross-calidation. Here the data is divided randomly into \\(k\\) groups. We take the first group as validation set and the others as training set. This gives an out-of-sample error, call it \\(MSE_1\\). We continue by treating the second group as validation set, giving \\(MSE_2\\), and so on. The k-fold CV estimate of the out-of-sample error is then \\[ CV_{(k)} = \\frac{1}{k}\\sum_{i=1}^k MSE_i. \\] In practice usually \\(k=5\\) or \\(k=10\\) is used. "],["4.10-an-application-i.html", "4.10 An application I", " 4.10 An application I We will use the Hitters data set from the ISL book and predict the salary of a Baseball player based on various covariates. library(caret) library(ISLR) data(&quot;Hitters&quot;, package = &quot;ISLR&quot;) Hitters &lt;- na.omit(Hitters) Then we randomly split the data into a train and a test set. We keep 70% of the observations in the training set and the rest in the test set. set.seed(3) training.samples &lt;- caret::createDataPartition(Hitters$Salary, p = 0.7, list = FALSE) train.data &lt;- Hitters[training.samples, ] test.data &lt;- Hitters[-training.samples, ] We then do linear regression and calculate the out-of-sample error ls &lt;- lm(Salary ~., data = train.data) predictions &lt;- predict(ls, test.data) sqrt(mean((predictions - test.data$Salary)^2)) ## [1] 367.6942 Next we fit a lasso using 10-fold CV. lambda.grid &lt;- 10^seq(-2, 2, length = 100) set.seed(42) lasso &lt;- train( Salary ~., data = train.data, method = &quot;glmnet&quot;, trControl = trainControl(&quot;repeatedcv&quot;, number = 10, repeats = 5), tuneGrid = expand.grid(alpha = 1, lambda = lambda.grid) ) We may plot the cross-validated error against the regularization parameter. plot(lasso) Figure 4.22: Cross-validated error against the regularization parameter for Lasso. If we print the parameters, we see that some of them have been set to 0. coef(lasso$finalModel, lasso$bestTune$lambda) ## 20 x 1 sparse Matrix of class &quot;dgCMatrix&quot; ## s1 ## (Intercept) -66.88333624 ## AtBat . ## Hits 2.00894789 ## HmRun . ## Runs . ## RBI 0.63610478 ## Walks 3.46312810 ## Years . ## CAtBat . ## CHits . ## CHmRun 0.01178261 ## CRuns 0.08620313 ## CRBI 0.51517895 ## CWalks . ## LeagueN 29.48507342 ## DivisionW -100.72607403 ## PutOuts 0.17109758 ## Assists -0.06283794 ## Errors . ## NewLeagueN . We calculate the out-of-sample error. predictions &lt;- predict(lasso, test.data) sqrt(mean((predictions - test.data$Salary)^2)) ## [1] 358.6614 An improvement over least squares. We might also try elastic-net. lambda &lt;- 10^seq(-2, 2.5, length = 100) alpha &lt;- seq(0, 0.5, length = 10) set.seed(42) e.net &lt;- train( Salary ~., data = train.data, method = &quot;glmnet&quot;, trControl = trainControl(&quot;repeatedcv&quot;, number = 10, repeats = 5), tuneGrid = expand.grid(alpha = alpha, lambda = lambda) ) plot(e.net) Figure 4.23: Cross-validated error against the regularization parameter for elastic net predictions &lt;- predict(e.net, test.data) sqrt( mean((predictions - test.data$Salary)^2) ) ## [1] 361.3435 We get a worse out-of-sample error. But we need to keep in mind that the number of observations in the test-set is not very large. "],["4.11-an-application-ii.html", "4.11 An application II", " 4.11 An application II Here we show an application using a dataset on whether or not a patient has diabetes, based on certain diagnostic measurements. First load the data and print some observations library(caret) library(mlbench) data(&quot;PimaIndiansDiabetes2&quot;, package = &quot;mlbench&quot;) PimaIndiansDiabetes2 &lt;- na.omit(PimaIndiansDiabetes2) knitr::kable( head(PimaIndiansDiabetes2), booktabs = TRUE, caption = &#39;Diabetes data&#39;) Table 4.1: Diabetes data pregnant glucose pressure triceps insulin mass pedigree age diabetes 4 1 89 66 23 94 28.1 0.167 21 neg 5 0 137 40 35 168 43.1 2.288 33 pos 7 3 78 50 32 88 31.0 0.248 26 pos 9 2 197 70 45 543 30.5 0.158 53 pos 14 1 189 60 23 846 30.1 0.398 59 pos 15 5 166 72 19 175 25.8 0.587 51 pos Divide the data into a train and a test set. set.seed(42) training.samples &lt;- createDataPartition(PimaIndiansDiabetes2$diabetes, p = 0.7, list = FALSE) train.data &lt;- PimaIndiansDiabetes2[training.samples, ] test.data &lt;- PimaIndiansDiabetes2[-training.samples, ] Then use a SVM, where the parameter is estimated by 10-fold CV. If you have a computer with multiple cores, there may be a speed-up by using the library doMC. library(doMC) registerDoMC(cores=8) model &lt;- train( diabetes ~., data = train.data, method = &quot;svmRadial&quot;, trControl = trainControl(&quot;cv&quot;, number = 10), tuneLength = 10, preProcess = c(&quot;center&quot;,&quot;scale&quot;) ) We may print a summary of the training of the model print(model) ## Support Vector Machines with Radial Basis Function Kernel ## ## 275 samples ## 8 predictor ## 2 classes: &#39;neg&#39;, &#39;pos&#39; ## ## Pre-processing: centered (8), scaled (8) ## Resampling: Cross-Validated (10 fold) ## Summary of sample sizes: 248, 247, 248, 248, 247, 246, ... ## Resampling results across tuning parameters: ## ## C Accuracy Kappa ## 0.25 0.7602354 0.4003850 ## 0.50 0.7635605 0.4129658 ## 1.00 0.7671319 0.4258676 ## 2.00 0.7484902 0.3867206 ## 4.00 0.7561440 0.4115484 ## 8.00 0.7340540 0.3687010 ## 16.00 0.7230660 0.3528600 ## 32.00 0.7082604 0.3172943 ## 64.00 0.6942209 0.3047575 ## 128.00 0.7087803 0.3466723 ## ## Tuning parameter &#39;sigma&#39; was held constant at a value of 0.114713 ## Accuracy was used to select the optimal model using the largest value. ## The final values used for the model were sigma = 0.114713 and C = 1. We can also plot the cross-validated accuracy as a function of the regularization parameter plot(model) Figure 4.24: Cross-validated accuracy against the regularization parameter for the SVM The final results are predicted &lt;- predict(model, test.data) mean(predicted == test.data$diabetes) ## [1] 0.7606838 "],["4.12-review-questions-3.html", "4.12 Review questions", " 4.12 Review questions What is a classifier? What is a loss function? What is the out-of-sample error? What is the in-sample error? What is the Bayes classifier? What is the Bayes risk? What is the hinge loss? Explain in general terms what Hoeffding’s inequality is. What is generalisation error? What is the VC-dimension? How does in general the generalisation error depend on the complexity of the model? How does in general the generalisation error depend on the number of training samples? What is the optimisation problem solved by support vector machines? What is the bias-variance decomposition What is the optimisation problem that Ridge regression/Lasso solves? What is the main difference between the Ridge regression and Lasso? What is a validation set? What is the drawback of leave-one-out cross validation? Explain k-fold cross validation. "],["5-beyond-linearity-draft.html", "Chapter 5 Beyond linearity (draft)", " Chapter 5 Beyond linearity (draft) In this module we cover some state-of-the-art non-linear models. Decision trees, bagging, random forest and boosting is covered well in ISLR, so we will no repeat it here. But it is part of the course. The new version of ISLR also includes a chapter on deep learning. The first part of this chapter is also part of the course Readings for this chapter is therefore: ISLR 8 ISLR 10.1-3 and 10.6-7 "],["5.1-an-application-i-1.html", "5.1 An application I", " 5.1 An application I Let us see an example of how to implement a neural network classifier. We will use Keras, which is just a wrapper for the machine learning library Tensorflow. You may find the documentation useful Our goal is to classify hand-written digits from the MNIST database, which is conveniently included in Keras. The first time you install Keras, you do. install.packages(&quot;keras&quot;) keras::install_keras() If you get a prompt asking to install miniconda, you should choose “Yes.” If this installation fails another option is to follow the instructions at the ISLR webpage After that, it should be enough to library(keras) The MNIST database is already divided in a training and a test set mnist &lt;- dataset_mnist() x_train &lt;- mnist$train$x y_train &lt;- mnist$train$y x_test &lt;- mnist$test$x y_test &lt;- mnist$test$y Let us see what the pictures look like. Figure 5.1: Examples from MNIST Each image is represented as a 28x28 matrix of pixel values between 0 and 255. We reshape each matrix in to a vector and scale the pixel value so that it is between 0 and 1. dim(x_train) &lt;- c(nrow(x_train), 784) dim(x_test) &lt;- c(nrow(x_test), 784) x_train &lt;- x_train / 255 x_test &lt;- x_test / 255 The \\(y\\) variables are given as an integer between 0 and 9. We transform it to a vector of dummy variables. y_train &lt;- to_categorical(y_train, 10) y_test &lt;- to_categorical(y_test, 10) Now we specify a 2-layer NN with Relu activation in the hidden layer and softmax in the last layer. model &lt;- keras_model_sequential() model %&gt;% layer_dense(units = 50, activation = &quot;relu&quot;, input_shape = c(784)) %&gt;% layer_dense(units = 10, activation = &quot;softmax&quot;) We compile the model by specifying the loss and the optimization method. model %&gt;% compile( loss = &quot;categorical_crossentropy&quot;, optimizer = optimizer_rmsprop(), metrics = c(&quot;accuracy&quot;) ) Here, cross entropy loss is just the negative of a multinomial log likelihood. The optimizer, RMSprop, is a way of choosing the learning rate adaptively. Now we train the NN. history &lt;- model %&gt;% fit( x_train, y_train, epochs = 10, batch_size = 128, validation_split = 0.2 ) Here we use 20‰ as a validation set. Usually NN does not include a regularization term and so there is a risk of overfitting. Instead one usually restricts the number of epochs and the optimization algorithm is not run until convergence. This is called early stopping. Figure 5.2: Training and validation loss/accuracy for each epoch We see that the validation accuracy is still increasing, so we could probably run more epochs. Let us evaluate the model on the test set. model %&gt;% evaluate(x_test, y_test,verbose = 0) ## loss accuracy ## 0.1103705 0.9698000 The accuracy is 97%, which is not too bad. Let us make predictions on the test set and plot some of them. Figure 5.3: Predictions on the test set "],["5.2-an-application-ii-1.html", "5.2 An application II", " 5.2 An application II In this section we demonstrate how to use boosting to predict the salary of baseball players using the Hitters dataset. We start by loading the required packages and splitting the data into a training and test set library(caret) library(ISLR2) library(tidyverse) library(gbm) Hitters &lt;- na.omit(Hitters) set.seed(3) training.samples &lt;- caret::createDataPartition(Hitters$Salary, p = 0.7, list = FALSE) train.data &lt;- Hitters[training.samples, ] test.data &lt;- Hitters[-training.samples, ] Boosting has a number of different parameters and we use a grid search and cross-validation to find the best choice. gbmGrid &lt;- expand.grid(interaction.depth = c(1, 2, 3), n.trees = (1:20)*2000, shrinkage = 0.001, n.minobsinnode = 5) fitControl &lt;- trainControl( method = &quot;repeatedcv&quot;, number = 5, repeats = 5 ) The performance of the model is usually better the smaller the shrinkage parameter, or learning rate, is chosen. But with a small shrinkage we need many iterations, i.e. trees. So there is a tradeof between performance and the time it takes to train the model and the amount of storage required. To speed up the training we use parallel processes. library(doParallel) cl &lt;- makePSOCKcluster(4) registerDoParallel(cl) Now we fit the model gbmFit &lt;- train( Salary ~ ., data = train.data, method = &quot;gbm&quot;, trControl = fitControl, verbose = FALSE, distribution = &quot;gaussian&quot;, tuneGrid = gbmGrid ) Here, gaussian means that we are doing regression that minimizes the square error. We may know predict the observations in the test set and calculate the out-of-sample error. predictions &lt;- predict(gbmFit, test.data) sqrt(mean((predictions - test.data$Salary)^2)) ## [1] 251.9191 This is an improvement over the regularized linear regression we did previously. We can also see the importance of each variable. vip::vip(gbmFit) + theme_minimal() By making a partial dependence plot we can illustrate how each variable affect the prediction on average. gbmFit$finalModel %&gt;% pdp::partial( pred.var = &quot;CHmRun&quot;, n.trees = gbmFit$finalModel$n.trees, grid.resolution = 100, train = train.data, plot = TRUE, rug = TRUE, plot.engine = &quot;ggplot2&quot;) + theme_minimal() "],["5.3-review-questions-4.html", "5.3 Review questions", " 5.3 Review questions What is an activation function? What is an ReLU? What is the softmax function? What is a layer? What is gradient descent? What is the trade-off in choosing the learning rate? What is a mini-batch? What is an epoch? What is backpropagation? How are NNs usually regularized? "],["references.html", "References", " References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
